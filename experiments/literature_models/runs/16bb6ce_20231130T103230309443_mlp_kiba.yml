active: true
cv:
  call: bipartite_learn.model_selection.multipartite_cross_validate
  params:
    cv: 4
    diagonal: false
    n_jobs: 4
    pairwise: true
    random_state: 0
    return_estimator: false
    return_train_score: false
    scoring:
    - roc_auc
    - average_precision
    shuffle: true
    verbose: 10
dataset:
  X:
  - force_download: false
    path: datasets/kiba/final/ligand_similarity.tsv
    read:
      call: data_loading.read_table_to_array
      params: {}
  - force_download: false
    path: datasets/kiba/final/normalized_target_similarity.tsv
    read:
      call: data_loading.read_table_to_array
      params: {}
  name: kiba
  pairwise: true
  y:
    force_download: false
    path: datasets/kiba/final/binary_affinity.tsv
    read:
      call: data_loading.read_table_to_array
      params: {}
directory: literature_models/runs
end: 2023-12-01 16:45:15.968639
estimator:
  call: literature_models.estimators.mlp
  final_params:
    cv:
      call: bipartite_learn.model_selection._split.MultipartiteCrossValidator
      params: {}
    diagonal: false
    error_score: .nan
    estimator:
      call: bipartite_learn.wrappers.GlobalSingleOutputWrapper
      params:
        estimator:
          call: sklearn.neural_network._multilayer_perceptron.MLPRegressor
          params:
            activation: relu
            alpha: 0.0001
            batch_size: auto
            beta_1: 0.9
            beta_2: 0.999
            early_stopping: false
            epsilon: 1.0e-08
            hidden_layer_sizes:
            - 100
            learning_rate: constant
            learning_rate_init: 0.001
            max_fun: 15000
            max_iter: 200
            momentum: 0.9
            n_iter_no_change: 10
            nesterovs_momentum: true
            power_t: 0.5
            random_state: null
            shuffle: true
            solver: adam
            tol: 0.0001
            validation_fraction: 0.1
            verbose: false
            warm_start: false
        under_sampler:
          call: imblearn.under_sampling._prototype_selection._random_under_sampler.RandomUnderSampler
          params:
            random_state: null
            replacement: false
            sampling_strategy: auto
    n_jobs: 4
    pairwise: true
    param_grid:
      estimator__hidden_layer_sizes:
      - - 100
        - 100
        - 100
        - 100
        - 100
      - - 100
        - 100
        - 100
        - 100
        - 100
        - 100
        - 100
        - 100
        - 100
        - 100
      - - 200
        - 100
        - 100
        - 100
        - 50
      - - 1024
        - 512
        - 256
        - 128
        - 64
        - 32
    pre_dispatch: 2*n_jobs
    refit: true
    return_train_score: false
    scoring: average_precision
    train_test_combinations: null
    verbose: 0
  name: mlp
  params: {}
hash: 16bb6ce5d6b253f8610193373db208d45caec94ae4a2dea02ff399440b7ff713
path: /home/pedro/master_thesis/experiments/literature_models/runs/16bb6ce_20231130T103230309443_mlp_kiba.yml
results:
  LL_average_precision:
  - 0.7986813889900491
  - 0.8515087791485381
  - 0.7421524283415245
  - 0.8285165140694205
  - 0.8432223744924353
  - 0.847136508590587
  - 0.6362890500859685
  - 0.7419882193945104
  - 0.7884760138634384
  - 0.8478986730284785
  - 0.7476572608177748
  - 0.8369048921610488
  - 0.8004402711613818
  - 0.8072975886040912
  - 0.8142802520072012
  - 0.819363276175394
  LL_roc_auc:
  - 0.9503475764960301
  - 0.9659105550438625
  - 0.9229805628028603
  - 0.9615001387317557
  - 0.9597642195801298
  - 0.9623015972124798
  - 0.88466014261653
  - 0.9338290933707108
  - 0.944762888500738
  - 0.9652077455285861
  - 0.9323265921371586
  - 0.9626793379703908
  - 0.9453767946825622
  - 0.9525657106086206
  - 0.9501788818822284
  - 0.9570681967694081
  LT_average_precision:
  - 0.45136591586423375
  - 0.40143138956247937
  - 0.3384616071216655
  - 0.3494514360803279
  - 0.4611572110487494
  - 0.4077795844598646
  - 0.36571172047531464
  - 0.3356874842030038
  - 0.3463174672603633
  - 0.3627064194095809
  - 0.3548368753220932
  - 0.3559095097011817
  - 0.4571117284091659
  - 0.3333774261675576
  - 0.38940214849703314
  - 0.31994035126777043
  LT_roc_auc:
  - 0.8075558394870067
  - 0.7361941740973775
  - 0.708385940374239
  - 0.7086548350743407
  - 0.8063046026960758
  - 0.7464492252128702
  - 0.7632005774766168
  - 0.6989874299285619
  - 0.6893846791253782
  - 0.7008215937661764
  - 0.7148484382762068
  - 0.7168331465620852
  - 0.8118495900814372
  - 0.6702606137599801
  - 0.7504603432632755
  - 0.6590642455327625
  TL_average_precision:
  - 0.627042077322134
  - 0.6300206341861312
  - 0.57459487794454
  - 0.5980768195696452
  - 0.6345186676254236
  - 0.6097189853919222
  - 0.5101531327229798
  - 0.5059626271393792
  - 0.5657553981915519
  - 0.6178845283338709
  - 0.5686918877515266
  - 0.6116476313864131
  - 0.6107293535227712
  - 0.5614388479364928
  - 0.6085396671077693
  - 0.5600686129345024
  TL_roc_auc:
  - 0.8667762956206289
  - 0.8801803951223126
  - 0.8213412570806479
  - 0.8630590358825315
  - 0.8751335725362722
  - 0.8780959182328025
  - 0.8209860430296144
  - 0.8297931101386924
  - 0.8208191614751053
  - 0.8734983804132571
  - 0.8423742175458779
  - 0.8690299987653666
  - 0.877723274155753
  - 0.8431402141179356
  - 0.860289086249082
  - 0.8284657932995563
  TT_average_precision:
  - 0.37048915006601246
  - 0.3282431593718941
  - 0.2784407135976582
  - 0.29502171014597667
  - 0.3254991989368718
  - 0.3109636928323959
  - 0.29703103818353505
  - 0.26669805231709987
  - 0.26140107837658055
  - 0.2868607807146419
  - 0.2777986528444276
  - 0.26686127798527237
  - 0.3465515225828634
  - 0.2600946718557646
  - 0.29142930054424837
  - 0.25095203336968175
  TT_roc_auc:
  - 0.7178841863192427
  - 0.6597883736532643
  - 0.6310323242964155
  - 0.6407364601499621
  - 0.695377405769982
  - 0.6547234278090989
  - 0.6989398789284372
  - 0.6160709568555147
  - 0.6027766106365644
  - 0.6181991308970027
  - 0.630768886829463
  - 0.6101101181195405
  - 0.7277522289055897
  - 0.600463335951651
  - 0.6685283615418055
  - 0.5996169753877825
  fit_time:
  - 17538.363235473633
  - 35730.75937318802
  - 17889.362238645554
  - 33466.89552259445
  - 32006.49524950981
  - 13274.01409649849
  - 19374.967515707016
  - 27345.898002147675
  - 31775.74490261078
  - 18373.6969332695
  - 18999.756482839584
  - 41645.30556368828
  - 27102.680438518524
  - 27494.921359300613
  - 28267.573538064957
  - 13966.843688964844
  score_time:
  - 11.478323459625244
  - 81.79015111923218
  - 14.714754343032837
  - 77.03482222557068
  - 76.16195011138916
  - 16.29922389984131
  - 23.742761611938477
  - 23.059266328811646
  - 76.15384769439697
  - 17.079223155975342
  - 23.093700408935547
  - 74.4066846370697
  - 16.755735635757446
  - 18.25346279144287
  - 17.137619256973267
  - 12.254843711853027
start: 2023-11-30 10:32:30.309443
wrapper: null
