active: true
cv:
  call: bipartite_learn.model_selection.multipartite_cross_validate
  params:
    cv: 4
    diagonal: false
    n_jobs: 16
    pairwise: true
    random_state: 0
    return_estimator: false
    return_train_score: false
    scoring:
    - roc_auc
    - average_precision
    - matthews_corrcoef
    - balanced_accuracy
    - f1_macro
    - f1_micro
    - f1_weighted
    - precision_macro
    - precision_micro
    - precision_weighted
    - recall_macro
    - recall_micro
    - recall_weighted
    shuffle: true
    verbose: 10
dataset:
  X:
  - force_download: false
    path: datasets/kiba/final/ligand_similarity.tsv
    read:
      call: utils.read_table_to_array
      params: {}
  - force_download: false
    path: datasets/kiba/final/normalized_target_similarity.tsv
    read:
      call: utils.read_table_to_array
      params: {}
  name: kiba
  pairwise: true
  y:
    force_download: false
    path: datasets/kiba/final/binary_affinity.tsv
    read:
      call: utils.read_table_to_array
      params: {}
directory: runs
end: 2023-08-13 18:49:03.486375
estimator:
  call: y_reconstruction.estimators.bxt_ngsous
  final_params:
    memory: null
    steps:
    - - dropper
      - call: bipartite_positive_dropper.PositiveDropper
        params:
          drop: 0.9
          random_state: 0
    - - estimator
      - call: bipartite_approaches.estimators.RegressorToBinaryClassifier
        params:
          estimator:
            call: bipartite_learn.wrappers.GlobalSingleOutputWrapper
            params:
              estimator:
                call: sklearn.ensemble._forest.ExtraTreesRegressor
                params:
                  bootstrap: false
                  ccp_alpha: 0.0
                  criterion: squared_error
                  max_depth: null
                  max_features: 1.0
                  max_leaf_nodes: null
                  max_samples: null
                  min_impurity_decrease: 0.0
                  min_samples_leaf: 1
                  min_samples_split: 2
                  min_weight_fraction_leaf: 0.0
                  n_estimators: 100
                  n_jobs: 3
                  oob_score: false
                  random_state: 0
                  verbose: 0
                  warm_start: false
              estimator__bootstrap: false
              estimator__ccp_alpha: 0.0
              estimator__criterion: squared_error
              estimator__max_depth: null
              estimator__max_features: 1.0
              estimator__max_leaf_nodes: null
              estimator__max_samples: null
              estimator__min_impurity_decrease: 0.0
              estimator__min_samples_leaf: 1
              estimator__min_samples_split: 2
              estimator__min_weight_fraction_leaf: 0.0
              estimator__n_estimators: 100
              estimator__n_jobs: 3
              estimator__oob_score: false
              estimator__random_state: 0
              estimator__verbose: 0
              estimator__warm_start: false
              under_sampler:
                call: imblearn.under_sampling._prototype_selection._random_under_sampler.RandomUnderSampler
                params:
                  random_state: null
                  replacement: false
                  sampling_strategy: auto
              under_sampler__random_state: null
              under_sampler__replacement: false
              under_sampler__sampling_strategy: auto
          estimator__estimator:
            call: sklearn.ensemble._forest.ExtraTreesRegressor
            params:
              bootstrap: false
              ccp_alpha: 0.0
              criterion: squared_error
              max_depth: null
              max_features: 1.0
              max_leaf_nodes: null
              max_samples: null
              min_impurity_decrease: 0.0
              min_samples_leaf: 1
              min_samples_split: 2
              min_weight_fraction_leaf: 0.0
              n_estimators: 100
              n_jobs: 3
              oob_score: false
              random_state: 0
              verbose: 0
              warm_start: false
          estimator__estimator__bootstrap: false
          estimator__estimator__ccp_alpha: 0.0
          estimator__estimator__criterion: squared_error
          estimator__estimator__max_depth: null
          estimator__estimator__max_features: 1.0
          estimator__estimator__max_leaf_nodes: null
          estimator__estimator__max_samples: null
          estimator__estimator__min_impurity_decrease: 0.0
          estimator__estimator__min_samples_leaf: 1
          estimator__estimator__min_samples_split: 2
          estimator__estimator__min_weight_fraction_leaf: 0.0
          estimator__estimator__n_estimators: 100
          estimator__estimator__n_jobs: 3
          estimator__estimator__oob_score: false
          estimator__estimator__random_state: 0
          estimator__estimator__verbose: 0
          estimator__estimator__warm_start: false
          estimator__under_sampler:
            call: imblearn.under_sampling._prototype_selection._random_under_sampler.RandomUnderSampler
            params:
              random_state: null
              replacement: false
              sampling_strategy: auto
          estimator__under_sampler__random_state: null
          estimator__under_sampler__replacement: false
          estimator__under_sampler__sampling_strategy: auto
    verbose: false
  name: bxt_ngsous__drop90
  params:
    estimator__estimator__min_samples_leaf: 1
hash: 93313d98e81366cda90b8e9cc957fcd39d0ee5e52b8acd1ae41d97b648d55399
path: /home/pedro/mestrado/biomal_repo/scripts/run_experiments/literature_models2/runs/93313d9_20230813T184207824990_bxt_ngsous__drop90_kiba.yml
results:
  LL_average_precision:
  - 0.6544380266500616
  - 0.653550561148144
  - 0.6483549898461289
  - 0.644939435239691
  - 0.6468873344489576
  - 0.6682750613449295
  - 0.6734875787777987
  - 0.6470749472804258
  - 0.6555659465190767
  - 0.6181801570922258
  - 0.6310725850632405
  - 0.6320386551422196
  - 0.6501076355930272
  - 0.6531266740587638
  - 0.6787437928724266
  - 0.6453327249674196
  LL_balanced_accuracy:
  - 0.5727314741734726
  - 0.5821020106181413
  - 0.5914867442297095
  - 0.5812196698607563
  - 0.5645849404875292
  - 0.5809800943656485
  - 0.5943357839525962
  - 0.5779657562820267
  - 0.5745849902032705
  - 0.582431909334828
  - 0.588890631968152
  - 0.5768739781051004
  - 0.5827816339777284
  - 0.5786720306119804
  - 0.5850515383763178
  - 0.5753877649975033
  LL_f1_macro:
  - 0.3223941350517515
  - 0.33486404683100524
  - 0.3597460430326195
  - 0.3369142996993043
  - 0.31012673974179783
  - 0.3356315921765496
  - 0.36456116678846423
  - 0.33189875967874516
  - 0.3254550126711677
  - 0.33447371220408795
  - 0.3539545633231677
  - 0.3282266293936293
  - 0.34349199056885976
  - 0.3319520489253796
  - 0.3509568082895749
  - 0.3287099587162705
  LL_f1_micro:
  - 0.32491420169712554
  - 0.33587976905786776
  - 0.36045777079140284
  - 0.33821930687978374
  - 0.3140310240752439
  - 0.33675755483406544
  - 0.36521397405573763
  - 0.3337532503782926
  - 0.32766639698847033
  - 0.3354867854676872
  - 0.3548898911398728
  - 0.32997766971749254
  - 0.3450846181109339
  - 0.33362329692271553
  - 0.35255535001174537
  - 0.33118613460183227
  LL_f1_weighted:
  - 0.2973230746424702
  - 0.3188093351139678
  - 0.3469929603982583
  - 0.31898322661959433
  - 0.2788385903984432
  - 0.31884129810228296
  - 0.3524526280044529
  - 0.310566363769806
  - 0.3019878576554667
  - 0.3184135017146131
  - 0.3392329007317045
  - 0.3072554620475354
  - 0.3241624750634584
  - 0.3116044481887533
  - 0.33197174602867846
  - 0.3042139693882265
  LL_matthews_corrcoef:
  - 0.16884073088626644
  - 0.17980301244927846
  - 0.19625949578589066
  - 0.17989477229158332
  - 0.1567060794598634
  - 0.17810808691312816
  - 0.20161142523018563
  - 0.1764909853719733
  - 0.171266335959868
  - 0.18054805929071147
  - 0.19296186276551025
  - 0.17365013963783407
  - 0.18491560965829054
  - 0.17697512798702622
  - 0.18951689853878118
  - 0.17427641129419322
  LL_precision_macro:
  - 0.5979878131516208
  - 0.5984419353510082
  - 0.6052551110284885
  - 0.5996129667644626
  - 0.5950561971347745
  - 0.5979329885706796
  - 0.6077193750882803
  - 0.5998806059319624
  - 0.5983178979885158
  - 0.5988622063248434
  - 0.6047193603463075
  - 0.5980642986727345
  - 0.6032650029126532
  - 0.599527734578567
  - 0.6055732074852143
  - 0.6007201484703408
  LL_precision_micro:
  - 0.32491420169712554
  - 0.33587976905786776
  - 0.36045777079140284
  - 0.33821930687978374
  - 0.3140310240752439
  - 0.33675755483406544
  - 0.3652139740557376
  - 0.3337532503782926
  - 0.32766639698847033
  - 0.3354867854676872
  - 0.3548898911398728
  - 0.32997766971749254
  - 0.3450846181109339
  - 0.33362329692271553
  - 0.35255535001174537
  - 0.33118613460183227
  LL_precision_weighted:
  - 0.825353259940587
  - 0.8315130118051446
  - 0.825472534722357
  - 0.8277931364610869
  - 0.8199537852551935
  - 0.8282219190656396
  - 0.8266214544071729
  - 0.8265907441578048
  - 0.826049633967308
  - 0.8327326697255473
  - 0.8263824878981878
  - 0.8277881816853269
  - 0.8249195710267824
  - 0.8279150388976032
  - 0.8221239344114315
  - 0.8249264052857547
  LL_recall_macro:
  - 0.5727314741734726
  - 0.5821020106181413
  - 0.5914867442297095
  - 0.5812196698607563
  - 0.5645849404875292
  - 0.5809800943656485
  - 0.5943357839525962
  - 0.5779657562820267
  - 0.5745849902032705
  - 0.582431909334828
  - 0.588890631968152
  - 0.5768739781051004
  - 0.5827816339777284
  - 0.5786720306119804
  - 0.5850515383763178
  - 0.5753877649975033
  LL_recall_micro:
  - 0.32491420169712554
  - 0.33587976905786776
  - 0.36045777079140284
  - 0.33821930687978374
  - 0.3140310240752439
  - 0.33675755483406544
  - 0.3652139740557376
  - 0.3337532503782926
  - 0.32766639698847033
  - 0.3354867854676872
  - 0.3548898911398728
  - 0.32997766971749254
  - 0.3450846181109339
  - 0.33362329692271553
  - 0.35255535001174537
  - 0.33118613460183227
  LL_recall_weighted:
  - 0.32491420169712554
  - 0.33587976905786776
  - 0.36045777079140284
  - 0.33821930687978374
  - 0.3140310240752439
  - 0.33675755483406544
  - 0.3652139740557376
  - 0.3337532503782926
  - 0.32766639698847033
  - 0.3354867854676872
  - 0.3548898911398728
  - 0.32997766971749254
  - 0.3450846181109339
  - 0.33362329692271553
  - 0.35255535001174537
  - 0.33118613460183227
  LL_roc_auc:
  - 0.857285114949676
  - 0.8647475845684198
  - 0.8520716018244741
  - 0.8606200532429138
  - 0.8577856978614788
  - 0.875697777493509
  - 0.8696882651650897
  - 0.8581636694165808
  - 0.8663646571677885
  - 0.8482842802072901
  - 0.8431604155101291
  - 0.8531643823441242
  - 0.8593171303453979
  - 0.866563707172204
  - 0.8705007973027747
  - 0.8585685164299369
  LT_average_precision:
  - 0.3854288387469598
  - 0.36770115420611
  - 0.3531613386268414
  - 0.3504703491902367
  - 0.39459925744418156
  - 0.3636127278334713
  - 0.3583129369532854
  - 0.3675330216046302
  - 0.39270756565793025
  - 0.38024444682146824
  - 0.35765855329703766
  - 0.3656527271928888
  - 0.37657269507159374
  - 0.3807049424905562
  - 0.37166229890517877
  - 0.36341185168927415
  LT_balanced_accuracy:
  - 0.5674043818250033
  - 0.5625962301159728
  - 0.5865294199423937
  - 0.5681220943182015
  - 0.5599631181736134
  - 0.5564241816114732
  - 0.5763020062639713
  - 0.5635698516582982
  - 0.5648049442863423
  - 0.5650887022316423
  - 0.5807880014492914
  - 0.5646571598853679
  - 0.5787037243041402
  - 0.5619371305709863
  - 0.5739821940149521
  - 0.5622518445022766
  LT_f1_macro:
  - 0.3037694110358358
  - 0.3223338277229154
  - 0.34076712624928474
  - 0.32219746285309847
  - 0.2906633713192942
  - 0.3089715926641193
  - 0.3302420585524049
  - 0.31711014551728434
  - 0.2982128689020406
  - 0.32543710068032305
  - 0.3312350369917816
  - 0.3168613536514827
  - 0.32791169546864485
  - 0.3209451626707016
  - 0.3209721632796042
  - 0.3136233555489923
  LT_f1_micro:
  - 0.30797046202104256
  - 0.32692755261495493
  - 0.34082521528077936
  - 0.3247553501568197
  - 0.296643213453286
  - 0.31563431636577227
  - 0.33057374959825336
  - 0.32030011858452195
  - 0.30276428431393904
  - 0.32948764836918576
  - 0.3314492801808691
  - 0.3198900599572209
  - 0.33049242424242425
  - 0.3267433102959419
  - 0.3220361509835194
  - 0.3177055644160907
  LT_f1_weighted:
  - 0.2707147030877342
  - 0.290075107317368
  - 0.33681110546569193
  - 0.29710358702970696
  - 0.25102333120876863
  - 0.26992246696934086
  - 0.32079299336531797
  - 0.28914524946830367
  - 0.26353618026072484
  - 0.29507942754824756
  - 0.323574416904947
  - 0.28947545650447376
  - 0.3027753156151286
  - 0.2851833167520136
  - 0.3040668454971402
  - 0.28215581587749416
  LT_matthews_corrcoef:
  - 0.16614955849034357
  - 0.15257773684192796
  - 0.17673698272348565
  - 0.15798184518750732
  - 0.15594960772091856
  - 0.1451589242755612
  - 0.16065063330868057
  - 0.15032610619007453
  - 0.16183337047801008
  - 0.15647445071088548
  - 0.1684341598928327
  - 0.1523291678687282
  - 0.18291911721990983
  - 0.15515675561311223
  - 0.16291613006930533
  - 0.15102786236874652
  LT_precision_macro:
  - 0.6023882833693459
  - 0.5929767085680461
  - 0.5902466498764142
  - 0.5915938641443856
  - 0.6013968289553129
  - 0.5933602964856894
  - 0.5845607712240019
  - 0.5888705007671655
  - 0.6010341112421589
  - 0.5940418724210272
  - 0.58779170702908
  - 0.5897200535281721
  - 0.6062827831222181
  - 0.5971695757878072
  - 0.589689369821185
  - 0.591601362955755
  LT_precision_micro:
  - 0.30797046202104256
  - 0.32692755261495493
  - 0.34082521528077936
  - 0.3247553501568197
  - 0.296643213453286
  - 0.31563431636577227
  - 0.33057374959825336
  - 0.32030011858452195
  - 0.30276428431393904
  - 0.32948764836918576
  - 0.3314492801808691
  - 0.3198900599572209
  - 0.33049242424242425
  - 0.3267433102959419
  - 0.3220361509835194
  - 0.3177055644160907
  LT_precision_weighted:
  - 0.8371360876146059
  - 0.8000502523178307
  - 0.8325521993615685
  - 0.8132453046412902
  - 0.8355152411722476
  - 0.8004255967812793
  - 0.8218861048305243
  - 0.807550871445664
  - 0.8372863599159073
  - 0.8028694290659825
  - 0.8305554724357683
  - 0.810677144930803
  - 0.8350902431719602
  - 0.8013782334785856
  - 0.8274182102647
  - 0.8090730063110501
  LT_recall_macro:
  - 0.5674043818250033
  - 0.5625962301159728
  - 0.5865294199423937
  - 0.5681220943182015
  - 0.5599631181736134
  - 0.5564241816114732
  - 0.5763020062639713
  - 0.5635698516582982
  - 0.5648049442863423
  - 0.5650887022316423
  - 0.5807880014492914
  - 0.5646571598853679
  - 0.5787037243041402
  - 0.5619371305709863
  - 0.5739821940149521
  - 0.5622518445022766
  LT_recall_micro:
  - 0.30797046202104256
  - 0.32692755261495493
  - 0.34082521528077936
  - 0.3247553501568197
  - 0.296643213453286
  - 0.31563431636577227
  - 0.33057374959825336
  - 0.32030011858452195
  - 0.30276428431393904
  - 0.32948764836918576
  - 0.3314492801808691
  - 0.3198900599572209
  - 0.33049242424242425
  - 0.3267433102959419
  - 0.3220361509835194
  - 0.3177055644160907
  LT_recall_weighted:
  - 0.30797046202104256
  - 0.32692755261495493
  - 0.34082521528077936
  - 0.3247553501568197
  - 0.296643213453286
  - 0.31563431636577227
  - 0.33057374959825336
  - 0.32030011858452195
  - 0.30276428431393904
  - 0.32948764836918576
  - 0.3314492801808691
  - 0.3198900599572209
  - 0.33049242424242425
  - 0.3267433102959419
  - 0.3220361509835194
  - 0.3177055644160907
  LT_roc_auc:
  - 0.7541531293843574
  - 0.7213560736061034
  - 0.7487932874725512
  - 0.7351068001997284
  - 0.7701105327226925
  - 0.700206758880887
  - 0.7445053377353066
  - 0.7418357847217281
  - 0.7619670051603535
  - 0.7291442300971823
  - 0.7519482020002078
  - 0.7449479312045661
  - 0.7589372130252361
  - 0.7215242719292237
  - 0.7479225006017467
  - 0.7382016103287188
  TL_average_precision:
  - 0.5183706018213177
  - 0.51618035518799
  - 0.5343886159156477
  - 0.5175559941529726
  - 0.5033662896700405
  - 0.5091865733713549
  - 0.5134245510638498
  - 0.48377252397017645
  - 0.527009077301474
  - 0.48372093161566515
  - 0.5070102599419448
  - 0.48315654600204727
  - 0.4935601463231392
  - 0.4929489915134022
  - 0.5121425161197694
  - 0.4984749844044021
  TL_balanced_accuracy:
  - 0.5486725452901606
  - 0.5495630462678562
  - 0.5533977651199212
  - 0.5479034286035669
  - 0.548378978825901
  - 0.5599888220358749
  - 0.5672098693947376
  - 0.557250844319712
  - 0.5584565859296602
  - 0.5572408826898179
  - 0.5628739574927485
  - 0.5496548811121745
  - 0.5638747054388703
  - 0.5555760670398161
  - 0.5621421813065012
  - 0.5584342228374933
  TL_f1_macro:
  - 0.26990464525041685
  - 0.26828574802281574
  - 0.28238026401911975
  - 0.26666491600372305
  - 0.2643214690561096
  - 0.2824836485654042
  - 0.30344798372164394
  - 0.2798245892522161
  - 0.2887861525727836
  - 0.2822339287034552
  - 0.30013863475797165
  - 0.2716459746612553
  - 0.29354760223687115
  - 0.2744369857938947
  - 0.29407617678738796
  - 0.28235086279141436
  TL_f1_micro:
  - 0.2810451001240475
  - 0.2779796511627907
  - 0.292448467230444
  - 0.2776933579985906
  - 0.27470981747297535
  - 0.2887707011980268
  - 0.309526955602537
  - 0.28742732558139533
  - 0.2974149388623073
  - 0.29030126849894294
  - 0.30827167019027485
  - 0.28274753347427767
  - 0.297790649932865
  - 0.2796986011208685
  - 0.2992365738493447
  - 0.2874211199858788
  TL_f1_weighted:
  - 0.21625705712588064
  - 0.21724646234294767
  - 0.23240333934942875
  - 0.2127773559357784
  - 0.21131368432037143
  - 0.24101912757956515
  - 0.2646210944852189
  - 0.23470681822103295
  - 0.24239807512853123
  - 0.2363100857961962
  - 0.2561105845188291
  - 0.21827492372123286
  - 0.2595126974577654
  - 0.2353760187389774
  - 0.2571357241160026
  - 0.2446237305078366
  TL_matthews_corrcoef:
  - 0.1450071897333713
  - 0.1437828613477825
  - 0.15280228710556598
  - 0.14320984433274517
  - 0.14346066614840358
  - 0.15893481294829664
  - 0.17359927696879235
  - 0.1567064047172108
  - 0.16158785085695415
  - 0.1577375076216468
  - 0.1698806822157653
  - 0.14749312658248875
  - 0.15885031717525924
  - 0.14434340828758294
  - 0.15802562747424906
  - 0.14985648044959948
  TL_precision_macro:
  - 0.6080028019339101
  - 0.6042788568000275
  - 0.6093142142384398
  - 0.6070333591543905
  - 0.6063528170251762
  - 0.605270756741163
  - 0.6120992691828051
  - 0.6072337778210684
  - 0.6116667742792393
  - 0.6086684906901121
  - 0.6147511916735252
  - 0.6095271094292494
  - 0.5987614075607343
  - 0.5937229846668941
  - 0.6004635595887009
  - 0.5960779644284584
  TL_precision_micro:
  - 0.2810451001240475
  - 0.2779796511627907
  - 0.292448467230444
  - 0.2776933579985906
  - 0.27470981747297535
  - 0.2887707011980268
  - 0.309526955602537
  - 0.28742732558139533
  - 0.2974149388623073
  - 0.29030126849894294
  - 0.30827167019027485
  - 0.28274753347427767
  - 0.297790649932865
  - 0.2796986011208685
  - 0.2992365738493447
  - 0.2874211199858788
  TL_precision_weighted:
  - 0.8390160922331193
  - 0.840751468549138
  - 0.8351767616585297
  - 0.8407216732275183
  - 0.8446097018801022
  - 0.8481252500088242
  - 0.8425195925327345
  - 0.8464277170711582
  - 0.8407089550564409
  - 0.844262986178522
  - 0.8385490598921218
  - 0.840301324023554
  - 0.8396661613516281
  - 0.8411021229188181
  - 0.8359093415894336
  - 0.8393474462182363
  TL_recall_macro:
  - 0.5486725452901606
  - 0.5495630462678562
  - 0.5533977651199212
  - 0.5479034286035669
  - 0.548378978825901
  - 0.5599888220358749
  - 0.5672098693947376
  - 0.557250844319712
  - 0.5584565859296602
  - 0.5572408826898179
  - 0.5628739574927485
  - 0.5496548811121745
  - 0.5638747054388703
  - 0.5555760670398161
  - 0.5621421813065012
  - 0.5584342228374933
  TL_recall_micro:
  - 0.2810451001240475
  - 0.2779796511627907
  - 0.292448467230444
  - 0.2776933579985906
  - 0.27470981747297535
  - 0.2887707011980268
  - 0.309526955602537
  - 0.28742732558139533
  - 0.2974149388623073
  - 0.29030126849894294
  - 0.30827167019027485
  - 0.28274753347427767
  - 0.297790649932865
  - 0.2796986011208685
  - 0.2992365738493447
  - 0.2874211199858788
  TL_recall_weighted:
  - 0.2810451001240475
  - 0.2779796511627907
  - 0.292448467230444
  - 0.2776933579985906
  - 0.27470981747297535
  - 0.2887707011980268
  - 0.309526955602537
  - 0.28742732558139533
  - 0.2974149388623073
  - 0.29030126849894294
  - 0.30827167019027485
  - 0.28274753347427767
  - 0.297790649932865
  - 0.2796986011208685
  - 0.2992365738493447
  - 0.2874211199858788
  TL_roc_auc:
  - 0.8215710099382962
  - 0.8215381233565755
  - 0.8229176553172068
  - 0.8259446563636607
  - 0.8263287737620921
  - 0.8352974167390671
  - 0.8306975829374417
  - 0.8230545037566283
  - 0.8393223923120935
  - 0.8141577713665977
  - 0.8103975639178722
  - 0.8156335916191202
  - 0.8226873228449965
  - 0.8252811352726617
  - 0.8344849090400248
  - 0.826665358178146
  TT_average_precision:
  - 0.3184979013378097
  - 0.3075511924022909
  - 0.3172514977593361
  - 0.3128932627512723
  - 0.31597966399096605
  - 0.2974281273607622
  - 0.29439354108845744
  - 0.30622135671436235
  - 0.330821386202501
  - 0.3212072619057216
  - 0.3209284004233767
  - 0.3043231555591463
  - 0.29586565199130965
  - 0.30320949381313617
  - 0.2972397014186442
  - 0.29773701245800455
  TT_balanced_accuracy:
  - 0.5428564363238588
  - 0.5365145653786196
  - 0.5476274295966531
  - 0.5379896403426407
  - 0.5442570417753543
  - 0.5414171184490145
  - 0.5562931093926634
  - 0.5471496539989691
  - 0.5489681287766435
  - 0.5443773694431422
  - 0.5551515067010427
  - 0.5389182655560835
  - 0.5625534913917285
  - 0.5412957964171676
  - 0.5525664295672589
  - 0.5468729789331116
  TT_f1_macro:
  - 0.2596669105063454
  - 0.2695229265748457
  - 0.27049710091944773
  - 0.2611485615062208
  - 0.2587814285812159
  - 0.2744571138533359
  - 0.28182674351509535
  - 0.2793632649352707
  - 0.27366168517185285
  - 0.28653708637935005
  - 0.2845156085184646
  - 0.26429135981586516
  - 0.29352068212247195
  - 0.2779413565166091
  - 0.2741621077784066
  - 0.2795027911558195
  TT_f1_micro:
  - 0.27089864158829674
  - 0.2834928229665072
  - 0.2766812865497076
  - 0.2736576289207868
  - 0.2690700104493208
  - 0.2864167995746943
  - 0.2850877192982456
  - 0.2871477937267411
  - 0.2836990595611285
  - 0.2987772461456672
  - 0.28930754917597024
  - 0.27581738437001596
  - 0.29712752731793496
  - 0.2862279037251573
  - 0.2766736575784813
  - 0.28522920203735147
  TT_f1_weighted:
  - 0.20464572096968103
  - 0.21195836071300184
  - 0.22856247515525302
  - 0.20439138330856751
  - 0.20542224235173168
  - 0.22062614748018217
  - 0.25084104167027
  - 0.23435833569455752
  - 0.22274989174180457
  - 0.23401183457645275
  - 0.2480828514123129
  - 0.20976074935553204
  - 0.26190463614172876
  - 0.23194818694175529
  - 0.24617971903246944
  - 0.24000167531767938
  TT_matthews_corrcoef:
  - 0.12964457514128938
  - 0.1126967226862646
  - 0.12643307701350057
  - 0.1166328347354089
  - 0.13167358463145684
  - 0.12274375383966642
  - 0.13686071626055588
  - 0.12815810289754878
  - 0.14147958038245026
  - 0.1295714386869501
  - 0.13964213258344693
  - 0.1164335383786191
  - 0.15263230800713953
  - 0.11293177578765895
  - 0.12517375784264254
  - 0.12141085259049758
  TT_precision_macro:
  - 0.5980466255789001
  - 0.5869553777549641
  - 0.5839081759108048
  - 0.5895192611441266
  - 0.5979388374945144
  - 0.5909408335903382
  - 0.5831844956578005
  - 0.5870870618618846
  - 0.6021913240574018
  - 0.5945795003966853
  - 0.5883925315864683
  - 0.5870848730387873
  - 0.5931067991940528
  - 0.5772087421060859
  - 0.5745174714234713
  - 0.5786198971255195
  TT_precision_micro:
  - 0.27089864158829674
  - 0.2834928229665072
  - 0.2766812865497076
  - 0.2736576289207868
  - 0.2690700104493208
  - 0.2864167995746943
  - 0.2850877192982456
  - 0.2871477937267411
  - 0.2836990595611285
  - 0.2987772461456672
  - 0.28930754917597024
  - 0.27581738437001596
  - 0.29712752731793496
  - 0.2862279037251573
  - 0.2766736575784813
  - 0.28522920203735147
  TT_precision_weighted:
  - 0.8303516155173509
  - 0.7912571291337047
  - 0.821159254782045
  - 0.8087469752591383
  - 0.8352804562722707
  - 0.8017800851667746
  - 0.8294472445409619
  - 0.8099342978912986
  - 0.8306150769914925
  - 0.7963452638204845
  - 0.8251871105066013
  - 0.8058657999616002
  - 0.8340997891904972
  - 0.7911120533923255
  - 0.8270324864170683
  - 0.8060657114846504
  TT_recall_macro:
  - 0.5428564363238588
  - 0.5365145653786196
  - 0.5476274295966531
  - 0.5379896403426407
  - 0.5442570417753543
  - 0.5414171184490145
  - 0.5562931093926634
  - 0.5471496539989691
  - 0.5489681287766435
  - 0.5443773694431422
  - 0.5551515067010427
  - 0.5389182655560835
  - 0.5625534913917285
  - 0.5412957964171676
  - 0.5525664295672589
  - 0.5468729789331116
  TT_recall_micro:
  - 0.27089864158829674
  - 0.2834928229665072
  - 0.2766812865497076
  - 0.2736576289207868
  - 0.2690700104493208
  - 0.2864167995746943
  - 0.2850877192982456
  - 0.2871477937267411
  - 0.2836990595611285
  - 0.2987772461456672
  - 0.28930754917597024
  - 0.27581738437001596
  - 0.29712752731793496
  - 0.2862279037251573
  - 0.2766736575784813
  - 0.28522920203735147
  TT_recall_weighted:
  - 0.27089864158829674
  - 0.2834928229665072
  - 0.2766812865497076
  - 0.2736576289207868
  - 0.2690700104493208
  - 0.2864167995746943
  - 0.2850877192982456
  - 0.2871477937267411
  - 0.2836990595611285
  - 0.2987772461456672
  - 0.28930754917597024
  - 0.27581738437001596
  - 0.29712752731793496
  - 0.2862279037251573
  - 0.2766736575784813
  - 0.28522920203735147
  TT_roc_auc:
  - 0.6842797314894893
  - 0.6438510004196607
  - 0.6986897693199569
  - 0.6737785029137825
  - 0.6913090700105187
  - 0.6325458278310374
  - 0.6878399071967258
  - 0.6811659870501714
  - 0.6981323959577512
  - 0.6606378644551025
  - 0.7064049408641021
  - 0.6835383219403595
  - 0.6862262618547407
  - 0.6515310575797899
  - 0.6962164485180956
  - 0.675180816691814
  fit_time:
  - 323.13269543647766
  - 233.90452194213867
  - 246.17277646064758
  - 386.57726430892944
  - 279.614417552948
  - 372.21532678604126
  - 370.26943802833557
  - 321.1102669239044
  - 352.28747820854187
  - 265.2441647052765
  - 259.1531038284302
  - 345.9280118942261
  - 264.80672454833984
  - 316.9565522670746
  - 296.0484390258789
  - 237.83634853363037
  score_time:
  - 32.87892127037048
  - 46.621291399002075
  - 48.79145956039429
  - 28.207113027572632
  - 41.63572692871094
  - 33.0121214389801
  - 32.34591364860535
  - 34.530632734298706
  - 31.939838409423828
  - 44.69476556777954
  - 46.041444540023804
  - 29.298959255218506
  - 43.73568606376648
  - 34.70626163482666
  - 36.93605995178223
  - 52.642064809799194
start: 2023-08-13 18:42:07.824990
wrapper:
  call: bipartite_positive_dropper.wrap_estimator
  name: drop90
  params:
    drop: 0.9
    random_state: 0
