active: true
cv:
  call: bipartite_learn.model_selection.multipartite_cross_validate
  params:
    cv: 4
    diagonal: false
    n_jobs: 16
    pairwise: true
    random_state: 0
    return_estimator: false
    return_train_score: false
    scoring:
    - precision_weighted
    - recall_weighted
    - precision_micro
    - recall_macro
    - matthews_corrcoef
    - f1_micro
    - precision_macro
    - roc_auc
    - f1_weighted
    - balanced_accuracy
    - average_precision
    - recall_micro
    - f1_macro
    shuffle: true
    verbose: 10
dataset:
  X:
  - force_download: false
    path: datasets/ern/X1.txt
    read:
      call: data_loading.load_regulatory_network_features
      params: {}
    url: https://people.montefiore.uliege.be/schrynemackers/ern/X1.txt
  - force_download: false
    path: datasets/ern/X2.txt
    read:
      call: data_loading.load_regulatory_network_features
      params: {}
    url: https://people.montefiore.uliege.be/schrynemackers/ern/X2.txt
  name: ern
  pairwise: true
  y:
    force_download: false
    path: datasets/ern/Y.txt
    read:
      call: numpy.loadtxt
      params:
        delimiter: ','
    url: https://people.montefiore.uliege.be/schrynemackers/ern/Y.txt
directory: semisupervised_forests/runs
end: 2023-10-27 21:18:14.478870
estimator:
  call: semisupervised_forests.estimators.ds_bxt_gso
  final_params:
    memory: null
    steps:
    - - dropper
      - call: bipartite_positive_dropper.BipartitePositiveDropper
        params:
          drop: 0.5
          random_state: null
    - - estimator
      - call: bipartite_learn.ensemble._semisupervised_forest.BipartiteExtraTreesRegressorSS
        params:
          axis_decision_only: false
          bipartite_adapter: gmosa
          bootstrap: false
          ccp_alpha: 0.0
          criterion: squared_error_gso
          max_col_features: null
          max_depth: null
          max_features: 1.0
          max_leaf_nodes: null
          max_row_features: null
          max_samples: null
          min_col_weight_fraction_leaf: 0.0
          min_cols_leaf: 1
          min_cols_split: 1
          min_impurity_decrease: 0.0
          min_row_weight_fraction_leaf: 0.0
          min_rows_leaf: 1
          min_rows_split: 1
          min_samples_leaf: 1
          min_samples_split: 2
          min_weight_fraction_leaf: 0.0
          n_estimators: 100
          n_jobs: 3
          oob_score: false
          prediction_weights: null
          preprocess_X_targets: null
          random_state: 0
          ss_adapter: null
          supervision: 0.5
          unsupervised_criterion_cols: squared_error
          unsupervised_criterion_rows: squared_error
          update_supervision:
            load: semisupervised_forests.estimators.density_updater
          verbose: 10
          warm_start: false
    verbose: false
  name: ds_bxt_gso
  params: {}
hash: 02a0b8080b69a4e42402cb4b4cd7df4fe53c6e59c08f922fc74ecf896a3830d4
path: "/home/pedro/mestrado/disserta\xE7\xE3o/experiments/semisupervised_forests/runs/02a0b80_20231027T210650333107_ds_bxt_gso_ern.yml"
results:
  LL_average_precision:
  - 0.5095223865730365
  - 0.5107842944390422
  - 0.5102979928788673
  - 0.5066556069044515
  - 0.5096578689058979
  - 0.510803771170762
  - 0.5103654409046002
  - 0.5067938539321405
  - 0.509323173464814
  - 0.5106772027316135
  - 0.5099932851443694
  - 0.5069237739911575
  - 0.5095124259176254
  - 0.5105782160466159
  - 0.5102006556859028
  - 0.5067346052059881
  LL_balanced_accuracy:
  - 0.75
  - 0.7501179801793298
  - 0.7501227295041728
  - 0.75
  - 0.750132485426603
  - 0.7501177578897786
  - 0.750121891760117
  - 0.75
  - 0.75
  - 0.7501192179303767
  - 0.75
  - 0.7501883948756594
  - 0.75
  - 0.75
  - 0.75
  - 0.75
  LL_f1_macro:
  - 0.8309182405189923
  - 0.8307586814018457
  - 0.8308907741279159
  - 0.8316526526826057
  - 0.8310692710241894
  - 0.8307533419130508
  - 0.8308722135951685
  - 0.8316173829761762
  - 0.830969482074128
  - 0.8307880564775882
  - 0.830796992481203
  - 0.8318477578032308
  - 0.8309208033345965
  - 0.8306461407663812
  - 0.8307435430737526
  - 0.8316325003740834
  LL_f1_micro:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_f1_weighted:
  - 0.988913546445512
  - 0.987722980614685
  - 0.9882987906880089
  - 0.9922463112278831
  - 0.9890650969602176
  - 0.9876998468075384
  - 0.9882185032999534
  - 0.9920854949952507
  - 0.989145002886385
  - 0.9878502202741921
  - 0.9883665137089277
  - 0.992373769952719
  - 0.9889251187756575
  - 0.9876871736491468
  - 0.9881256525918501
  - 0.9921544150315158
  LL_matthews_corrcoef:
  - 0.7036995331209558
  - 0.7034934238032008
  - 0.7036800473667042
  - 0.7047339211336413
  - 0.7039328307015822
  - 0.7034858763540875
  - 0.7036537950796627
  - 0.7046842107963786
  - 0.7037716557806432
  - 0.7035349496681379
  - 0.7035289052508503
  - 0.7050384450672653
  - 0.7037031401185525
  - 0.7033166743918887
  - 0.7034537008880977
  - 0.7047055173504467
  LL_precision_macro:
  - 0.9951930329146512
  - 0.9946695525242866
  - 0.9949226426198765
  - 0.9966498995963974
  - 0.9952589717549346
  - 0.9946593780516042
  - 0.9948873725584086
  - 0.9965798369457148
  - 0.9952945434802281
  - 0.9947255048014012
  - 0.99495292052346
  - 0.996704902389567
  - 0.9951981094127111
  - 0.9946543444776661
  - 0.994847109293161
  - 0.9966098661841608
  LL_precision_micro:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_precision_weighted:
  - 0.9905691610246246
  - 0.9895641206011611
  - 0.9900495467435512
  - 0.993388986998302
  - 0.9906961659372497
  - 0.9895446267221922
  - 0.9899818391422291
  - 0.9932526182442893
  - 0.990764566109916
  - 0.9896713515806191
  - 0.9901075886643413
  - 0.9934961616644096
  - 0.9905789293393269
  - 0.9895348789514361
  - 0.9899044700418723
  - 0.9933110572197021
  LL_recall_macro:
  - 0.75
  - 0.7501179801793298
  - 0.7501227295041728
  - 0.75
  - 0.750132485426603
  - 0.7501177578897786
  - 0.750121891760117
  - 0.75
  - 0.75
  - 0.7501192179303767
  - 0.75
  - 0.7501883948756594
  - 0.75
  - 0.75
  - 0.75
  - 0.75
  LL_recall_micro:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_recall_weighted:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_roc_auc:
  - 0.75
  - 0.7501179801793298
  - 0.7501227295041728
  - 0.75
  - 0.750132485426603
  - 0.7501177578897786
  - 0.750121891760117
  - 0.75
  - 0.75
  - 0.7501192179303767
  - 0.75
  - 0.7501883948756594
  - 0.75
  - 0.75
  - 0.75
  - 0.75
  LT_average_precision:
  - 0.06329322308755114
  - 0.14818684760695858
  - 0.021261579108120234
  - 0.03441073390488432
  - 0.05466591856092784
  - 0.1555877268885821
  - 0.02263208115053572
  - 0.03454824574045992
  - 0.05163272270602582
  - 0.13775625783723733
  - 0.023413659717834573
  - 0.03555561533833244
  - 0.05524306998735201
  - 0.1490135627130668
  - 0.020334016643923036
  - 0.035452403721919275
  LT_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_roc_auc:
  - 0.5944331275676275
  - 0.6005151596832751
  - 0.5628526278722028
  - 0.5023506420683874
  - 0.5776979060214003
  - 0.6150893330296259
  - 0.5736154690506539
  - 0.5114950924983335
  - 0.5942992064817592
  - 0.6186278120824131
  - 0.5923220238039327
  - 0.5144807630714509
  - 0.5746238501341063
  - 0.5966942351654224
  - 0.5787651331158532
  - 0.5102630453972608
  TL_average_precision:
  - 0.3260212967764859
  - 0.35579536242033855
  - 0.34166821518560386
  - 0.2315221508957474
  - 0.3698168980843257
  - 0.35722550081239735
  - 0.3774400820284749
  - 0.18462858213245711
  - 0.35734070823370767
  - 0.37178423779263575
  - 0.361942270185636
  - 0.18101219872692506
  - 0.33859681662192476
  - 0.3677338856129759
  - 0.41601823141481825
  - 0.1884009605144629
  TL_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_roc_auc:
  - 0.7810696068604522
  - 0.7863197862434808
  - 0.7668257256466102
  - 0.7304051290991648
  - 0.7974793174028941
  - 0.7893985431390498
  - 0.7899071974849612
  - 0.7053800361928907
  - 0.7932743461661056
  - 0.7960273465560355
  - 0.7946185185702338
  - 0.7047183471005383
  - 0.7870424581254771
  - 0.7884278701059788
  - 0.8094577711487612
  - 0.7142539621281501
  TT_average_precision:
  - 0.03379567407844992
  - 0.029600181804655703
  - 0.024666013904615192
  - 0.03301396433094659
  - 0.024889593137524785
  - 0.07603924722534645
  - 0.02358214448491406
  - 0.0353454592445715
  - 0.02750084631742602
  - 0.05699874479329703
  - 0.020898592251749108
  - 0.036770625760317845
  - 0.02925082964818246
  - 0.057271448473378936
  - 0.03034345635262572
  - 0.03535081298657127
  TT_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_roc_auc:
  - 0.5699764491443536
  - 0.599027589664539
  - 0.6319967938584083
  - 0.5043470766189113
  - 0.5460225256499267
  - 0.6199187584610615
  - 0.5976075628186891
  - 0.5074001516145242
  - 0.5697505047787534
  - 0.631270295292183
  - 0.5595194652926578
  - 0.5286641080844957
  - 0.5544911326865777
  - 0.6231128936070667
  - 0.6664664414791597
  - 0.524621360031403
  fit_time:
  - 634.8247029781342
  - 674.680722951889
  - 645.6539964675903
  - 561.5716648101807
  - 637.1032438278198
  - 657.3780069351196
  - 627.8617014884949
  - 588.0488891601562
  - 636.9497108459473
  - 679.6370191574097
  - 665.7755887508392
  - 518.9766013622284
  - 623.6865019798279
  - 258.0304262638092
  - 235.30955243110657
  - 194.80804681777954
  score_time:
  - 5.995865106582642
  - 4.325115442276001
  - 5.03710675239563
  - 11.522613525390625
  - 6.078393220901489
  - 4.7248780727386475
  - 6.723026990890503
  - 10.983856201171875
  - 6.042934894561768
  - 4.279034614562988
  - 4.765404462814331
  - 11.882123708724976
  - 8.939163446426392
  - 4.4128804206848145
  - 4.489672660827637
  - 4.610115051269531
start: 2023-10-27 21:06:50.333107
wrapper:
  call: bipartite_positive_dropper.wrap_estimator
  name: drop50
  params:
    drop: 0.5
