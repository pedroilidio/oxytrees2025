active: true
cv:
  call: bipartite_learn.model_selection.multipartite_cross_validate
  params:
    cv: 4
    diagonal: false
    n_jobs: 16
    pairwise: true
    random_state: 0
    return_estimator: false
    return_train_score: false
    scoring:
    - precision_weighted
    - recall_weighted
    - precision_micro
    - recall_macro
    - matthews_corrcoef
    - f1_micro
    - precision_macro
    - roc_auc
    - f1_weighted
    - balanced_accuracy
    - average_precision
    - recall_micro
    - f1_macro
    shuffle: true
    verbose: 10
dataset:
  X:
  - force_download: false
    path: datasets/ern/X1.txt
    read:
      call: data_loading.load_regulatory_network_features
      params: {}
    url: https://people.montefiore.uliege.be/schrynemackers/ern/X1.txt
  - force_download: false
    path: datasets/ern/X2.txt
    read:
      call: data_loading.load_regulatory_network_features
      params: {}
    url: https://people.montefiore.uliege.be/schrynemackers/ern/X2.txt
  name: ern
  pairwise: true
  y:
    force_download: false
    path: datasets/ern/Y.txt
    read:
      call: numpy.loadtxt
      params:
        delimiter: ','
    url: https://people.montefiore.uliege.be/schrynemackers/ern/Y.txt
directory: semisupervised_forests/runs
end: 2023-10-27 22:16:37.286549
estimator:
  call: semisupervised_forests.estimators.md_ds_bxt_gso
  final_params:
    memory: null
    steps:
    - - dropper
      - call: bipartite_positive_dropper.BipartitePositiveDropper
        params:
          drop: 0.5
          random_state: null
    - - estimator
      - call: bipartite_learn.ensemble._semisupervised_forest.BipartiteExtraTreesRegressorSS
        params:
          axis_decision_only: false
          bipartite_adapter: gmosa
          bootstrap: false
          ccp_alpha: 0.0
          criterion: squared_error_gso
          max_col_features: null
          max_depth: null
          max_features: 1.0
          max_leaf_nodes: null
          max_row_features: null
          max_samples: null
          min_col_weight_fraction_leaf: 0.0
          min_cols_leaf: 1
          min_cols_split: 1
          min_impurity_decrease: 0.0
          min_row_weight_fraction_leaf: 0.0
          min_rows_leaf: 1
          min_rows_split: 1
          min_samples_leaf: 1
          min_samples_split: 2
          min_weight_fraction_leaf: 0.0
          n_estimators: 100
          n_jobs: 3
          oob_score: false
          prediction_weights: null
          preprocess_X_targets: null
          random_state: 0
          ss_adapter: null
          supervision: 0.5
          unsupervised_criterion_cols: mean_distance
          unsupervised_criterion_rows: mean_distance
          update_supervision:
            load: semisupervised_forests.estimators.node_size_updater
          verbose: 10
          warm_start: false
    verbose: false
  name: md_ds_bxt_gso
  params: {}
hash: dc70e96ea567b779fc64280e4972eb2da46f7fd68e96d02aceb9c36eafc760ff
path: "/home/pedro/mestrado/disserta\xE7\xE3o/experiments/semisupervised_forests/runs/dc70e96_20231027T215320776554_md_ds_bxt_gso_ern.yml"
results:
  LL_average_precision:
  - 0.5095223865730365
  - 0.5107842944390422
  - 0.5102979928788673
  - 0.5066556069044515
  - 0.5096578689058979
  - 0.510803771170762
  - 0.5103654409046002
  - 0.5067938539321405
  - 0.509323173464814
  - 0.5106772027316135
  - 0.5099932851443694
  - 0.5069237739911575
  - 0.5095124259176254
  - 0.5105782160466159
  - 0.5102006556859028
  - 0.5067346052059881
  LL_balanced_accuracy:
  - 0.75
  - 0.7501179801793298
  - 0.7501227295041728
  - 0.75
  - 0.750132485426603
  - 0.7501177578897786
  - 0.750121891760117
  - 0.75
  - 0.75
  - 0.7501192179303767
  - 0.75
  - 0.7501883948756594
  - 0.75
  - 0.75
  - 0.75
  - 0.75
  LL_f1_macro:
  - 0.8309182405189923
  - 0.8307586814018457
  - 0.8308907741279159
  - 0.8316526526826057
  - 0.8310692710241894
  - 0.8307533419130508
  - 0.8308722135951685
  - 0.8316173829761762
  - 0.830969482074128
  - 0.8307880564775882
  - 0.830796992481203
  - 0.8318477578032308
  - 0.8309208033345965
  - 0.8306461407663812
  - 0.8307435430737526
  - 0.8316325003740834
  LL_f1_micro:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_f1_weighted:
  - 0.988913546445512
  - 0.987722980614685
  - 0.9882987906880089
  - 0.9922463112278831
  - 0.9890650969602176
  - 0.9876998468075384
  - 0.9882185032999534
  - 0.9920854949952507
  - 0.989145002886385
  - 0.9878502202741921
  - 0.9883665137089277
  - 0.992373769952719
  - 0.9889251187756575
  - 0.9876871736491468
  - 0.9881256525918501
  - 0.9921544150315158
  LL_matthews_corrcoef:
  - 0.7036995331209558
  - 0.7034934238032008
  - 0.7036800473667042
  - 0.7047339211336413
  - 0.7039328307015822
  - 0.7034858763540875
  - 0.7036537950796627
  - 0.7046842107963786
  - 0.7037716557806432
  - 0.7035349496681379
  - 0.7035289052508503
  - 0.7050384450672653
  - 0.7037031401185525
  - 0.7033166743918887
  - 0.7034537008880977
  - 0.7047055173504467
  LL_precision_macro:
  - 0.9951930329146512
  - 0.9946695525242866
  - 0.9949226426198765
  - 0.9966498995963974
  - 0.9952589717549346
  - 0.9946593780516042
  - 0.9948873725584086
  - 0.9965798369457148
  - 0.9952945434802281
  - 0.9947255048014012
  - 0.99495292052346
  - 0.996704902389567
  - 0.9951981094127111
  - 0.9946543444776661
  - 0.994847109293161
  - 0.9966098661841608
  LL_precision_micro:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_precision_weighted:
  - 0.9905691610246246
  - 0.9895641206011611
  - 0.9900495467435512
  - 0.993388986998302
  - 0.9906961659372497
  - 0.9895446267221922
  - 0.9899818391422291
  - 0.9932526182442893
  - 0.990764566109916
  - 0.9896713515806191
  - 0.9901075886643413
  - 0.9934961616644096
  - 0.9905789293393269
  - 0.9895348789514361
  - 0.9899044700418723
  - 0.9933110572197021
  LL_recall_macro:
  - 0.75
  - 0.7501179801793298
  - 0.7501227295041728
  - 0.75
  - 0.750132485426603
  - 0.7501177578897786
  - 0.750121891760117
  - 0.75
  - 0.75
  - 0.7501192179303767
  - 0.75
  - 0.7501883948756594
  - 0.75
  - 0.75
  - 0.75
  - 0.75
  LL_recall_micro:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_recall_weighted:
  - 0.9904776134269635
  - 0.9894516659196175
  - 0.9899474661294783
  - 0.9933443930955485
  - 0.9906071019473082
  - 0.9894317446087952
  - 0.9898783426156338
  - 0.9932061460678595
  - 0.990676826535186
  - 0.9895612331291399
  - 0.9900067148556306
  - 0.9934530157601612
  - 0.9904875740823746
  - 0.9894217839533841
  - 0.9897993443140972
  - 0.9932653947940119
  LL_roc_auc:
  - 0.75
  - 0.7501179801793298
  - 0.7501227295041728
  - 0.75
  - 0.750132485426603
  - 0.7501177578897786
  - 0.750121891760117
  - 0.75
  - 0.75
  - 0.7501192179303767
  - 0.75
  - 0.7501883948756594
  - 0.75
  - 0.75
  - 0.75
  - 0.75
  LT_average_precision:
  - 0.05367403193845703
  - 0.16740833237838465
  - 0.02852860985682141
  - 0.03540601706790237
  - 0.06838556249083977
  - 0.10488751760737507
  - 0.02393856623147661
  - 0.03614047650847331
  - 0.052298042615012495
  - 0.11979782091787523
  - 0.023683084024767726
  - 0.03432418984685471
  - 0.04776498339977999
  - 0.16879023127873277
  - 0.02348839573755794
  - 0.03528336746458794
  LT_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_roc_auc:
  - 0.5675471344956892
  - 0.6098880929868566
  - 0.5942809956700381
  - 0.5075214716773724
  - 0.5650347936114265
  - 0.6029084051720545
  - 0.5623888000018267
  - 0.5140241451473558
  - 0.5661555656709756
  - 0.6137693166853395
  - 0.582935921113809
  - 0.5074364223656875
  - 0.571155035682971
  - 0.5993222850539597
  - 0.5927532083710527
  - 0.5162327268891468
  TL_average_precision:
  - 0.35933571050822544
  - 0.343925653714761
  - 0.3428637577678705
  - 0.24822596111796297
  - 0.3998316202044986
  - 0.38481291478712326
  - 0.3563743401526023
  - 0.24739057250170482
  - 0.37963204873879186
  - 0.370091679602533
  - 0.3704149276092456
  - 0.19996559555649368
  - 0.3536514972420364
  - 0.37636348788307666
  - 0.3720291677083612
  - 0.26979318714234835
  TL_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_roc_auc:
  - 0.7813433567212384
  - 0.7876880476230353
  - 0.7734767336738977
  - 0.7355153488618441
  - 0.7923637899847678
  - 0.801739869844241
  - 0.7834042143470518
  - 0.7262748453674729
  - 0.7879417464885728
  - 0.7833656282077265
  - 0.7847277493084355
  - 0.7327855977324033
  - 0.7895745769917288
  - 0.793894260138316
  - 0.8046362143919251
  - 0.7542600130752186
  TT_average_precision:
  - 0.02571251015938321
  - 0.05285876510261453
  - 0.025385863464405828
  - 0.0343642084310579
  - 0.03566787699357476
  - 0.025496509196210677
  - 0.02710608057902801
  - 0.036843121274484567
  - 0.04202087046167706
  - 0.0666647893151392
  - 0.026719824221125264
  - 0.035872126586662695
  - 0.030624549959916328
  - 0.03559640528703834
  - 0.0331860495384539
  - 0.036269578227358115
  TT_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_roc_auc:
  - 0.5500438256766187
  - 0.5939245867536685
  - 0.630946685415384
  - 0.5199577867428999
  - 0.5461220396513152
  - 0.6223845535168853
  - 0.5920651151942261
  - 0.5257156972138702
  - 0.5818054094890253
  - 0.6213440837927842
  - 0.5808524448422151
  - 0.5196889762551093
  - 0.5525380933226832
  - 0.5957819753745384
  - 0.6125800611102861
  - 0.5313443452937562
  fit_time:
  - 1258.059958934784
  - 1391.2517201900482
  - 1301.8929271697998
  - 1126.0958263874054
  - 1338.1699044704437
  - 1300.1076049804688
  - 1362.5448324680328
  - 1080.909380197525
  - 1252.2175068855286
  - 1389.4073803424835
  - 1351.063711643219
  - 1097.6377403736115
  - 477.8814706802368
  - 548.6857671737671
  - 525.4826664924622
  - 1036.766627073288
  score_time:
  - 8.792453289031982
  - 5.002403259277344
  - 6.099347114562988
  - 9.049514532089233
  - 5.442026138305664
  - 6.225281715393066
  - 5.131033420562744
  - 9.399428606033325
  - 8.878764152526855
  - 5.116999864578247
  - 5.112915515899658
  - 9.255956172943115
  - 5.229127883911133
  - 5.097910165786743
  - 5.015683174133301
  - 10.121978044509888
start: 2023-10-27 21:53:20.776554
wrapper:
  call: bipartite_positive_dropper.wrap_estimator
  name: drop50
  params:
    drop: 0.5
