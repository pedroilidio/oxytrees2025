active: true
cv:
  call: bipartite_learn.model_selection.multipartite_cross_validate
  params:
    cv: 4
    diagonal: false
    n_jobs: 16
    pairwise: true
    random_state: 0
    return_estimator: false
    return_train_score: false
    scoring:
    - recall_micro
    - f1_micro
    - precision_micro
    - f1_weighted
    - average_precision
    - recall_macro
    - roc_auc
    - matthews_corrcoef
    - precision_macro
    - balanced_accuracy
    - precision_weighted
    - recall_weighted
    - f1_macro
    shuffle: true
    verbose: 10
dataset:
  X:
  - force_download: false
    path: datasets/srn/X1.txt
    read:
      call: data_loading.load_regulatory_network_features
      params: {}
    url: https://people.montefiore.uliege.be/schrynemackers/srn/X1.txt
  - force_download: false
    path: datasets/srn/X2.txt
    read:
      call: data_loading.load_regulatory_network_features
      params: {}
    url: https://people.montefiore.uliege.be/schrynemackers/srn/X2.txt
  name: srn
  pairwise: true
  y:
    force_download: false
    path: datasets/srn/Y.txt
    read:
      call: numpy.loadtxt
      params:
        delimiter: ','
    url: https://people.montefiore.uliege.be/schrynemackers/srn/Y.txt
directory: semisupervised_forests/runs
end: 2023-10-30 22:43:56.749932
estimator:
  call: semisupervised_forests.estimators.md_ss_bxt_gso
  final_params:
    memory: null
    steps:
    - - dropper
      - call: bipartite_positive_dropper.BipartitePositiveDropper
        params:
          drop: 0.7
          random_state: null
    - - estimator
      - call: bipartite_learn.ensemble._semisupervised_forest.BipartiteExtraTreesRegressorSS
        params:
          axis_decision_only: false
          bipartite_adapter: gmosa
          bootstrap: false
          ccp_alpha: 0.0
          criterion: squared_error_gso
          max_col_features: null
          max_depth: null
          max_features: 1.0
          max_leaf_nodes: null
          max_row_features: null
          max_samples: null
          min_col_weight_fraction_leaf: 0.0
          min_cols_leaf: 1
          min_cols_split: 1
          min_impurity_decrease: 0.0
          min_row_weight_fraction_leaf: 0.0
          min_rows_leaf: 1
          min_rows_split: 1
          min_samples_leaf: 1
          min_samples_split: 2
          min_weight_fraction_leaf: 0.0
          n_estimators: 100
          n_jobs: 3
          oob_score: false
          prediction_weights: null
          preprocess_X_targets: null
          random_state: 0
          ss_adapter: null
          supervision: 0.5
          unsupervised_criterion_cols: mean_distance
          unsupervised_criterion_rows: mean_distance
          update_supervision: null
          verbose: 10
          warm_start: false
    verbose: false
  name: md_ss_bxt_gso
  params: {}
hash: f88766a9debdcece76344fd2ad69748a4f5cbff854ead4654510481a46039713
path: "/home/pedro/mestrado/disserta\xE7\xE3o/experiments/semisupervised_forests/runs/f88766a_20231030T211949829573_md_ss_bxt_gso_srn.yml"
results:
  LL_average_precision:
  - 0.31043956043956045
  - 0.3135321585011493
  - 0.31393697904693796
  - 0.31289824492932156
  - 0.3104626920106975
  - 0.31309691275241186
  - 0.31374651818975763
  - 0.3122514757489135
  - 0.31050772523023307
  - 0.31332852192953764
  - 0.3140485358674154
  - 0.31274634668205165
  - 0.3108996636445437
  - 0.31333143351602716
  - 0.31392743184478733
  - 0.31294731263378284
  LL_balanced_accuracy:
  - 0.65
  - 0.6501597444089458
  - 0.6500217108119843
  - 0.6502177068214804
  - 0.6500589622641509
  - 0.650093370681606
  - 0.6500219973603167
  - 0.6500496031746031
  - 0.6500291885580852
  - 0.6500455373406193
  - 0.6500868809730669
  - 0.6501204819277109
  - 0.6502033701336433
  - 0.6500685244403838
  - 0.6500435540069687
  - 0.6502166586422725
  LL_f1_macro:
  - 0.7281337930487852
  - 0.7276140047113996
  - 0.7272761330148422
  - 0.7278745634629974
  - 0.7282277202975188
  - 0.7276129923249521
  - 0.7273254783927879
  - 0.7277551796969631
  - 0.7281658357729319
  - 0.7274725625454239
  - 0.727358050792975
  - 0.7277486910318385
  - 0.72836076529905
  - 0.7275107921372921
  - 0.7273156338691005
  - 0.7278602308042376
  LL_f1_micro:
  - 0.9895604395604396
  - 0.986787330316742
  - 0.9861064425770308
  - 0.9875371687136393
  - 0.9896552325176045
  - 0.9870898286108001
  - 0.9862974765308758
  - 0.9878477306002929
  - 0.9895506518859374
  - 0.986762552751701
  - 0.9861252260787184
  - 0.9874946171733701
  - 0.9895070766227428
  - 0.9868056153647403
  - 0.9861596761691499
  - 0.9874860046507622
  LL_f1_weighted:
  - 0.9867773014842327
  - 0.9832767564481393
  - 0.9824151148968876
  - 0.9842242865375411
  - 0.9868978376855441
  - 0.9836576134985787
  - 0.9826562370890688
  - 0.9846140199382498
  - 0.9867652915002223
  - 0.9832436907593204
  - 0.9824398921378842
  - 0.9841691217693783
  - 0.9867123798169393
  - 0.9832984138780857
  - 0.9824826606909075
  - 0.9841596703263348
  LL_matthews_corrcoef:
  - 0.5448431486319819
  - 0.5443609417010167
  - 0.5439207436585922
  - 0.5446751228599022
  - 0.5449765507868785
  - 0.5443249928425076
  - 0.5439745794585498
  - 0.5444568100737082
  - 0.5448934338702066
  - 0.544147002223111
  - 0.5440441015538537
  - 0.5444869889588942
  - 0.5451975133055292
  - 0.5442006887412761
  - 0.5439751945855913
  - 0.5446589593200364
  LL_precision_macro:
  - 0.9947567610186865
  - 0.9933559856804806
  - 0.9930116010890111
  - 0.9937350525111781
  - 0.9948045695277279
  - 0.9935089679302311
  - 0.993108257961673
  - 0.9938920393056578
  - 0.9947518165105489
  - 0.9933434963750856
  - 0.9930210797182439
  - 0.9937135781516535
  - 0.9947297924308613
  - 0.9933652655193022
  - 0.9930385285173409
  - 0.99370919418804
  LL_precision_micro:
  - 0.9895604395604396
  - 0.986787330316742
  - 0.9861064425770308
  - 0.9875371687136393
  - 0.9896552325176045
  - 0.9870898286108001
  - 0.9862974765308758
  - 0.9878477306002929
  - 0.9895506518859374
  - 0.986762552751701
  - 0.9861252260787184
  - 0.9874946171733701
  - 0.9895070766227428
  - 0.9868056153647403
  - 0.9861596761691499
  - 0.9874860046507622
  LL_precision_weighted:
  - 0.9896699137809285
  - 0.9869629006498913
  - 0.9863006300201597
  - 0.9876933266807815
  - 0.9897627235580176
  - 0.9872574292838272
  - 0.9864863450449225
  - 0.9879961817679739
  - 0.989660332078433
  - 0.9869387829828868
  - 0.9863188879609665
  - 0.9876518453970167
  - 0.9896176763911533
  - 0.9869806978421226
  - 0.9863523742084683
  - 0.9876434508801099
  LL_recall_macro:
  - 0.65
  - 0.6501597444089458
  - 0.6500217108119843
  - 0.6502177068214804
  - 0.6500589622641509
  - 0.650093370681606
  - 0.6500219973603167
  - 0.6500496031746031
  - 0.6500291885580852
  - 0.6500455373406193
  - 0.6500868809730669
  - 0.6501204819277109
  - 0.6502033701336433
  - 0.6500685244403838
  - 0.6500435540069687
  - 0.6502166586422725
  LL_recall_micro:
  - 0.9895604395604396
  - 0.986787330316742
  - 0.9861064425770308
  - 0.9875371687136393
  - 0.9896552325176045
  - 0.9870898286108001
  - 0.9862974765308758
  - 0.9878477306002929
  - 0.9895506518859374
  - 0.986762552751701
  - 0.9861252260787184
  - 0.9874946171733701
  - 0.9895070766227428
  - 0.9868056153647403
  - 0.9861596761691499
  - 0.9874860046507622
  LL_recall_weighted:
  - 0.9895604395604396
  - 0.986787330316742
  - 0.9861064425770308
  - 0.9875371687136393
  - 0.9896552325176045
  - 0.9870898286108001
  - 0.9862974765308758
  - 0.9878477306002929
  - 0.9895506518859374
  - 0.986762552751701
  - 0.9861252260787184
  - 0.9874946171733701
  - 0.9895070766227428
  - 0.9868056153647403
  - 0.9861596761691499
  - 0.9874860046507622
  LL_roc_auc:
  - 0.65
  - 0.6501597444089458
  - 0.6500217108119843
  - 0.6502177068214804
  - 0.6500589622641509
  - 0.650093370681606
  - 0.6500219973603167
  - 0.6500496031746031
  - 0.6500291885580852
  - 0.6500455373406193
  - 0.6500868809730669
  - 0.6501204819277109
  - 0.6502033701336433
  - 0.6500685244403838
  - 0.6500435540069687
  - 0.6502166586422725
  LT_average_precision:
  - 0.030214646012135617
  - 0.016948146625636477
  - 0.013366798192966163
  - 0.02262300099408691
  - 0.028185229012837758
  - 0.01778045713118408
  - 0.015563286150949575
  - 0.02033852107855421
  - 0.028117182129735234
  - 0.017632499725183347
  - 0.013469609383699333
  - 0.020161584722173942
  - 0.02806973795677226
  - 0.01895366364722327
  - 0.015744324009498874
  - 0.021023846510493226
  LT_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  LT_roc_auc:
  - 0.529037626588981
  - 0.5204820120565888
  - 0.5296257139664352
  - 0.542716512780114
  - 0.5198095978414647
  - 0.5186336360181711
  - 0.5544117286682447
  - 0.5284686644292556
  - 0.5164970880381344
  - 0.5274583491419641
  - 0.5364364290941223
  - 0.5262768293803458
  - 0.521854946137494
  - 0.5222653845782629
  - 0.556125343963363
  - 0.5360520189792268
  TL_average_precision:
  - 0.07138915060724849
  - 0.08608851711867638
  - 0.07464170625711708
  - 0.0875390139284079
  - 0.09143392402690156
  - 0.10285263838566305
  - 0.10294614090002747
  - 0.10085887807680328
  - 0.0668495881092307
  - 0.0792693718367553
  - 0.09605084498603608
  - 0.09769439424459848
  - 0.06376508882915068
  - 0.08032060855450449
  - 0.0828547334750594
  - 0.07203398216316262
  TL_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TL_roc_auc:
  - 0.5788504992007767
  - 0.5874491653700485
  - 0.577387935188033
  - 0.5886618636480602
  - 0.5786763793264099
  - 0.5896490519749962
  - 0.5898306361189699
  - 0.5835392067147329
  - 0.5807539337094307
  - 0.5829497888158663
  - 0.5806830516342898
  - 0.5877352744238287
  - 0.5696382885471221
  - 0.5785358227911448
  - 0.5705556505144082
  - 0.5718771431798965
  TT_average_precision:
  - 0.026460184791325284
  - 0.01590345380464698
  - 0.012480969405434089
  - 0.019308101245848825
  - 0.030681652653766794
  - 0.01750698242177991
  - 0.014804753839825475
  - 0.02029235501772976
  - 0.027369608101496877
  - 0.017383431905195118
  - 0.011396219744328282
  - 0.021352810096661914
  - 0.026474091208964923
  - 0.018350396675298072
  - 0.01172462205634879
  - 0.019484046684343392
  TT_balanced_accuracy:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_f1_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_matthews_corrcoef:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_precision_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_macro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_micro:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_recall_weighted:
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  - .nan
  TT_roc_auc:
  - 0.5145010770447224
  - 0.5163824349858277
  - 0.5261564996664334
  - 0.5121005764011304
  - 0.5261430904588799
  - 0.5319517718599287
  - 0.5190202490092958
  - 0.5326040751098683
  - 0.5236233853729443
  - 0.5405209484579611
  - 0.5096173905901357
  - 0.5456097583765662
  - 0.5140847722308879
  - 0.5346318361372124
  - 0.5166541950113378
  - 0.5247641815689152
  fit_time:
  - 4226.6088218688965
  - 4905.509566545486
  - 5038.898396015167
  - 4760.891098976135
  - 4339.14710855484
  - 4834.445700883865
  - 5028.475486755371
  - 4706.882814645767
  - 4297.3345079422
  - 4936.055157899857
  - 4406.376200914383
  - 4116.51970410347
  - 3538.538699865341
  - 4443.826879739761
  - 4404.691036224365
  - 4217.449970245361
  score_time:
  - 9.481980800628662
  - 6.5817506313323975
  - 6.611377954483032
  - 6.927478551864624
  - 7.547494888305664
  - 7.35842227935791
  - 6.551785707473755
  - 7.087273120880127
  - 8.477734804153442
  - 6.528011798858643
  - 7.679070234298706
  - 10.01727032661438
  - 10.684452056884766
  - 7.046945571899414
  - 7.808194398880005
  - 8.22440767288208
start: 2023-10-30 21:19:49.829573
wrapper:
  call: bipartite_positive_dropper.wrap_estimator
  name: drop70
  params:
    drop: 0.7
