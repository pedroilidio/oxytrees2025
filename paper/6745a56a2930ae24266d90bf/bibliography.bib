
@article{yamanishi_prediction_2008,
	title = {Prediction of drug–target interaction networks from the integration of chemical and genomic spaces},
	volume = {24},
	number = {13},
	journal = {Bioinformatics},
	author = {Yamanishi, Yoshihiro and Araki, Michihiro and Gutteridge, Alex and Honda, Wataru and Kanehisa, Minoru},
	year = {2008},
	note = {Publisher: Oxford University Press},
	pages = {i232--i240},
}

@misc{landrum_rdkitrdkit_2023,
	title = {rdkit/rdkit: 2023\_03\_2 ({Q1} 2023) {Release}},
	url = {https://doi.org/10.5281/zenodo.8053810},
	publisher = {Zenodo},
	author = {Landrum, Greg and Tosco, Paolo and Kelley, Brian and {Ric} and Cosgrove, David and {sriniker} and {gedeck} and Vianello, Riccardo and {NadineSchneider} and Kawashima, Eisuke and N, Dan and Jones, Gareth and Dalke, Andrew and Cole, Brian and Swain, Matt and Turk, Samo and {AlexanderSavelyev} and Vaucher, Alain and Wójcikowski, Maciej and Take, Ichiru and Probst, Daniel and Ujihara, Kazuya and Scalfani, Vincent F. and godin, guillaume and Lehtivarjo, Juuso and Pahl, Axel and Walker, Rachel and Berenger, Francois and {jasondbiggs} and {strets123}},
	month = jun,
	year = {2023},
	doi = {10.5281/zenodo.8053810},
}

@article{liu_predicting_2020,
	title = {Predicting {lncRNA}–{miRNA} interactions based on logistic matrix factorization with neighborhood regularized},
	volume = {191},
	journal = {Knowledge-Based Systems},
	author = {Liu, Hongsheng and Ren, Guofei and Chen, Haoyu and Liu, Qi and Yang, Yingjuan and Zhao, Qi},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {105261},
}

@article{liu_lpi-nrlmf_2017,
	title = {{LPI}-{NRLMF}: {lncRNA}-protein interaction prediction by neighborhood regularized logistic matrix factorization},
	volume = {8},
	number = {61},
	journal = {Oncotarget},
	author = {Liu, Hongsheng and Ren, Guofei and Hu, Huan and Zhang, Li and Ai, Haixin and Zhang, Wen and Zhao, Qi},
	year = {2017},
	note = {Publisher: Impact Journals, LLC},
	pages = {103975},
}

@article{hughes_functional_2000,
	title = {Functional discovery via a compendium of expression profiles},
	volume = {102},
	number = {1},
	journal = {Cell},
	author = {Hughes, Timothy R and Marton, Matthew J and Jones, Allan R and Roberts, Christopher J and Stoughton, Roland and Armour, Christopher D and Bennett, Holly A and Coffey, Ernest and Dai, Hongyue and He, Yudong D and {others}},
	year = {2000},
	note = {Publisher: Elsevier},
	pages = {109--126},
}

@article{li_dnilmf-lda_2019,
	title = {{DNILMF}-{LDA}: prediction of {lncRNA}-disease associations by dual-network integrated logistic matrix factorization and {Bayesian} optimization},
	volume = {10},
	number = {8},
	journal = {Genes},
	author = {Li, Yan and Li, Junyi and Bian, Naizheng},
	year = {2019},
	note = {Publisher: MDPI},
	pages = {608},
}

@article{huang_deeppurpose_2020,
	title = {{DeepPurpose}: a deep learning library for drug–target interaction prediction},
	volume = {36},
	number = {22-23},
	journal = {Bioinformatics},
	author = {Huang, Kexin and Fu, Tianfan and Glass, Lucas M and Zitnik, Marinka and Xiao, Cao and Sun, Jimeng},
	year = {2020},
	note = {Publisher: Oxford University Press},
	pages = {5545--5547},
}

@article{ding_similarity-based_2014,
	title = {Similarity-based machine learning methods for predicting drug–target interactions: a brief review},
	volume = {15},
	number = {5},
	journal = {Briefings in bioinformatics},
	author = {Ding, Hao and Takigawa, Ichigaku and Mamitsuka, Hiroshi and Zhu, Shanfeng},
	year = {2014},
	note = {Publisher: Oxford University Press},
	pages = {734--747},
}

@article{cock_biopython_2009,
	title = {Biopython: freely available {Python} tools for computational molecular biology and bioinformatics},
	volume = {25},
	number = {11},
	journal = {Bioinformatics},
	author = {Cock, Peter JA and Antao, Tiago and Chang, Jeffrey T and Chapman, Brad A and Cox, Cymon J and Dalke, Andrew and Friedberg, Iddo and Hamelryck, Thomas and Kauff, Frank and Wilczynski, Bartek and {others}},
	year = {2009},
	note = {Publisher: Oxford University Press},
	pages = {1422},
}

@article{chua_identifying_2006,
	title = {Identifying transcription factor functions and targets by phenotypic activation},
	volume = {103},
	number = {32},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Chua, Gordon and Morris, Quaid D and Sopko, Richelle and Robinson, Mark D and Ryan, Owen and Chan, Esther T and Frey, Brendan J and Andrews, Brenda J and Boone, Charles and Hughes, Timothy R},
	year = {2006},
	note = {Publisher: National Acad Sciences},
	pages = {12045--12050},
}

@article{alaimo_drugtarget_2013,
	title = {Drug–target interaction prediction through domain-tuned network-based inference},
	volume = {29},
	number = {16},
	journal = {Bioinformatics},
	author = {Alaimo, Salvatore and Pulvirenti, Alfredo and Giugno, Rosalba and Ferro, Alfredo},
	year = {2013},
	note = {Publisher: Oxford University Press},
	pages = {2004--2008},
}

@article{pliakos_global_2018,
	title = {Global multi-output decision trees for interaction prediction},
	volume = {107},
	doi = {10.1007/s10994-018-5700-x},
	journal = {Machine Learning},
	author = {Pliakos, Konstantinos and Geurts, Pierre and Vens, Celine},
	year = {2018},
	note = {Publisher: Springer},
	pages = {1257--1281},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\X66MLN8W\\Pliakos et al. - 2018 - Global multi-output decision trees for interaction.pdf:application/pdf},
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00058655},
	doi = {10.1007/BF00058655},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	language = {en},
	number = {2},
	urldate = {2024-03-18},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	note = {13430 citations (Crossref) [2024-03-18]},
	keywords = {Aggregation, Averaging, Bootstrap, Combining},
	pages = {123--140},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\UM52ZBPY\\Breiman - 1996 - Bagging predictors.pdf:application/pdf},
}

@article{menon_predicting_2010,
	title = {Predicting labels for dyadic data},
	volume = {21},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-010-0189-3},
	doi = {10.1007/s10618-010-0189-3},
	abstract = {In dyadic prediction, the input consists of a pair of items (a dyad), and the goal is to predict the value of an observation related to the dyad. Special cases of dyadic prediction include collaborative filtering, where the goal is to predict ratings associated with (user, movie) pairs, and link prediction, where the goal is to predict the presence or absence of an edge between two nodes in a graph. In this paper, we study the problem of predicting labels associated with dyad members. Special cases of this problem include predicting characteristics of users in a collaborative filtering scenario, and predicting the label of a node in a graph, which is a task sometimes called within-network classification or relational learning. This paper shows how to extend a recent dyadic prediction method to predict labels for nodes and labels for edges simultaneously. The new method learns latent features within a log-linear model in a supervised way, to maximize predictive accuracy for both dyad observations and item labels. We compare the new approach to existing methods for within-network classification, both experimentally and analytically. The experiments show, surprisingly, that learning latent features in an unsupervised way is superior for some applications to learning them in a supervised way.},
	language = {en},
	number = {2},
	urldate = {2024-03-17},
	journal = {Data Mining and Knowledge Discovery},
	author = {Menon, Aditya Krishna and Elkan, Charles},
	month = sep,
	year = {2010},
	note = {22 citations (Crossref) [2024-03-17]},
	keywords = {Collaborative filtering, Dyadic prediction, Link prediction, Relational learning, Social networks, Within-network classification},
	pages = {327--343},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\5Q2VDCE4\\Menon and Elkan - 2010 - Predicting labels for dyadic data.pdf:application/pdf},
}

@article{zhou_progresses_2021,
	title = {Progresses and challenges in link prediction},
	volume = {24},
	url = {https://www.cell.com/iscience/pdf/S2589-0042(21)01185-8.pdf},
	number = {11},
	urldate = {2024-03-17},
	journal = {Iscience},
	author = {Zhou, Tao},
	year = {2021},
	note = {Publisher: Elsevier},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\RBJJ8N7A\\Zhou - 2021 - Progresses and challenges in link prediction.pdf:application/pdf},
}

@article{bonissone_fuzzy_2010,
	title = {A fuzzy random forest},
	volume = {51},
	issn = {0888-613X},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X10000435},
	doi = {10.1016/j.ijar.2010.02.003},
	abstract = {When individual classifiers are combined appropriately, a statistically significant increase in classification accuracy is usually obtained. Multiple classifier systems are the result of combining several individual classifiers. Following Breiman’s methodology, in this paper a multiple classifier system based on a “forest” of fuzzy decision trees, i.e., a fuzzy random forest, is proposed. This approach combines the robustness of multiple classifier systems, the power of the randomness to increase the diversity of the trees, and the flexibility of fuzzy logic and fuzzy sets for imperfect data management. Various combination methods to obtain the final decision of the multiple classifier system are proposed and compared. Some of them are weighted combination methods which make a weighting of the decisions of the different elements of the multiple classifier system (leaves or trees). A comparative study with several datasets is made to show the efficiency of the proposed multiple classifier system and the various combination methods. The proposed multiple classifier system exhibits a good accuracy classification, comparable to that of the best classifiers when tested with conventional data sets. However, unlike other classifiers, the proposed classifier provides a similar accuracy when tested with imperfect datasets (with missing and fuzzy values) and with datasets with noise.},
	number = {7},
	urldate = {2024-03-16},
	journal = {International Journal of Approximate Reasoning},
	author = {Bonissone, Piero and Cadenas, José M. and Carmen Garrido, M. and Andrés Díaz-Valladares, R.},
	month = sep,
	year = {2010},
	note = {126 citations (Crossref) [2024-03-16]},
	keywords = {Combination methods, Fuzzy decision tree, Fuzzy sets, Random forest},
	pages = {729--747},
	file = {ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\IXQ5N5PN\\S0888613X10000435.html:text/html},
}

@article{natekin_gradient_2013,
	title = {Gradient boosting machines, a tutorial},
	volume = {7},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021},
	doi = {10.3389/fnbot.2013.00021},
	abstract = {Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods. A theoretical information is complemented with many descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. A set of practical examples of gradient boosting applications are presented and comprehensively analyzed.},
	language = {English},
	urldate = {2024-03-16},
	journal = {Frontiers in Neurorobotics},
	author = {Natekin, Alexey and Knoll, Alois},
	month = dec,
	year = {2013},
	note = {1412 citations (Crossref) [2024-03-16]
Publisher: Frontiers},
	keywords = {boosting, Classification, gradient boosting, machine learning, regression, robotic control, text classification},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\WGVGS3D8\\Natekin and Knoll - 2013 - Gradient boosting machines, a tutorial.pdf:application/pdf},
}

@article{tang_making_2014,
	title = {Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis},
	volume = {54},
	doi = {10.1021/ci400709d},
	number = {3},
	journal = {Journal of Chemical Information and Modeling},
	author = {Tang, Jing and Szwajda, Agnieszka and Shakyawar, Sushil and Xu, Tao and Hintsanen, Petteri and Wennerberg, Krister and Aittokallio, Tero},
	year = {2014},
	note = {Publisher: ACS Publications},
	pages = {735--743},
}

@article{lu_recommender_2012,
	series = {Recommender {Systems}},
	title = {Recommender systems},
	volume = {519},
	issn = {0370-1573},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157312000828},
	doi = {10.1016/j.physrep.2012.02.006},
	abstract = {The ongoing rapid expansion of the Internet greatly increases the necessity of effective recommender systems for filtering the abundant information. Extensive research for recommender systems is conducted by a broad range of communities including social and computer scientists, physicists, and interdisciplinary researchers. Despite substantial theoretical and practical achievements, unification and comparison of different approaches are lacking, which impedes further advances. In this article, we review recent developments in recommender systems and discuss the major challenges. We compare and evaluate available algorithms and examine their roles in the future developments. In addition to algorithms, physical aspects are described to illustrate macroscopic behavior of recommender systems. Potential impacts and future directions are discussed. We emphasize that recommendation has great scientific depth and combines diverse research fields which makes it interesting for physicists as well as interdisciplinary researchers.},
	number = {1},
	urldate = {2024-03-15},
	journal = {Physics Reports},
	author = {Lü, Linyuan and Medo, Matúš and Yeung, Chi Ho and Zhang, Yi-Cheng and Zhang, Zi-Ke and Zhou, Tao},
	month = oct,
	year = {2012},
	note = {766 citations (Crossref) [2024-03-15]},
	keywords = {Information filtering, Networks, Recommender systems},
	pages = {1--49},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\RRG9JB2C\\Lü et al. - 2012 - Recommender systems.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\KUTF6XJ6\\S0370157312000828.html:text/html},
}

@article{levatic_semi-supervised_2017,
	title = {Semi-supervised classification trees},
	volume = {49},
	doi = {10.1007/s10844-017-0457-4},
	journal = {Journal of Intelligent Information Systems},
	author = {Levatić, Jurica and Ceci, Michelangelo and Kocev, Dragi and Džeroski, Sašo},
	year = {2017},
	note = {Publisher: Springer},
	pages = {461--486},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\7IYJFW5X\\Levatić et al. - 2017 - Semi-supervised classification trees.pdf:application/pdf},
}

@book{quinlan_c4_2014,
	title = {C4. 5: programs for machine learning},
	shorttitle = {C4. 5},
	url = {https://books.google.com/books?hl=pt-BR&lr=&id=b3ujBQAAQBAJ&oi=fnd&pg=PP1&dq=quinlan+programs&ots=sS2mWLHrD2&sig=N3A5U153LT9H8Z9okfYYd-y9L7c},
	urldate = {2024-03-15},
	publisher = {Elsevier},
	author = {Quinlan, J. Ross},
	year = {2014},
}

@article{quinlan_induction_1986,
	title = {Induction of decision trees},
	volume = {1},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00116251},
	doi = {10.1007/BF00116251},
	language = {en},
	number = {1},
	urldate = {2024-03-15},
	journal = {Machine Learning},
	author = {Quinlan, J. R.},
	month = mar,
	year = {1986},
	note = {8796 citations (Crossref) [2024-03-15]},
	pages = {81--106},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\796KZYJC\\Quinlan - 1986 - Induction of decision trees.pdf:application/pdf},
}

@article{quinlan_induction_1986-1,
	title = {Induction of decision trees},
	volume = {1},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00116251},
	doi = {10.1007/BF00116251},
	abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
	language = {en},
	number = {1},
	urldate = {2024-03-15},
	journal = {Machine Learning},
	author = {Quinlan, J. R.},
	month = mar,
	year = {1986},
	note = {8796 citations (Crossref) [2024-03-15]},
	keywords = {classification, decision trees, expert systems, induction, information theory, knowledge acquisition},
	pages = {81--106},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\SCC8M9SC\\Quinlan - 1986 - Induction of decision trees.pdf:application/pdf},
}

@article{schrynemackers_classifying_2015,
	title = {Classifying pairs with trees for supervised biological network inference},
	volume = {11},
	doi = {https://doi.org/10.1039/c5mb00174a},
	number = {8},
	journal = {Molecular BioSystems},
	author = {Schrynemackers, Marie and Wehenkel, Louis and Babu, M Madan and Geurts, Pierre},
	year = {2015},
	note = {Publisher: Royal Society of Chemistry},
	pages = {2116--2125},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\DEUEP6HJ\\Schrynemackers et al. - 2015 - Classifying pairs with trees for supervised biolog.pdf:application/pdf},
}

@article{grinsztajn_why_2022,
	title = {Why do tree-based models still outperform deep learning on typical tabular data?},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/0378c7692da36807bdec87ab043cdadc-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2024-03-15},
	journal = {Advances in Neural Information Processing Systems},
	author = {Grinsztajn, Leo and Oyallon, Edouard and Varoquaux, Gael},
	month = dec,
	year = {2022},
	pages = {507--520},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\S6D3ZEPV\\Grinsztajn et al. - 2022 - Why do tree-based models still outperform deep lea.pdf:application/pdf},
}

@article{hyafil_constructing_1976,
	title = {Constructing optimal binary decision trees is {NP}-complete},
	volume = {5},
	issn = {0020-0190},
	url = {https://www.sciencedirect.com/science/article/pii/0020019076900958},
	doi = {10.1016/0020-0190(76)90095-8},
	number = {1},
	urldate = {2024-03-13},
	journal = {Information Processing Letters},
	author = {Hyafil, Laurent and Rivest, Ronald L.},
	month = may,
	year = {1976},
	note = {542 citations (Crossref) [2024-03-13]},
	keywords = {Binary decision trees, computational complexity, NP-complete},
	pages = {15--17},
	file = {ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\NSC2QXKY\\0020019076900958.html:text/html},
}

@article{mei_drugtarget_2013,
	title = {Drug–target interaction prediction by learning from local information and neighbors},
	volume = {29},
	doi = {https://doi.org/10.1093/bioinformatics/bts670},
	number = {2},
	journal = {Bioinformatics},
	author = {Mei, Jian-Ping and Kwoh, Chee-Keong and Yang, Peng and Li, Xiao-Li and Zheng, Jie},
	year = {2013},
	note = {Publisher: Oxford University Press},
	pages = {238--245},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\BGH2XR6D\\Mei et al. - 2013 - Drug–target interaction prediction by learning fro.pdf:application/pdf},
}

@article{bekker_learning_2020,
	title = {Learning from positive and unlabeled data: a survey},
	volume = {109},
	issn = {1573-0565},
	shorttitle = {Learning from positive and unlabeled data},
	url = {https://doi.org/10.1007/s10994-020-05877-5},
	doi = {10.1007/s10994-020-05877-5},
	abstract = {Learning from positive and unlabeled data or PU learning is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the machine learning literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art in PU learning. It proposes seven key research questions that commonly arise in this field and provides a broad overview of how the field has tried to address them.},
	language = {en},
	number = {4},
	urldate = {2024-01-24},
	journal = {Machine Learning},
	author = {Bekker, Jessa and Davis, Jesse},
	month = apr,
	year = {2020},
	note = {215 citations (Crossref) [2024-01-24]},
	keywords = {Classification, 68T05, PU learning, Weakly supervised learning},
	pages = {719--760},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\V4MIVCVY\\Bekker and Davis - 2020 - Learning from positive and unlabeled data a surve.pdf:application/pdf},
}

@article{sun_multi-label_2010,
	title = {Multi-{Label} {Learning} with {Weak} {Label}},
	volume = {24},
	copyright = {Copyright (c) 2021 Proceedings of the AAAI Conference on Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7699},
	doi = {10.1609/aaai.v24i1.7699},
	abstract = {Multi-label learning deals with data associated with multiple labels simultaneously. Previous work on multi-label learning assumes that for each instance, the \&ldquo;full\&rdquo; label set associated with each training instance is given by users. In many applications, however, to get the full label set for each instance is difficult and only a \&ldquo;partial\&rdquo; set of labels is available. In such cases, the appearance of a label means that the instance is associated with this label, while the absence of a label does not imply that this label is not proper for the instance. We call this kind of problem \&ldquo;weak label\&rdquo; problem. In this paper, we propose the WELL (WEak Label Learning) method to solve the weak label problem. We consider that the classification boundary for each label should go across low density regions, and that each label generally has much smaller number of positive examples than negative examples. The objective is formulated as a convex optimization problem which can be solved efficiently. Moreover, we exploit the correlation between labels by assuming that there is a group of low-rank base similarities, and the appropriate similarities between instances for different labels can be derived from these base similarities. Experiments validate the performance of WELL.},
	language = {en},
	number = {1},
	urldate = {2024-02-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sun, Yu-Yin and Zhang, Yin and Zhou, Zhi-Hua},
	month = jul,
	year = {2010},
	note = {78 citations (Crossref) [2024-02-06]
Number: 1},
	pages = {593--598},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\YRCPYGLF\\Sun et al. - 2010 - Multi-Label Learning with Weak Label.pdf:application/pdf},
}

@article{plessis2017classprior,
	title = {Class-prior {Estimation} for {Learning} from {Positive} and {Unlabeled} {Data}},
	volume = {106},
	issn = {0885-6125, 1573-0565},
	doi = {10.1007/s10994-016-5604-6},
	number = {4},
	journal = {Machine Learning},
	author = {du Plessis, MarthinusC. and Niu, Gang and Sugiyama, Masashi},
	month = apr,
	year = {2017},
	note = {38 citations (Crossref) [2024-01-30]
arXiv: 1611.01586 [cs, stat]},
	pages = {463--492},
}

@inproceedings{perini2020class,
	title = {Class {Prior} {Estimation} in {Active} {Positive} and {Unlabeled} {Learning}},
	doi = {10.24963/ijcai.2020/399},
	abstract = {Estimating the proportion of positive examples (i.e., the class prior) from positive and unlabeled (PU) data is an important task that facilitates learning a classifier from such data. In this paper, we explore how to tackle this problem when the observed labels were acquired via active learning. This introduces the challenge that the observed labels were not selected completely at random, which is the primary assumption underpinning existing approaches to estimating the class prior from PU data. We analyze this new setting and design an algorithm that is able to estimate the class prior for a given active learning strategy. Empirically, we show that our approach accurately recovers the true class prior on a benchmark of anomaly detection datasets and that it does so more accurately than existing methods.},
	author = {Perini, Lorenzo and Vercruyssen, Vincent and Davis, Jesse},
	month = jul,
	year = {2020},
	note = {2 citations (Crossref) [2024-01-30]},
	pages = {2887--2893},
	file = {Perini et al. - 2020 - Class Prior Estimation in Active Positive and Unla:C\:\\Users\\u0170502\\Zotero\\storage\\TLLGJGPM\\Perini et al. - 2020 - Class Prior Estimation in Active Positive and Unla.pdf:application/pdf},
}

@article{nakano2022deep,
	title = {Deep tree-ensembles for multi-output prediction},
	volume = {121},
	issn = {0031-3203},
	doi = {10.1016/j.patcog.2021.108211},
	journal = {Pattern Recognition},
	author = {Nakano, Felipe Kenji and Pliakos, Konstantinos and Vens, Celine},
	month = jan,
	year = {2022},
	note = {8 citations (Crossref) [2024-01-30]},
	pages = {108211},
}

@article{liu2016classification,
	title = {Classification with {Noisy} {Labels} by {Importance} {Reweighting}},
	volume = {38},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/7159100?casa_token=rVemoYbQsooAAAAA:c72v9w99z0p3IabvydtzlsHRo9nQS30jHmCyUgkUXyillfSijS8-eryb_X5qGaIWeayEmVzDntL8},
	doi = {10.1109/TPAMI.2015.2456899},
	abstract = {In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability {\textbackslash}rho ın [0,0.5) , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate ρ . We show that the rate is upper bounded by the conditional probability P({\textbackslash}hatY{\textbar}X) of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.},
	number = {3},
	urldate = {2023-12-31},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Tongliang and Tao, Dacheng},
	year = {2016},
	note = {412 citations (Crossref) [2024-01-30]
tex.eventtitle: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {447--461},
	file = {7159100:C\:\\Users\\u0170502\\Zotero\\storage\\8WQQT8I8\\7159100.html:text/html;Liu and Tao - 2016 - Classification with Noisy Labels by Importance Rew:C\:\\Users\\u0170502\\Zotero\\storage\\ZP4HVPT4\\Liu and Tao - 2016 - Classification with Noisy Labels by Importance Rew.pdf:application/pdf},
}

@inproceedings{elkan2008learning,
	series = {{KDD} '08},
	title = {Learning classifiers from only positive and unlabeled data},
	isbn = {978-1-60558-193-4},
	url = {https://doi.org/10.1145/1401890.1401920},
	doi = {10.1145/1401890.1401920},
	abstract = {The input to an algorithm that learns a binary classifier normally consists of two sets of examples, where one set consists of positive examples of the concept to be learned, and the other set consists of negative examples. However, it is often the case that the available training data are an incomplete set of positive examples, and a set of unlabeled examples, some of which are positive and some of which are negative. The problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature. Under the assumption that the labeled examples are selected randomly from the positive examples, we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. We show how to use this result in two different ways to learn a classifier from a nontraditional training set. We then apply these two new methods to solve a real-world problem: identifying protein records that should be included in an incomplete specialized molecular biology database. Our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased SVM method for learning from positive and unlabeled examples.},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Elkan, Charles and Noto, Keith},
	month = aug,
	year = {2008},
	note = {487 citations (Crossref) [2024-01-30]
Place: New York, NY, USA},
	keywords = {bioinformatics, supervised learning, text mining, unlabeled examples},
	pages = {213--220},
	file = {Elkan and Noto - 2008 - Learning classifiers from only positive and unlabe:C\:\\Users\\u0170502\\Zotero\\storage\\PQ46LD47\\Elkan and Noto - 2008 - Learning classifiers from only positive and unlabe.pdf:application/pdf},
}

@article{bekker2018estimating,
	title = {Estimating the {Class} {Prior} in {Positive} and {Unlabeled} {Data} {Through} {Decision} {Tree} {Induction}},
	volume = {32},
	issn = {2374-3468},
	doi = {10.1609/aaai.v32i1.11715},
	number = {1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bekker, Jessa and Davis, Jesse},
	month = apr,
	year = {2018},
	note = {37 citations (Crossref) [2024-01-30]},
}

@inproceedings{sun2010multi,
	title = {Multi-label learning with weak label},
	volume = {24},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Sun, Yu-Yin and Zhang, Yin and Zhou, Zhi-Hua},
	year = {2010},
	note = {Number: 1},
	pages = {593--598},
}

@inproceedings{zhan2017inductive,
	title = {Inductive semi-supervised multi-label learning with co-training},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} international conference on knowledge discovery and data mining},
	author = {Zhan, Wang and Zhang, Min-Ling},
	year = {2017},
	pages = {1305--1314},
}

@inproceedings{tsoumakas2007random,
	title = {Random k-labelsets: {An} ensemble method for multilabel classification},
	booktitle = {European conference on machine learning},
	publisher = {Springer},
	author = {Tsoumakas, Grigorios and Vlahavas, Ioannis},
	year = {2007},
	pages = {406--417},
}

@article{hinton2006reducing,
	title = {Reducing the dimensionality of data with neural networks},
	volume = {313},
	number = {5786},
	journal = {Science (New York, N.Y.)},
	author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
	year = {2006},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {504--507},
}

@article{nakano2019machine,
	title = {Machine learning for discovering missing or wrong protein function annotations: {A} comparison using updated benchmark datasets},
	volume = {20},
	journal = {BMC bioinformatics},
	author = {Nakano, Felipe Kenji and Lietaert, Mathias and Vens, Celine},
	year = {2019},
	note = {Publisher: Springer},
	pages = {1--32},
}

@inproceedings{xie2018partial,
	title = {Partial multi-label learning},
	volume = {32},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Xie, Ming-Kun and Huang, Sheng-Jun},
	year = {2018},
	note = {Number: 1},
}

@inproceedings{wang2020dual,
	title = {Dual relation semi-supervised multi-label learning},
	volume = {34},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Wang, Lichen and Liu, Yunyu and Qin, Can and Sun, Gan and Fu, Yun},
	year = {2020},
	note = {Number: 04},
	pages = {6227--6234},
}

@article{xu2013speedup,
	title = {Speedup matrix completion with side information: {Application} to multi-label learning},
	volume = {26},
	journal = {Advances in neural information processing systems},
	author = {Xu, Miao and Jin, Rong and Zhou, Zhi-Hua},
	year = {2013},
}

@article{zhang2007ml,
	title = {{ML}-{KNN}: {A} lazy learning approach to multi-label learning},
	volume = {40},
	number = {7},
	journal = {Pattern recognition},
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	year = {2007},
	note = {Publisher: Elsevier},
	pages = {2038--2048},
}

@article{ma2020cost,
	title = {Cost-sensitive deep forest for price prediction},
	volume = {107},
	journal = {Pattern Recognition},
	author = {Ma, Chao and Liu, Zhenbing and Cao, Zhiguang and Song, Wen and Zhang, Jie and Zeng, Weiliang},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {107499},
}

@article{su2019deep,
	title = {Deep-{Resp}-{Forest}: a deep forest model to predict anti-cancer drug response},
	volume = {166},
	journal = {Methods (San Diego, Calif.)},
	author = {Su, Ran and Liu, Xinyi and Wei, Leyi and Zou, Quan},
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {91--102},
}

@article{zhou2019hashing,
	title = {Deep forest hashing for image retrieval},
	volume = {95},
	journal = {Pattern Recognition},
	author = {Zhou, Meng and Zeng, Xianhua and Chen, Aozhu},
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {114--127},
}

@article{utkin2018siamese,
	title = {A siamese deep forest},
	volume = {139},
	journal = {Knowledge-Based Systems},
	author = {Utkin, Lev V and Ryabinin, Mikhail A},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {13--22},
}

@article{yang2019multi,
	title = {Multi-label learning with deep forest},
	journal = {arXiv preprint arXiv:1911.06557},
	author = {Yang, Liang and Wu, Xi-Zhu and Jiang, Yuan and Zhou, Zhi-Hua},
	year = {2019},
}

@article{xu2019survey,
	title = {Survey on multi-output learning},
	volume = {31},
	number = {7},
	journal = {IEEE transactions on neural networks and learning systems},
	author = {Xu, Donna and Shi, Yaxin and Tsang, Ivor W and Ong, Yew-Soon and Gong, Chen and Shen, Xiaobo},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {2409--2429},
}

@article{read2021classifier,
	title = {Classifier chains: {A} review and perspectives},
	volume = {70},
	journal = {Journal of Artificial Intelligence Research},
	author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoffrey and Frank, Eibe},
	year = {2021},
	pages = {683--718},
}

@article{guo2018bcdforest,
	title = {{BCDForest}: a boosting cascade deep forest model towards the classification of cancer subtypes based on gene expression data},
	volume = {19},
	number = {5},
	journal = {BMC bioinformatics},
	author = {Guo, Yang and Liu, Shuhui and Li, Zhanhuai and Shang, Xuequn},
	year = {2018},
	note = {Publisher: BioMed Central},
	pages = {1--13},
}

@article{waegeman2019multi,
	title = {Multi-target prediction: a unifying view on problems and methods},
	volume = {33},
	journal = {Data Mining and Knowledge Discovery},
	author = {Waegeman, Willem and Dembczyński, Krzysztof and Hüllermeier, Eyke},
	year = {2019},
	note = {Publisher: Springer},
	pages = {293--324},
}

@misc{zhao2022boosting,
	title = {A {Boosting} {Algorithm} for {Positive}-{Unlabeled} {Learning}},
	author = {Zhao, Yawen and Zhang, Mingzhe and Zhang, Chenhao and Chen, Weitong and Ye, Nan and Xu, Miao},
	month = dec,
	year = {2022},
	doi = {10.48550/arXiv.2205.09485},
	note = {arXiv: 2205.09485 [cs]},
}

@inproceedings{szymanski2017network,
	title = {A {Network} {Perspective} on {Stratification} of {Multi}-{Label} {Data}},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {Learning} with {Imbalanced} {Domains}: {Theory} and {Applications}},
	publisher = {PMLR},
	author = {Szymański, Piotr and Kajdanowicz, Tomasz},
	month = oct,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {22--35},
}

@article{grinsztajn2022tree,
	title = {Why do tree-based models still outperform deep learning on typical tabular data?},
	volume = {35},
	journal = {Advances in Neural Information Processing Systems},
	author = {Grinsztajn, Léo and Oyallon, Edouard and Varoquaux, Gaël},
	year = {2022},
	pages = {507--520},
}

@incollection{sechidis2011stratification,
	address = {Berlin, Heidelberg},
	title = {On the {Stratification} of {Multi}-label {Data}},
	volume = {6913},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	pages = {145--158},
}

@article{northcuttlearning,
	title = {Learning with {Confident} {Examples}: {Rank} {Pruning} for {Robust} {Classification} with {Noisy} {Labels}},
	abstract = {P˜N˜ learning is the problem of binary classification when training examples may be mislabeled (flipped) uniformly with noise rate ρ1 for positive examples and ρ0 for negative examples. We propose Rank Pruning (RP) to solve P˜N˜ learning and the open problem of estimating the noise rates. Unlike prior solutions, RP is efficient and general, requiring O(T ) for any unrestricted choice of probabilistic classifier with T fitting time. We prove RP achieves consistent noise estimation and equivalent expected risk as learning with uncorrupted labels in ideal conditions, and derive closed-form solutions when conditions are non-ideal. RP achieves state-of-the-art noise estimation and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the amount of noise. To highlight, RP with a CNN classifier can predict if an MNIST digit is a one or not with only 0.25\% error, and 0.46\% error across all digits, even when 50\% of positive examples are mislabeled and 50\% of observed positive labels are mislabeled negative examples.},
	language = {english},
	author = {Northcutt, Curtis G and Wu, Tailin and Chuang, Isaac L},
	file = {Northcutt et al. - Learning with Conﬁdent Examples Rank Pruning for:C\:\\Users\\u0170502\\Zotero\\storage\\7BEK9FTM\\Northcutt et al. - Learning with Conﬁdent Examples Rank Pruning for .pdf:application/pdf},
}

@inproceedings{lundberg2017unified,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2024-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	file = {Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio:C\:\\Users\\u0170502\\Zotero\\storage\\2VWVXBLT\\Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@inproceedings{garg2021mixture,
	title = {Mixture {Proportion} {Estimation} and {PU} {Learning}:{A} {Modern} {Approach}},
	volume = {34},
	shorttitle = {Mixture {Proportion} {Estimation} and {PU} {Learning}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/47b4f1bfdf6d298682e610ad74b37dca-Abstract.html},
	urldate = {2023-12-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Garg, Saurabh and Wu, Yifan and Smola, Alexander J and Balakrishnan, Sivaraman and Lipton, Zachary},
	year = {2021},
	pages = {8532--8544},
	file = {Garg et al. - 2021 - Mixture Proportion Estimation and PU LearningA Mo:C\:\\Users\\u0170502\\Zotero\\storage\\JWVGU5JT\\Garg et al. - 2021 - Mixture Proportion Estimation and PU LearningA Mo.pdf:application/pdf},
}

@inproceedings{szymanski_network_2017,
	title = {A {Network} {Perspective} on {Stratification} of {Multi}-{Label} {Data}},
	url = {https://proceedings.mlr.press/v74/szyma%C5%84ski17a.html},
	abstract = {We present a new approach to stratifying multi-label data for classification purposes based on the iterative stratification approach proposed by Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the iterative approach to take into account second-order relationships between labels. Obtained results are evaluated using statistical properties of obtained strata as presented by Sechidis. We also propose new statistical measures relevant to second-order quality: label pairs distribution, the percentage of label pairs without positive evidence in folds and label pair - fold pairs that have no positive evidence for the label pair. We verify the impact of new methods on classification performance of Binary Relevance, Label Powerset and a fast greedy community detection based label space partitioning classifier. The proposed approach lowers the variance of classification quality, improves label pair oriented measures and example distribution while maintaining a competitive quality in label-oriented measures. We also witness an increase in stability of network characteristics.},
	language = {en},
	urldate = {2024-01-28},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {Learning} with {Imbalanced} {Domains}: {Theory} and {Applications}},
	publisher = {PMLR},
	author = {Szymański, Piotr and Kajdanowicz, Tomasz},
	month = oct,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {22--35},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\VFSF5A5C\\Szymański and Kajdanowicz - 2017 - A Network Perspective on Stratification of Multi-L.pdf:application/pdf},
}

@incollection{gunopulos_stratification_2011,
	address = {Berlin, Heidelberg},
	title = {On the {Stratification} of {Multi}-label {Data}},
	volume = {6913},
	isbn = {978-3-642-23807-9 978-3-642-23808-6},
	url = {http://link.springer.com/10.1007/978-3-642-23808-6_10},
	abstract = {Stratiﬁed sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained. In single-label classiﬁcation tasks, groups are diﬀerentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratiﬁed sampling could/should be performed. This paper investigates stratiﬁcation in the multi-label data context. It considers two stratiﬁcation methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
	language = {en},
	urldate = {2024-01-28},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
	editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
	year = {2011},
	doi = {10.1007/978-3-642-23808-6_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {145--158},
	file = {Sechidis et al. - 2011 - On the Stratification of Multi-label Data.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\EKDGYBFA\\Sechidis et al. - 2011 - On the Stratification of Multi-label Data.pdf:application/pdf},
}

@article{northcutt_learning_nodate,
	title = {Learning with {Conﬁdent} {Examples}: {Rank} {Pruning} for {Robust} {Classiﬁcation} with {Noisy} {Labels}},
	abstract = {P˜N˜ learning is the problem of binary classiﬁcation when training examples may be mislabeled (ﬂipped) uniformly with noise rate ρ1 for positive examples and ρ0 for negative examples. We propose Rank Pruning (RP) to solve P˜N˜ learning and the open problem of estimating the noise rates. Unlike prior solutions, RP is efﬁcient and general, requiring O(T ) for any unrestricted choice of probabilistic classiﬁer with T ﬁtting time. We prove RP achieves consistent noise estimation and equivalent expected risk as learning with uncorrupted labels in ideal conditions, and derive closed-form solutions when conditions are non-ideal. RP achieves state-of-the-art noise estimation and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the amount of noise. To highlight, RP with a CNN classiﬁer can predict if an MNIST digit is a one or not with only 0.25\% error, and 0.46\% error across all digits, even when 50\% of positive examples are mislabeled and 50\% of observed positive labels are mislabeled negative examples.},
	language = {en},
	author = {Northcutt, Curtis G and Wu, Tailin and Chuang, Isaac L},
	file = {Northcutt et al. - Learning with Conﬁdent Examples Rank Pruning for .pdf:C\:\\Users\\u0170502\\Zotero\\storage\\CDMQGBM2\\Northcutt et al. - Learning with Conﬁdent Examples Rank Pruning for .pdf:application/pdf},
}

@inproceedings{elkan_learning_2008,
	address = {New York, NY, USA},
	series = {{KDD} '08},
	title = {Learning classifiers from only positive and unlabeled data},
	isbn = {978-1-60558-193-4},
	url = {https://doi.org/10.1145/1401890.1401920},
	doi = {10.1145/1401890.1401920},
	abstract = {The input to an algorithm that learns a binary classifier normally consists of two sets of examples, where one set consists of positive examples of the concept to be learned, and the other set consists of negative examples. However, it is often the case that the available training data are an incomplete set of positive examples, and a set of unlabeled examples, some of which are positive and some of which are negative. The problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature. Under the assumption that the labeled examples are selected randomly from the positive examples, we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. We show how to use this result in two different ways to learn a classifier from a nontraditional training set. We then apply these two new methods to solve a real-world problem: identifying protein records that should be included in an incomplete specialized molecular biology database. Our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased SVM method for learning from positive and unlabeled examples.},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Elkan, Charles and Noto, Keith},
	month = aug,
	year = {2008},
	note = {487 citations (Crossref) [2024-01-24]},
	keywords = {bioinformatics, supervised learning, text mining, unlabeled examples},
	pages = {213--220},
	file = {Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\5PT2V2D5\\Elkan and Noto - 2008 - Learning classifiers from only positive and unlabe.pdf:application/pdf},
}

@article{cai_item_2016,
	title = {Item {Response} {Theory}},
	volume = {3},
	url = {https://doi.org/10.1146/annurev-statistics-041715-033702},
	doi = {10.1146/annurev-statistics-041715-033702},
	abstract = {This review introduces classical item response theory (IRT) models as well as more contemporary extensions to the case of multilevel, multidimensional, and mixtures of discrete and continuous latent variables through the lens of discrete multivariate analysis. A general modeling framework is discussed, and the applications of this framework in diverse contexts are presented, including large-scale educational surveys, randomized efficacy studies, and diagnostic measurement. Other topics covered include parameter estimation and model fit evaluation. Both classical (numerical integration based) and more modern (stochastic) parameter estimation approaches are discussed. Similarly, limited information goodness-of-fit testing and posterior predictive model checking are reviewed and contrasted. The review concludes with a discussion of some emerging strands in IRT research such as response time modeling, crossed random effects models, and non-standard models for response processes.},
	number = {1},
	urldate = {2024-01-14},
	journal = {Annual Review of Statistics and Its Application},
	author = {Cai, Li and Choi, Kilchan and Hansen, Mark and Harrell, Lauren},
	year = {2016},
	note = {37 citations (Crossref) [2024-01-14]
\_eprint: https://doi.org/10.1146/annurev-statistics-041715-033702},
	keywords = {diagnostic classification models, discrete multivariate analysis, item factor analysis, model fit testing, multidimensional IRT, multilevel IRT},
	pages = {297--321},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\RFE9THQN\\Cai et al. - 2016 - Item Response Theory.pdf:application/pdf},
}

@article{andridge_review_2010,
	title = {A {Review} of {Hot} {Deck} {Imputation} for {Survey} {Non}‐response},
	volume = {78},
	issn = {0306-7734, 1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1751-5823.2010.00103.x},
	doi = {10.1111/j.1751-5823.2010.00103.x},
	abstract = {Summary
            
              Hot deck imputation is a method for handling missing data in which each missing value is replaced with an observed response from a “similar” unit. Despite being used extensively in practice, the theory is not as well developed as that of other imputation methods. We have found that no consensus exists as to the best way to apply the hot deck and obtain inferences from the completed data set. Here we review different forms of the hot deck and existing research on its statistical properties. We describe applications of the hot deck currently in use, including the U.S. Census Bureau's hot deck for the Current Population Survey (CPS). We also provide an extended example of variations of the hot deck applied to the third National Health and Nutrition Examination Survey (NHANES III). Some potential areas for future research are highlighted.
            
          , 
            Résumé
            L'imputation hot deck est une méthode de gestion des données manquantes dans laquelle chaque valeur manquante est remplacée par une réponse observée à partir d'une unité“similaire.” Bien qu'elle soit largement utilisée en pratique, sa théorie n'est pas aussi développée que celle des autres méthodes d'imputation. Nous avons constaté qu'il n'existe aucun consensus quant à la meilleure faon d'appliquer les hot deck et obtenir des inférences à partir de la série de données complète. Ici, nous passons en revue les différentes formes de hot deck et les recherches existantes sur ses propriétés statistiques. Nous décrivons les applications du hot deck actuellement utilisées, y compris le hot deck du Bureau US du recensement pour la Current Population Survey (CPS). Nous proposons aussi des exemples nombreux de variations du hot deck à la troisième National Health and Nutrition Examination Survey (NHANES III). Certains domaines possibles de recherches futures sont mises en évidence.},
	language = {en},
	number = {1},
	urldate = {2024-01-13},
	journal = {International Statistical Review},
	author = {Andridge, Rebecca R. and Little, Roderick J. A.},
	month = apr,
	year = {2010},
	note = {627 citations (Crossref) [2024-01-13]},
	pages = {40--64},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\C6G49KFI\\Andridge and Little - 2010 - A Review of Hot Deck Imputation for Survey Non‐res.pdf:application/pdf},
}

@article{yu_veridical_2020,
	title = {Veridical data science},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1901326117},
	doi = {10.1073/pnas.1901326117},
	abstract = {Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.},
	number = {8},
	urldate = {2024-01-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yu, Bin and Kumbier, Karl},
	month = feb,
	year = {2020},
	note = {65 citations (Crossref) [2024-01-13]
Publisher: Proceedings of the National Academy of Sciences},
	pages = {3920--3929},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\P8UM6D7A\\Yu and Kumbier - 2020 - Veridical data science.pdf:application/pdf},
}

@misc{klusowski_large_2023,
	title = {Large {Scale} {Prediction} with {Decision} {Trees}},
	url = {http://arxiv.org/abs/2104.13881},
	doi = {10.48550/arXiv.2104.13881},
	abstract = {This paper shows that decision trees constructed with Classification and Regression Trees (CART) and C4.5 methodology are consistent for regression and classification tasks, even when the number of predictor variables grows sub-exponentially with the sample size, under natural 0-norm and 1-norm sparsity constraints. The theory applies to a wide range of models, including (ordinary or logistic) additive regression models with component functions that are continuous, of bounded variation, or, more generally, Borel measurable. Consistency holds for arbitrary joint distributions of the predictor variables, thereby accommodating continuous, discrete, and/or dependent data. Finally, we show that these qualitative properties of individual trees are inherited by Breiman's random forests. A key step in the analysis is the establishment of an oracle inequality, which allows for a precise characterization of the goodness-of-fit and complexity tradeoff for a mis-specified model.},
	urldate = {2024-01-13},
	publisher = {arXiv},
	author = {Klusowski, Jason M. and Tian, Peter M.},
	month = nov,
	year = {2023},
	note = {arXiv:2104.13881 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\VUYGXD4R\\Klusowski and Tian - 2023 - Large Scale Prediction with Decision Trees.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\M6XN23GQ\\2104.html:text/html},
}

@inproceedings{agarwal_hierarchical_2022,
	title = {Hierarchical {Shrinkage}: {Improving} the accuracy and interpretability of tree-based models.},
	shorttitle = {Hierarchical {Shrinkage}},
	url = {https://proceedings.mlr.press/v162/agarwal22b.html},
	abstract = {Decision trees and random forests (RF) are a cornerstone of modern machine learning practice. Due to their tendency to overfit, trees are typically regularized by a variety of techniques that modify their structure (e.g. pruning). We introduce Hierarchical Shrinkage (HS), a post-hoc algorithm which regularizes the tree not by altering its structure, but by shrinking the prediction over each leaf toward the sample means over each of its ancestors, with weights depending on a single regularization parameter and the number of samples in each ancestor. Since HS is a post-hoc method, it is extremely fast, compatible with any tree-growing algorithm and can be used synergistically with other regularization techniques. Extensive experiments over a wide variety of real-world datasets show that HS substantially increases the predictive performance of decision trees even when used in conjunction with other regularization techniques. Moreover, we find that applying HS to individual trees in a RF often improves its accuracy and interpretability by simplifying and stabilizing decision boundaries and SHAP values. We further explain HS by showing that it to be equivalent to ridge regression on a basis that is constructed of decision stumps associated to the internal nodes of a tree. All code and models are released in a full-fledged package available on Github},
	language = {en},
	urldate = {2024-01-13},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Agarwal, Abhineet and Tan, Yan Shuo and Ronen, Omer and Singh, Chandan and Yu, Bin},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {111--135},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\ZP3QUCKH\\Agarwal et al. - 2022 - Hierarchical Shrinkage Improving the accuracy and.pdf:application/pdf},
}

@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2024-01-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\XXFV7FIA\\Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@misc{noauthor_learning_nodate,
	title = {Learning from positive and unlabeled data: a survey {\textbar} {Machine} {Learning}},
	url = {https://link.springer.com/article/10.1007/s10994-020-05877-5#Sec35},
	urldate = {2024-01-12},
	file = {Learning from positive and unlabeled data\: a survey | Machine Learning:C\:\\Users\\u0170502\\Zotero\\storage\\RW2U2KPP\\s10994-020-05877-5.html:text/html},
}

@article{plessis_class-prior_2017,
	title = {Class-prior {Estimation} for {Learning} from {Positive} and {Unlabeled} {Data}},
	volume = {106},
	issn = {0885-6125, 1573-0565},
	url = {http://arxiv.org/abs/1611.01586},
	doi = {10.1007/s10994-016-5604-6},
	abstract = {We consider the problem of estimating the class prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized \$L\_1\$-distance gives a computationally efficient algorithm with an analytic solution. The consistency, stability, and estimation error are theoretically analyzed. Finally, we experimentally demonstrate the usefulness of the proposed method.},
	number = {4},
	urldate = {2023-12-31},
	journal = {Machine Learning},
	author = {Plessis, Marthinus C. du and Niu, Gang and Sugiyama, Masashi},
	month = apr,
	year = {2017},
	note = {36 citations (Crossref) [2023-12-31]
arXiv:1611.01586 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {463--492},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\6E7HYWKZ\\Plessis et al. - 2017 - Class-prior Estimation for Learning from Positive .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\662Q4REV\\1611.html:text/html},
}

@article{liu_classification_2016,
	title = {Classification with {Noisy} {Labels} by {Importance} {Reweighting}},
	volume = {38},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/7159100?casa_token=rVemoYbQsooAAAAA:c72v9w99z0p3IabvydtzlsHRo9nQS30jHmCyUgkUXyillfSijS8-eryb_X5qGaIWeayEmVzDntL8},
	doi = {10.1109/TPAMI.2015.2456899},
	abstract = {In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability {\textbackslash}rho ın [0,0.5) , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate ρ . We show that the rate is upper bounded by the conditional probability P({\textbackslash}hatY{\textbar}X) of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.},
	number = {3},
	urldate = {2023-12-31},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Tongliang and Tao, Dacheng},
	month = mar,
	year = {2016},
	note = {401 citations (Crossref) [2023-12-31]
Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {447--461},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\u0170502\\Zotero\\storage\\UXJ6REXV\\7159100.html:text/html;Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\5G6E4E6C\\Liu and Tao - 2016 - Classification with Noisy Labels by Importance Rew.pdf:application/pdf},
}

@inproceedings{perini_class_2020,
	title = {Class {Prior} {Estimation} in {Active} {Positive} and {Unlabeled} {Learning}},
	doi = {10.24963/ijcai.2020/399},
	abstract = {Estimating the proportion of positive examples (i.e., the class prior) from positive and unlabeled (PU) data is an important task that facilitates learning a classifier from such data. In this paper, we explore how to tackle this problem when the observed labels were acquired via active learning. This introduces the challenge that the observed labels were not selected completely at random, which is the primary assumption underpinning existing approaches to estimating the class prior from PU data. We analyze this new setting and design an algorithm that is able to estimate the class prior for a given active learning strategy. Empirically, we show that our approach accurately recovers the true class prior on a benchmark of anomaly detection datasets and that it does so more accurately than existing methods.},
	author = {Perini, Lorenzo and Vercruyssen, Vincent and Davis, Jesse},
	month = jul,
	year = {2020},
	note = {2 citations (Crossref) [2023-12-31]},
	pages = {2887--2893},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\QBJEWNGM\\Perini et al. - 2020 - Class Prior Estimation in Active Positive and Unla.pdf:application/pdf},
}

@misc{zhao_boosting_2022,
	title = {A {Boosting} {Algorithm} for {Positive}-{Unlabeled} {Learning}},
	url = {http://arxiv.org/abs/2205.09485},
	doi = {10.48550/arXiv.2205.09485},
	abstract = {Positive-unlabeled (PU) learning deals with binary classification problems when only positive (P) and unlabeled (U) data are available. Many recent PU methods are based on neural networks, but little has been done to develop boosting algorithms for PU learning, despite boosting algorithms' strong performance on many fully supervised classification problems. In this paper, we propose a novel boosting algorithm, AdaPU, for PU learning. Similarly to AdaBoost, AdaPU aims to optimize an empirical exponential loss, but the loss is based on the PU data, rather than on positive-negative (PN) data. As in AdaBoost, we learn a weighted combination of weak classifiers by learning one weak classifier and its weight at a time. However, AdaPU requires a very different algorithm for learning the weak classifiers and determining their weights. This is because AdaPU learns a weak classifier and its weight using a weighted positive-negative (PN) dataset with some negative data weights \$-\$ the dataset is derived from the original PU data, and the data weights are determined by the current weighted classifier combination, but some data weights are negative. Our experiments showed that AdaPU outperforms neural networks on several benchmark PU datasets, including a large-scale challenging cyber security dataset.},
	urldate = {2023-12-31},
	publisher = {arXiv},
	author = {Zhao, Yawen and Zhang, Mingzhe and Zhang, Chenhao and Chen, Weitong and Ye, Nan and Xu, Miao},
	month = dec,
	year = {2022},
	note = {arXiv:2205.09485 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\4KZW89TQ\\Zhao et al. - 2022 - A Boosting Algorithm for Positive-Unlabeled Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\GJEKD24T\\2205.html:text/html},
}

@inproceedings{garg_mixture_2021,
	title = {Mixture {Proportion} {Estimation} and {PU} {Learning}:{A} {Modern} {Approach}},
	volume = {34},
	shorttitle = {Mixture {Proportion} {Estimation} and {PU} {Learning}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/47b4f1bfdf6d298682e610ad74b37dca-Abstract.html},
	urldate = {2023-12-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Garg, Saurabh and Wu, Yifan and Smola, Alexander J and Balakrishnan, Sivaraman and Lipton, Zachary},
	year = {2021},
	pages = {8532--8544},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\KTSAR2CF\\Garg et al. - 2021 - Mixture Proportion Estimation and PU LearningA Mo.pdf:application/pdf},
}

@article{zhou_deep_2019,
	title = {Deep forest},
	volume = {6},
	issn = {2095-5138},
	url = {https://doi.org/10.1093/nsr/nwy108},
	doi = {10.1093/nsr/nwy108},
	abstract = {Current deep-learning models are mostly built upon neural networks, i.e. multiple layers of parameterized differentiable non-linear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules such as decision trees. After a discussion about the mystery behind deep neural networks, particularly by contrasting them with shallow neural networks and traditional machine-learning techniques such as decision trees and boosting machines, we conjecture that the success of deep neural networks owes much to three characteristics, i.e. layer-by-layer processing, in-model feature transformation and sufficient model complexity. On one hand, our conjecture may offer inspiration for theoretical understanding of deep learning; on the other hand, to verify the conjecture, we propose an approach that generates deep forest holding these characteristics. This is a decision-tree ensemble approach, with fewer hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to achieve excellent performance by using the same default setting. This study opens the door to deep learning based on non-differentiable modules without gradient-based adjustment, and exhibits the possibility of constructing deep models without backpropagation.},
	number = {1},
	urldate = {2023-12-18},
	journal = {National Science Review},
	author = {Zhou, Zhi-Hua and Feng, Ji},
	month = jan,
	year = {2019},
	note = {323 citations (Crossref) [2023-12-18]},
	pages = {74--86},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\FLGZNTY8\\Zhou and Feng - 2019 - Deep forest.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\KUIT2NKQ\\5123737.html:text/html},
}

@article{nakano_deep_2022,
	title = {Deep tree-ensembles for multi-output prediction},
	volume = {121},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321003927},
	doi = {10.1016/j.patcog.2021.108211},
	abstract = {Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks.},
	urldate = {2023-12-18},
	journal = {Pattern Recognition},
	author = {Nakano, Felipe Kenji and Pliakos, Konstantinos and Vens, Celine},
	month = jan,
	year = {2022},
	note = {8 citations (Crossref) [2023-12-18]},
	keywords = {Ensemble learning, Deep-forest, Multi-label classification, Multi-output prediction, Multi-target regression},
	pages = {108211},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\7MXAYVQJ\\Nakano et al. - 2022 - Deep tree-ensembles for multi-output prediction.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\UN6B3RWS\\S0031320321003927.html:text/html},
}

@misc{noauthor_deep_nodate,
	title = {Deep tree-ensembles for multi-output prediction - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321003927},
	urldate = {2023-12-18},
	file = {Deep tree-ensembles for multi-output prediction - ScienceDirect:C\:\\Users\\u0170502\\Zotero\\storage\\W7VJLMBT\\S0031320321003927.html:text/html},
}

@article{bekker_estimating_2018,
	title = {Estimating the {Class} {Prior} in {Positive} and {Unlabeled} {Data} {Through} {Decision} {Tree} {Induction}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11715},
	doi = {10.1609/aaai.v32i1.11715},
	abstract = {For tasks such as medical diagnosis and knowledge base completion, a classifier may only have access to positive and unlabeled examples, where the unlabeled data consists of both positive and negative examples. One way that enables learning from this type of data is knowing the true class prior. In this paper, we propose a simple yet effective method for estimating the class prior, by estimating the probability that a positive example is selected to be labeled. Our key insight is that subdomains of the data give a lower bound on this probability. This lower bound gets closer to the real probability as the ratio of labeled examples increases. Finding such subsets can naturally be done via top-down decision tree induction. Experiments show that our method makes estimates which are equivalently accurate as those of the state of the art methods, and is an order of magnitude faster.},
	language = {en},
	number = {1},
	urldate = {2023-12-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bekker, Jessa and Davis, Jesse},
	month = apr,
	year = {2018},
	note = {36 citations (Crossref) [2023-12-18]
Number: 1},
	keywords = {Positive Unlabeled Class Prior},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\9M3LDM3L\\Bekker and Davis - 2018 - Estimating the Class Prior in Positive and Unlabel.pdf:application/pdf},
}

@article{yang_deep_2023,
	title = {Deep reinforcement learning for multi-class imbalanced training: applications in healthcare},
	issn = {1573-0565},
	shorttitle = {Deep reinforcement learning for multi-class imbalanced training},
	url = {https://doi.org/10.1007/s10994-023-06481-z},
	doi = {10.1007/s10994-023-06481-z},
	abstract = {With the rapid growth of memory and computing power, datasets are becoming increasingly complex and imbalanced. This is especially severe in the context of clinical data, where there may be one rare event for many cases in the majority class. We introduce an imbalanced classification framework, based on reinforcement learning, for training extremely imbalanced data sets, and extend it for use in multi-class settings. We combine dueling and double deep Q-learning architectures, and formulate a custom reward function and episode-training procedure, specifically with the capability of handling multi-class imbalanced training. Using real-world clinical case studies, we demonstrate that our proposed framework outperforms current state-of-the-art imbalanced learning methods, achieving more fair and balanced classification, while also significantly improving the prediction of minority classes.},
	language = {en},
	urldate = {2023-11-29},
	journal = {Machine Learning},
	author = {Yang, Jenny and El-Bouri, Rasheed and O’Donoghue, Odhran and Lachapelle, Alexander S. and Soltan, Andrew A. S. and Eyre, David W. and Lu, Lei and Clifton, David A.},
	month = nov,
	year = {2023},
	note = {0 citations (Crossref) [2023-11-29]},
	keywords = {Imbalanced data, Imbalanced learning, Reinforcement learning},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\2V7JZYHJ\\Yang et al. - 2023 - Deep reinforcement learning for multi-class imbala.pdf:application/pdf},
}

@article{verreet_modeling_2023,
	title = {Modeling {PU} learning using probabilistic logic programming},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-023-06461-3},
	doi = {10.1007/s10994-023-06461-3},
	abstract = {The goal of learning from positive and unlabeled (PU) examples is to learn a classifier that predicts the posterior class probability. The challenge is that the available labels in the data are determined by (1) the true class, and (2) the labeling mechanism that selects which positive examples get labeled, where often certain examples have a higher probability to be selected than others. Incorrectly assuming an unbiased labeling mechanism leads to learning a biased classifier. Yet, this is what most existing methods do. A handful of methods makes more realistic assumptions, but they are either so general that it is impossible to distinguish between the effects of the true classification and of the labeling mechanism, or too restrictive to correctly model the real situation, or require knowledge that is typically unavailable. This paper studies how to formulate and integrate more realistic assumptions for learning better classifiers, by exploiting the strengths of probabilistic logic programming (PLP). Concretely, (1) we propose PU ProbLog: a PLP-based general method that allows to (partially) model the labeling mechanism. (2) We show that our method generalizes existing methods, in the sense that it can model the same assumptions. (3) Thanks to the use of PLP, our method supports also PU learning in relational domains. (4) Our empirical analysis shows that partially modeling the labeling bias, improves the learned classifiers.},
	language = {en},
	urldate = {2023-11-29},
	journal = {Machine Learning},
	author = {Verreet, Victor and De Raedt, Luc and Bekker, Jessa},
	month = nov,
	year = {2023},
	note = {0 citations (Crossref) [2023-11-29]},
	keywords = {Modeling, Positive unlabeled learning, Probabilistic logic programming, Unidentifiability, Weak supervision},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\JWFFAGVD\\Verreet et al. - 2023 - Modeling PU learning using probabilistic logic pro.pdf:application/pdf},
}

@article{fu_mvgcn_2022,
	title = {{MVGCN}: data integration through multi-view graph convolutional network for predicting links in biomedical bipartite networks},
	volume = {38},
	issn = {1367-4803},
	shorttitle = {{MVGCN}},
	url = {https://doi.org/10.1093/bioinformatics/btab651},
	doi = {10.1093/bioinformatics/btab651},
	abstract = {There are various interaction/association bipartite networks in biomolecular systems. Identifying unobserved links in biomedical bipartite networks helps to understand the underlying molecular mechanisms of human complex diseases and thus benefits the diagnosis and treatment of diseases. Although a great number of computational methods have been proposed to predict links in biomedical bipartite networks, most of them heavily depend on features and structures involving the bioentities in one specific bipartite network, which limits the generalization capacity of applying the models to other bipartite networks. Meanwhile, bioentities usually have multiple features, and how to leverage them has also been challenging.In this study, we propose a novel multi-view graph convolution network (MVGCN) framework for link prediction in biomedical bipartite networks. We first construct a multi-view heterogeneous network (MVHN) by combining the similarity networks with the biomedical bipartite network, and then perform a self-supervised learning strategy on the bipartite network to obtain node attributes as initial embeddings. Further, a neighborhood information aggregation (NIA) layer is designed for iteratively updating the embeddings of nodes by aggregating information from inter- and intra-domain neighbors in every view of the MVHN. Next, we combine embeddings of multiple NIA layers in each view, and integrate multiple views to obtain the final node embeddings, which are then fed into a discriminator to predict the existence of links. Extensive experiments show MVGCN performs better than or on par with baseline methods and has the generalization capacity on six benchmark datasets involving three typical tasks.Source code and data can be downloaded from https://github.com/fuhaitao95/MVGCN.Supplementary data are available at Bioinformatics online.},
	number = {2},
	urldate = {2023-11-23},
	journal = {Bioinformatics},
	author = {Fu, Haitao and Huang, Feng and Liu, Xuan and Qiu, Yang and Zhang, Wen},
	month = jan,
	year = {2022},
	note = {31 citations (Crossref) [2023-11-23]},
	pages = {426--434},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\4JHJHT7G\\Fu et al. - 2022 - MVGCN data integration through multi-view graph c.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\BD3TCWWD\\6367769.html:text/html},
}

@article{ru_nerltr-dta_2022,
	title = {{NerLTR}-{DTA}: drug–target binding affinity prediction based on neighbor relationship and learning to rank},
	volume = {38},
	issn = {1367-4803},
	shorttitle = {{NerLTR}-{DTA}},
	url = {https://doi.org/10.1093/bioinformatics/btac048},
	doi = {10.1093/bioinformatics/btac048},
	abstract = {Drug–target interaction prediction plays an important role in new drug discovery and drug repurposing. Binding affinity indicates the strength of drug–target interactions. Predicting drug–target binding affinity is expected to provide promising candidates for biologists, which can effectively reduce the workload of wet laboratory experiments and speed up the entire process of drug research. Given that, numerous new proteins are sequenced and compounds are synthesized, several improved computational methods have been proposed for such predictions, but there are still some challenges. (i) Many methods only discuss and implement one application scenario, they focus on drug repurposing and ignore the discovery of new drugs and targets. (ii) Many methods do not consider the priority order of proteins (or drugs) related to each target drug (or protein). Therefore, it is necessary to develop a comprehensive method that can be used in multiple scenarios and focuses on candidate order.In this study, we propose a method called NerLTR-DTA that uses the neighbor relationship of similarity and sharing to extract features, and applies a ranking framework with regression attributes to predict affinity values and priority order of query drug (or query target) and its related proteins (or compounds). It is worth noting that using the characteristics of learning to rank to set different queries can smartly realize the multi-scenario application of the method, including the discovery of new drugs and new targets. Experimental results on two commonly used datasets show that NerLTR-DTA outperforms some state-of-the-art competing methods. NerLTR-DTA achieves excellent performance in all application scenarios mentioned in this study, and the rm(test)2 values guarantee such excellent performance is not obtained by chance. Moreover, it can be concluded that NerLTR-DTA can provide accurate ranking lists for the relevant results of most queries through the statistics of the association relationship of each query drug (or query protein). In general, NerLTR-DTA is a powerful tool for predicting drug–target associations and can contribute to new drug discovery and drug repurposing.The proposed method is implemented in Python and Java. Source codes and datasets are available at https://github.com/RUXIAOQING964914140/NerLTR-DTA.},
	number = {7},
	urldate = {2023-11-22},
	journal = {Bioinformatics},
	author = {Ru, Xiaoqing and Ye, Xiucai and Sakurai, Tetsuya and Zou, Quan},
	month = mar,
	year = {2022},
	note = {23 citations (Crossref) [2023-11-22]},
	pages = {1964--1971},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\4ZIU7WYQ\\Ru et al. - 2022 - NerLTR-DTA drug–target binding affinity predictio.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\48E4ZXZX\\6522110.html:text/html},
}

@book{noauthor_notitle_nodate,
}

@article{van_laarhoven_gaussian_nodate,
	title = {Gaussian interaction proﬁle kernels for predicting drug–target interaction},
	abstract = {Motivation: The in silico prediction of potential interactions between drugs and target proteins is of core importance for the identiﬁcation of new drugs or novel targets for existing drugs. However, only a tiny portion of all drug–target pairs in current datasets are experimentally validated interactions. This motivates the need for developing computational methods that predict true interaction pairs with high accuracy.},
	language = {en},
	author = {van Laarhoven, Twan and Nabuurs, Sander B and Marchiori, Elena},
	file = {van Laarhoven et al. - Gaussian interaction proﬁle kernels for predicting.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\NQYS3FQT\\van Laarhoven et al. - Gaussian interaction proﬁle kernels for predicting.pdf:application/pdf},
}

@article{van_laarhoven_gaussian_2011,
	title = {Gaussian interaction profile kernels for predicting drug–target interaction},
	volume = {27},
	number = {21},
	journal = {Bioinformatics},
	author = {Van Laarhoven, Twan and Nabuurs, Sander B and Marchiori, Elena},
	year = {2011},
	note = {Publisher: Oxford University Press},
	pages = {3036--3043},
	file = {Van Laarhoven et al. - 2011 - Gaussian interaction profile kernels for predictin.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\G29MDZ3G\\Van Laarhoven et al. - 2011 - Gaussian interaction profile kernels for predictin.pdf:application/pdf},
}

@misc{noauthor_gaussian_nodate,
	title = {Gaussian interaction profile kernels for predicting drug–target interaction {\textbar} {Bioinformatics} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/bioinformatics/article/27/21/3036/216840},
	urldate = {2023-11-18},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\WQE9DH3A\\Gaussian interaction profile kernels for predictin.pdf:application/pdf;Gaussian interaction profile kernels for predicting drug–target interaction | Bioinformatics | Oxford Academic:C\:\\Users\\u0170502\\Zotero\\storage\\I3QSSJE4\\216840.html:text/html;supmat.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\2SJJ22K3\\supmat.pdf:application/pdf},
}

@article{yu_fpsc-dti_2020,
	title = {{FPSC}-{DTI}: drug–target interaction prediction based on feature projection fuzzy classification and super cluster fusion},
	volume = {16},
	issn = {2515-4184},
	shorttitle = {{FPSC}-{DTI}},
	url = {https://pubs.rsc.org/en/content/articlelanding/2020/mo/d0mo00062k},
	doi = {10.1039/D0MO00062K},
	abstract = {Identifying drug–target interactions (DTIs) is an important part of drug discovery and development. However, identifying DTIs is a complex process that is time consuming, costly, long, and often inefficient, with a low success rate, especially with wet-experimental methods. Computational methods based on drug repositioning and network pharmacology can effectively overcome these defects. In this paper, we develop a new fusion method, called FPSC-DTI, that fuses feature projection fuzzy classification (FP) and super cluster classification (SC) to predict DTI. As the experimental result, the mean percentile ranking (MPR) that was yielded by FPSC-DTI achieved 0.043, 0.084, 0.072, and 0.146 on enzyme, ion channel (IC), G-protein-coupled receptor (GPCR), and nuclear receptor (NR) datasets, respectively. And the AUC values exceeded 0.969 over all four datasets. Compared with other methods, FPSC-DTI obtained better predictive performance and became more robust.},
	language = {en},
	number = {6},
	urldate = {2023-10-31},
	journal = {Molecular Omics},
	author = {Yu, Donghua and Liu, Guojun and Zhao, Ning and Liu, Xiaoyan and Guo, Maozu},
	month = dec,
	year = {2020},
	note = {4 citations (Crossref) [2023-10-31]
Publisher: The Royal Society of Chemistry},
	pages = {583--591},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\XXJKBNH8\\Yu et al. - 2020 - FPSC-DTI drug–target interaction prediction based.pdf:application/pdf;Supplementary Information PDF:C\:\\Users\\u0170502\\Zotero\\storage\\6WW3IIGU\\Yu et al. - 2020 - FPSC-DTI drug–target interaction prediction based.pdf:application/pdf},
}

@article{hao_open-source_2019,
	title = {Open-source chemogenomic data-driven algorithms for predicting drug–target interactions},
	volume = {20},
	issn = {1477-4054},
	url = {https://doi.org/10.1093/bib/bby010},
	doi = {10.1093/bib/bby010},
	abstract = {While novel technologies such as high-throughput screening have advanced together with significant investment by pharmaceutical companies during the past decades, the success rate for drug development has not yet been improved prompting researchers looking for new strategies of drug discovery. Drug repositioning is a potential approach to solve this dilemma. However, experimental identification and validation of potential drug targets encoded by the human genome is both costly and time-consuming. Therefore, effective computational approaches have been proposed to facilitate drug repositioning, which have proved to be successful in drug discovery. Doubtlessly, the availability of open-accessible data from basic chemical biology research and the success of human genome sequencing are crucial to develop effective in silico drug repositioning methods allowing the identification of potential targets for existing drugs. In this work, we review several chemogenomic data-driven computational algorithms with source codes publicly accessible for predicting drug–target interactions (DTIs). We organize these algorithms by model properties and model evolutionary relationships. We re-implemented five representative algorithms in R programming language, and compared these algorithms by means of mean percentile ranking, a new recall-based evaluation metric in the DTI prediction research field. We anticipate that this review will be objective and helpful to researchers who would like to further improve existing algorithms or need to choose appropriate algorithms to infer potential DTIs in the projects. The source codes for DTI predictions are available at: https://github.com/minghao2016/chemogenomicAlg4DTIpred.},
	number = {4},
	urldate = {2023-10-31},
	journal = {Briefings in Bioinformatics},
	author = {Hao, Ming and Bryant, Stephen H and Wang, Yanli},
	month = jul,
	year = {2019},
	note = {25 citations (Crossref) [2023-10-31]},
	pages = {1465--1474},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\NLY33QRD\\Hao et al. - 2019 - Open-source chemogenomic data-driven algorithms fo.pdf:application/pdf},
}

@article{ezzat_computational_2019,
	title = {Computational prediction of drug–target interactions using chemogenomic approaches: an empirical survey},
	volume = {20},
	issn = {1477-4054},
	shorttitle = {Computational prediction of drug–target interactions using chemogenomic approaches},
	url = {https://doi.org/10.1093/bib/bby002},
	doi = {10.1093/bib/bby002},
	abstract = {Computational prediction of drug–target interactions (DTIs) has become an essential task in the drug discovery process. It narrows down the search space for interactions by suggesting potential interaction candidates for validation via wet-lab experiments that are well known to be expensive and time-consuming. In this article, we aim to provide a comprehensive overview and empirical evaluation on the computational DTI prediction techniques, to act as a guide and reference for our fellow researchers. Specifically, we first describe the data used in such computational DTI prediction efforts. We then categorize and elaborate the state-of-the-art methods for predicting DTIs. Next, an empirical comparison is performed to demonstrate the prediction performance of some representative methods under different scenarios. We also present interesting findings from our evaluation study, discussing the advantages and disadvantages of each method. Finally, we highlight potential avenues for further enhancement of DTI prediction performance as well as related research directions.},
	number = {4},
	urldate = {2023-10-31},
	journal = {Briefings in Bioinformatics},
	author = {Ezzat, Ali and Wu, Min and Li, Xiao-Li and Kwoh, Chee-Keong},
	month = jul,
	year = {2019},
	note = {156 citations (Crossref) [2023-10-31]},
	pages = {1337--1357},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\9T9QPRYD\\Ezzat et al. - 2019 - Computational prediction of drug–target interactio.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\XYRLVQ42\\4824712.html:text/html},
}

@article{ozenne_precisionrecall_2015,
	title = {The precision–recall curve overcame the optimism of the receiver operating characteristic curve in rare diseases},
	volume = {68},
	issn = {0895-4356},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435615001067},
	doi = {10.1016/j.jclinepi.2015.02.010},
	abstract = {Objectives
Compare the area under the receiver operating characteristic curve (AUC) vs. the area under the precision–recall curve (AUPRC) in summarizing the performance of a diagnostic biomarker according to the disease prevalence.
Study Design and Setting
A simulation study was performed considering different sizes of diseased and nondiseased groups. Values of a biomarker were sampled with various variances and differences in mean values between the two groups. The AUCs and the AUPRCs were examined regarding their agreement and vs. the positive predictive value (PPV) and the negative predictive value (NPV) of the biomarker.
Results
With a disease prevalence of 50\%, the AUC and the AUPRC showed high correlations with the PPV and the NPV (ρ {\textgreater} 0.95). With a prevalence of 1\%, small PPV and AUPRC values ({\textless}0.2) but high AUC values ({\textgreater}0.9) were found. The AUPRC reflected better than the AUC the discriminant ability of the biomarker; it had a higher correlation with the PPV (ρ = 0.995 vs. 0.724; P {\textless} 0.001).
Conclusion
In uncommon and rare diseases, the AUPRC should be preferred to the AUC because it summarizes better the performance of a biomarker.},
	number = {8},
	urldate = {2023-10-31},
	journal = {Journal of Clinical Epidemiology},
	author = {Ozenne, Brice and Subtil, Fabien and Maucort-Boulch, Delphine},
	month = aug,
	year = {2015},
	note = {183 citations (Crossref) [2023-10-30]},
	keywords = {Area under the curve, Binary biomarker, Performance assessment, Precision-Recall curve, Rare events, Receiver operating curve},
	pages = {855--859},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\8DLVAUSM\\Ozenne et al. - 2015 - The precision–recall curve overcame the optimism o.pdf:application/pdf},
}

@article{hand_measuring_2009,
	title = {Measuring classifier performance: a coherent alternative to the area under the {ROC} curve},
	volume = {77},
	issn = {1573-0565},
	shorttitle = {Measuring classifier performance},
	url = {https://doi.org/10.1007/s10994-009-5119-5},
	doi = {10.1007/s10994-009-5119-5},
	abstract = {The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p≠P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.},
	language = {en},
	number = {1},
	urldate = {2023-10-31},
	journal = {Machine Learning},
	author = {Hand, David J.},
	month = oct,
	year = {2009},
	note = {626 citations (Crossref) [2023-10-30]},
	keywords = {Classification, AUC, Cost, Error rate, Loss, Misclassification rate, ROC curves, Sensitivity, Specificity},
	pages = {103--123},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\WQBRK3I6\\Hand - 2009 - Measuring classifier performance a coherent alter.pdf:application/pdf},
}

@article{krawczyk_learning_2016,
	title = {Learning from imbalanced data: open challenges and future directions},
	volume = {5},
	issn = {2192-6360},
	shorttitle = {Learning from imbalanced data},
	url = {https://doi.org/10.1007/s13748-016-0094-0},
	doi = {10.1007/s13748-016-0094-0},
	abstract = {Despite more than two decades of continuous development learning from imbalanced data is still a focus of intense research. Starting as a problem of skewed distributions of binary tasks, this topic evolved way beyond this conception. With the expansion of machine learning and data mining, combined with the arrival of big data era, we have gained a deeper insight into the nature of imbalanced learning, while at the same time facing new emerging challenges. Data-level and algorithm-level methods are constantly being improved and hybrid approaches gain increasing popularity. Recent trends focus on analyzing not only the disproportion between classes, but also other difficulties embedded in the nature of data. New real-life problems motivate researchers to focus on computationally efficient, adaptive and real-time methods. This paper aims at discussing open issues and challenges that need to be addressed to further develop the field of imbalanced learning. Seven vital areas of research in this topic are identified, covering the full spectrum of learning from imbalanced data: classification, regression, clustering, data streams, big data analytics and applications, e.g., in social media and computer vision. This paper provides a discussion and suggestions concerning lines of future research for each of them.},
	language = {en},
	number = {4},
	urldate = {2023-10-17},
	journal = {Progress in Artificial Intelligence},
	author = {Krawczyk, Bartosz},
	month = nov,
	year = {2016},
	note = {1239 citations (Crossref) [2023-10-17]},
	keywords = {Imbalanced data, Big data, Data streams, Imbalanced clustering, Imbalanced regression, Machine learning, Multi-class imbalance},
	pages = {221--232},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\NMPJSVEU\\Krawczyk - 2016 - Learning from imbalanced data open challenges and.pdf:application/pdf},
}

@article{ali_error_1996,
	title = {Error reduction through learning multiple descriptions},
	volume = {24},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00058611},
	doi = {10.1007/BF00058611},
	language = {en},
	number = {3},
	urldate = {2023-09-23},
	journal = {Machine Learning},
	author = {Ali, Kamal M. and Pazzani, Michael J.},
	month = sep,
	year = {1996},
	note = {99 citations (Crossref) [2023-09-23]},
	pages = {173--202},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\TAHILZY7\\Ali and Pazzani - 1996 - Error reduction through learning multiple descript.pdf:application/pdf},
}

@article{polikar_ensemble_2006,
	title = {Ensemble based systems in decision making},
	volume = {6},
	issn = {1531-636X},
	url = {http://ieeexplore.ieee.org/document/1688199/},
	doi = {10.1109/MCAS.2006.1688199},
	number = {3},
	urldate = {2023-09-23},
	journal = {IEEE Circuits and Systems Magazine},
	author = {Polikar, R.},
	year = {2006},
	note = {1864 citations (Crossref) [2023-09-23]},
	pages = {21--45},
}

@article{fawagreh_random_2014,
	title = {Random forests: from early developments to recent advancements},
	volume = {2},
	issn = {2164-2583},
	shorttitle = {Random forests},
	url = {http://www.tandfonline.com/doi/abs/10.1080/21642583.2014.956265},
	doi = {10.1080/21642583.2014.956265},
	language = {en},
	number = {1},
	urldate = {2023-09-23},
	journal = {Systems Science \& Control Engineering},
	author = {Fawagreh, Khaled and Gaber, Mohamed Medhat and Elyan, Eyad},
	month = dec,
	year = {2014},
	note = {301 citations (Crossref) [2023-09-23]},
	pages = {602--609},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\IPLHPLU2\\Fawagreh et al. - 2014 - Random forests from early developments to recent .pdf:application/pdf},
}

@article{brown_diversity_2005,
	title = {Diversity creation methods: a survey and categorisation},
	volume = {6},
	issn = {15662535},
	shorttitle = {Diversity creation methods},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253504000375},
	doi = {10.1016/j.inffus.2004.04.004},
	language = {en},
	number = {1},
	urldate = {2023-09-23},
	journal = {Information Fusion},
	author = {Brown, Gavin and Wyatt, Jeremy and Harris, Rachel and Yao, Xin},
	month = mar,
	year = {2005},
	note = {668 citations (Crossref) [2023-09-23]},
	pages = {5--20},
	file = {Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\RWLSPCTY\\Brown et al. - 2005 - Diversity creation methods a survey and categoris.pdf:application/pdf},
}

@misc{stock_efficient_2016,
	title = {Efficient {Pairwise} {Learning} {Using} {Kernel} {Ridge} {Regression}: an {Exact} {Two}-{Step} {Method}},
	shorttitle = {Efficient {Pairwise} {Learning} {Using} {Kernel} {Ridge} {Regression}},
	url = {http://arxiv.org/abs/1606.04275},
	doi = {10.48550/arXiv.1606.04275},
	abstract = {Pairwise learning or dyadic prediction concerns the prediction of properties for pairs of objects. It can be seen as an umbrella covering various machine learning problems such as matrix completion, collaborative filtering, multi-task learning, transfer learning, network prediction and zero-shot learning. In this work we analyze kernel-based methods for pairwise learning, with a particular focus on a recently-suggested two-step method. We show that this method offers an appealing alternative for commonly-applied Kronecker-based methods that model dyads by means of pairwise feature representations and pairwise kernels. In a series of theoretical results, we establish correspondences between the two types of methods in terms of linear algebra and spectral filtering, and we analyze their statistical consistency. In addition, the two-step method allows us to establish novel algorithmic shortcuts for efficient training and validation on very large datasets. Putting those properties together, we believe that this simple, yet powerful method can become a standard tool for many problems. Extensive experimental results for a range of practical settings are reported.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Stock, Michiel and Pahikkala, Tapio and Airola, Antti and De Baets, Bernard and Waegeman, Willem},
	month = jun,
	year = {2016},
	note = {arXiv:1606.04275 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\2F7FFFGL\\Stock et al. - 2016 - Efficient Pairwise Learning Using Kernel Ridge Reg.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\I7666ASN\\1606.html:text/html},
}

@article{ozturk_deepdta_2018,
	title = {{DeepDTA}: deep drug–target binding affinity prediction},
	volume = {34},
	number = {17},
	journal = {Bioinformatics},
	author = {Öztürk, Hakime and Özgür, Arzucan and Ozkirimli, Elif},
	year = {2018},
	note = {Publisher: Oxford University Press},
	keywords = {deep learning, drug-target, predictor},
	pages = {i821--i829},
}

@article{qian_mcl-dti_2023,
	title = {{MCL}-{DTI}: using drug multimodal information and bi-directional cross-attention learning method for predicting drug–target interaction},
	volume = {24},
	issn = {1471-2105},
	shorttitle = {{MCL}-{DTI}},
	url = {https://doi.org/10.1186/s12859-023-05447-1},
	doi = {10.1186/s12859-023-05447-1},
	abstract = {Prediction of drug–target interaction (DTI) is an essential step for drug discovery and drug reposition. Traditional methods are mostly time-consuming and labor-intensive, and deep learning-based methods address these limitations and are applied to engineering. Most of the current deep learning methods employ representation learning of unimodal information such as SMILES sequences, molecular graphs, or molecular images of drugs. In addition, most methods focus on feature extraction from drug and target alone without fusion learning from drug–target interacting parties, which may lead to insufficient feature representation.},
	number = {1},
	urldate = {2023-08-30},
	journal = {BMC Bioinformatics},
	author = {Qian, Ying and Li, Xinyi and Wu, Jian and Zhang, Qian},
	month = aug,
	year = {2023},
	note = {0 citations (Crossref) [2023-08-29]},
	keywords = {Cross-attention mechanism, Deep learning, Drug–target interaction, Multi-head self-attention mechanism, Multimodal information},
	pages = {323},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\XYV7BTCU\\Qian et al. - 2023 - MCL-DTI using drug multimodal information and bi-.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\Y4EVU4NV\\s12859-023-05447-1.html:text/html},
}

@article{pahikkala_toward_2015,
	title = {Toward more realistic drug-target interaction predictions},
	volume = {16},
	url = {http://dx.doi.org/10.1093/bib/bbu010},
	doi = {10.1093/bib/bbu010},
	number = {2},
	journal = {Briefings in Bioinformatics},
	author = {Pahikkala, Tapio and Airola, Antti and Pietilä, Sami and Shakyawar, Sushil and Szwajda, Agnieszka and Tang, Jing and Aittokallio, Tero},
	year = {2015},
	note = {261 citations (Crossref) [2023-08-29]},
	pages = {325--337},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\ZUCZUUWS\\Pahikkala et al. - 2015 - Toward more realistic drug-target interaction pred.pdf:application/pdf},
}

@article{adiyeke_semi-supervised_2022,
	title = {Semi-supervised extensions of multi-task tree ensembles},
	volume = {123},
	url = {https://doi.org/10.1016%2Fj.patcog.2021.108393},
	doi = {10.1016/j.patcog.2021.108393},
	journal = {Pattern Recognition},
	author = {Adıyeke, Esra and Baydoğan, Mustafa GÃ¶kçe},
	month = mar,
	year = {2022},
	note = {2 citations (Crossref) [2023-08-29]
Publisher: Elsevier BV},
	pages = {108393},
	file = {Adıyeke and Baydoğan - 2022 - Semi-supervised extensions of multi-task tree ense.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\YIM5FJ8E\\Adıyeke and Baydoğan - 2022 - Semi-supervised extensions of multi-task tree ense.pdf:application/pdf},
}

@article{bleakley_supervised_2009,
	title = {Supervised prediction of drug–target interactions using bipartite local models},
	volume = {25},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btp433},
	doi = {10.1093/bioinformatics/btp433},
	number = {18},
	journal = {Bioinformatics},
	author = {Bleakley, Kevin and Yamanishi, Yoshihiro},
	month = jul,
	year = {2009},
	note = {454 citations (Crossref) [2023-08-29]
\_eprint: https://academic.oup.com/bioinformatics/article-pdf/25/18/2397/48994046/bioinformatics\_25\_18\_2397.pdf},
	pages = {2397--2403},
}

@article{bagherian_machine_2020,
	title = {Machine learning approaches and databases for prediction of drug–target interaction: a survey paper},
	volume = {22},
	url = {https://doi.org/10.1093%2Fbib%2Fbbz157},
	doi = {10.1093/bib/bbz157},
	number = {1},
	journal = {Briefings in Bioinformatics},
	author = {Bagherian, Maryam and Sabeti, Elyas and Wang, Kai and Sartor, Maureen A. and Nikolovska-Coleska, Zaneta and Najarian, Kayvan},
	month = jan,
	year = {2020},
	note = {157 citations (Crossref) [2023-08-29]
Publisher: Oxford University Press (OUP)},
	pages = {247--269},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\7M2BDEL5\\Bagherian et al. - 2020 - Machine learning approaches and databases for pred.pdf:application/pdf},
}

@article{yamanishi_drug-target_2010,
	title = {Drug-target interaction prediction from chemical, genomic and pharmacological data in an integrated framework},
	volume = {26},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btq176},
	doi = {10.1093/bioinformatics/btq176},
	number = {12},
	journal = {Bioinformatics},
	author = {Yamanishi, Yoshihiro and Kotera, Masaaki and Kanehisa, Minoru and Goto, Susumu},
	month = jun,
	year = {2010},
	note = {359 citations (Crossref) [2023-08-29]
\_eprint: https://academic.oup.com/bioinformatics/article-pdf/26/12/i246/48859547/bioinformatics\_26\_12\_i246.pdf},
	pages = {i246--i254},
}

@incollection{noauthor_notitle_nodate-1,
}

@inproceedings{ilidio_fast_2024,
	address = {New York, NY, USA},
	series = {{SAC} '24},
	title = {Fast {Bipartite} {Forests} for {Semi}-supervised {Interaction} {Prediction}},
	isbn = {9798400702433},
	url = {https://dl.acm.org/doi/10.1145/3605098.3636071},
	doi = {10.1145/3605098.3636071},
	abstract = {Numerous machine learning tasks can be framed as the prediction of interactions in a bipartite network, such as relationships between proteins and drug molecules, genes and transcription factors, or microRNAs and messenger RNAs. Such tasks present unique characteristics and challenges often overlooked by existing strategies, namely the high dimensionality and sparsity of available labels or the infeasibility of validating predictions for all possible pairs of entities. In this study, we investigate machine learning approaches tailored to these settings and propose refinements in forest algorithms shown to improve complexity by a factor of log n. We systematically re-implement and compare predecessor strategies on ten bipartite datasets, shedding light on their performance across diverse interaction prediction contexts. Bipartite decision forests were shown to overcome several traditional algorithms in binary interaction prediction and to surpass cutting-edge deep learning models in regressive drug-protein affinity prediction. We also assess the impact of missing positive interactions, comparing existing and newly proposed tree-based semi-supervised approaches. Our proposed forests employing semi-supervised impurities are shown to display notable resiliency in such a scenario, an especially relevant result in the realm of interaction prediction.},
	urldate = {2024-05-22},
	booktitle = {Proceedings of the 39th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Ilídio, Pedro and Alves, André and Cerri, Ricardo},
	month = may,
	year = {2024},
	keywords = {machine learning, bipartite networks, decision forests, interaction prediction, semi-supervised decision trees},
	pages = {979--986},
}

@article{kim_hybrid_2016,
	title = {A hybrid classification algorithm by subspace partitioning through semi-supervised decision tree},
	volume = {60},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316300620},
	doi = {10.1016/j.patcog.2016.04.016},
	abstract = {Among data mining techniques, the decision tree is one of the more widely used methods for building classification models in the real world because of its simplicity and ease of interpretation. However, the method has some drawbacks, including instability, the nonsmooth nature of the decision boundary, and the possibility of overfitting. To overcome these problems, several works have utilized the relative advantages of other classifiers, such as logistic regression, support vector machine, and neural networks, in combination with a decision tree, in hybrid models which avoid the drawbacks of other models. Some hybrid models have used decision trees to quickly and efficiently partition the input space, and many studies have proved the effectiveness of the hybrid methods. However, there is room for further improvement by considering the topological properties of a dataset, because typical decision trees split nodes based only on the target variable. The proposed semi-supervised decision tree splits internal nodes by utilizing both labels and the structural characteristics of data for subspace partitioning, to improve the accuracy of classifiers applied to terminal nodes in the hybrid models. Experimental results confirm the superiority of the proposed algorithm and demonstrate the detailed characteristics of the algorithm.},
	urldate = {2024-05-22},
	journal = {Pattern Recognition},
	author = {Kim, Kyoungok},
	month = dec,
	year = {2016},
	keywords = {Decision tree, Inhomogeneous measure, Semi-supervised decision tree, Subspace partitioning},
	pages = {157--163},
}

@article{levatic_semi-supervised_2018,
	title = {Semi-supervised trees for multi-target regression},
	volume = {450},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S002002551830210X},
	doi = {10.1016/j.ins.2018.03.033},
	abstract = {The predictive performance of traditional supervised methods heavily depends on the amount of labeled data. However, obtaining labels is a difficult process in many real-life tasks, and only a small amount of labeled data is typically available for model learning. As an answer to this problem, the concept of semi-supervised learning has emerged. Semi-supervised methods use unlabeled data in addition to labeled data to improve the performance of supervised methods. It is even more difficult to get labeled data for data mining problems with structured outputs since several labels need to be determined for each example. Multi-target regression (MTR) is one type of a structured output prediction problem, where we need to simultaneously predict multiple continuous variables. Despite the apparent need for semi-supervised methods able to deal with MTR, only a few such methods are available and even those are difficult to use in practice and/or their advantages over supervised methods for MTR are not clear. This paper presents an extension of predictive clustering trees for MTR and ensembles thereof towards semi-supervised learning. The proposed method preserves the appealing characteristic of decision trees while enabling the use of unlabeled examples. In particular, the proposed semi-supervised trees for MTR are interpretable, easy to understand, fast to learn, and can handle both numeric and nominal descriptive features. We perform an extensive empirical evaluation in both an inductive and a transductive semi-supervised setting. The results show that the proposed method improves the performance of supervised predictive clustering trees and enhances their interpretability (due to reduced tree size), whereas, in the ensemble learning scenario, it outperforms its supervised counterpart in the transductive setting. The proposed methods have a mechanism for controlling the influence of unlabeled examples, which makes them highly useful in practice: This mechanism can protect them against a degradation of performance of their supervised counterparts – an inherent risk of semi-supervised learning. The proposed methods also outperform two existing semi-supervised methods for MTR.},
	urldate = {2024-05-22},
	journal = {Information Sciences},
	author = {Levatić, Jurica and Kocev, Dragi and Ceci, Michelangelo and Džeroski, Sašo},
	month = jun,
	year = {2018},
	keywords = {Semi-supervised learning, Multi-target regression, Predictive clustering trees, Random forests, Structured outputs},
	pages = {109--127},
	file = {Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\RLSM4255\\Levatić et al. - 2018 - Semi-supervised trees for multi-target regression.pdf:application/pdf},
}

@article{tanha_semi-supervised_2017,
	title = {Semi-supervised self-training for decision tree classifiers},
	volume = {8},
	issn = {1868-808X},
	url = {https://doi.org/10.1007/s13042-015-0328-7},
	doi = {10.1007/s13042-015-0328-7},
	abstract = {We consider semi-supervised learning, learning task from both labeled and unlabeled instances and in particular, self-training with decision tree learners as base learners. We show that standard decision tree learning as the base learner cannot be effective in a self-training algorithm to semi-supervised learning. The main reason is that the basic decision tree learner does not produce reliable probability estimation to its predictions. Therefore, it cannot be a proper selection criterion in self-training. We consider the effect of several modifications to the basic decision tree learner that produce better probability estimation than using the distributions at the leaves of the tree. We show that these modifications do not produce better performance when used on the labeled data only, but they do benefit more from the unlabeled data in self-training. The modifications that we consider are Naive Bayes Tree, a combination of No-pruning and Laplace correction, grafting, and using a distance-based measure. We then extend this improvement to algorithms for ensembles of decision trees and we show that the ensemble learner gives an extra improvement over the adapted decision tree learners.},
	language = {en},
	number = {1},
	urldate = {2024-05-22},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Tanha, Jafar and van Someren, Maarten and Afsarmanesh, Hamideh},
	month = feb,
	year = {2017},
	keywords = {Ensemble learning, Semi-supervised learning, Decision tree learning, Self-training},
	pages = {355--370},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\2MT4S926\\Tanha et al. - 2017 - Semi-supervised self-training for decision tree cl.pdf:application/pdf},
}

@article{ait_hammou_effective_2019,
	title = {An effective distributed predictive model with {Matrix} factorization and random forest for {Big} {Data} recommendation systems},
	volume = {137},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304464},
	doi = {10.1016/j.eswa.2019.06.046},
	abstract = {Recommendation systems have been widely deployed to address the challenge of overwhelming information. They are used to enable users to find interesting information from a large volume of data. However, in the era of Big Data, as data become larger and more complicated, a recommendation algorithm that runs in a traditional environment cannot be fast and effective. It requires a high computational cost for performing the training task, which may limit its applicability in real-world Big Data applications. In this paper, we propose a novel distributed recommendation solution for Big Data. It is designed based on Apache Spark to handle large-scale data, improve the prediction quality, and address the data sparsity problem. In particular, thanks to a novel learning process, the model is able to significantly speed up the distributed training, as well as improve the performance in the context of Big Data. Experimental results on three real-world data sets demonstrate that our proposal outperforms existing recommendation methods in terms of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and computational time.},
	urldate = {2024-05-22},
	journal = {Expert Systems with Applications},
	author = {Ait Hammou, Badr and Ait Lahcen, Ayoub and Mouline, Salma},
	month = dec,
	year = {2019},
	keywords = {Random forest, Apache spark, Big Data, Distributed computing, Matrix factorization, Recommendation systems},
	pages = {253--265},
}

@article{panagiotakis_dual_2021,
	title = {A dual hybrid recommender system based on {SCoR} and the random forest},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {1820-0214, 2406-1018},
	url = {https://doiserbia.nb.rs/Article.aspx?ID=1820-02142000046P},
	doi = {10.2298/CSIS200515046P},
	abstract = {We propose a Dual Hybrid Recommender System based on SCoR, the Synthetic Coordinate Recommendation system, and the Random Forest method. By combining user ratings and user/item features, SCoR is initially employed to provide a recommendation which is fed into the Random Forest. The two systems are initially combined by splitting the training set into two “equivalent” parts, one of which is used to train SCoR while the other is used to train the Random Forest. This initial approach does not exhibit good performance due to reduced training. The resulted drawback is alleviated by the proposed dual training system which, using an innovative splitting method, exploits the entire training set for SCoR and the Random Forest, resulting to two recommender systems that are subsequently efﬁciently combined. Experimental results demonstrate the high performance of the proposed system on the Movielens datasets.},
	language = {en},
	number = {1},
	urldate = {2024-05-22},
	journal = {Computer Science and Information Systems},
	author = {Panagiotakis, Costas and Papadakis, Harris and Fragopoulou, Paraskevi},
	year = {2021},
	pages = {115--128},
	file = {Panagiotakis et al. - 2021 - A dual hybrid recommender system based on SCoR and.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\BFC5DA8N\\Panagiotakis et al. - 2021 - A dual hybrid recommender system based on SCoR and.pdf:application/pdf},
}

@article{panagiotakis_dual_2021-1,
	title = {A dual hybrid recommender system based on {SCoR} and the random forest},
	volume = {18},
	url = {https://doiserbia.nb.rs/Article.aspx?ID=1820-02142000046P},
	abstract = {We propose a Dual Hybrid Recommender System based on SCoR, the Synthetic Coordinate Recommendation system, and the Random Forest method. By combining user ratings and user/item features, SCoR is initially employed to provide a recommendation which is fed into the Random Forest. The two systems are initially combined by splitting the training set into two “equivalent” parts, one of which is used to train SCoR while the other is used to train the Random Forest. This initial approach does not exhibit good performance due to reduced training. The resulted drawback is alleviated by the proposed dual training system which, using an innovative splitting method, exploits the entire training set for SCoR and the Random Forest, resulting to two recommender systems that are subsequently efficiently combined. Experimental results demonstrate the high performance of the proposed system on the Movielens datasets.},
	number = {1},
	urldate = {2024-05-22},
	journal = {Computer Science and Information Systems},
	author = {Panagiotakis, Costas and Papadakis, Harris and Fragopoulou, Paraskevi},
	year = {2021},
	pages = {115--128},
}

@article{li_multi-dimensional_2018,
	title = {A {Multi}-{Dimensional} {Context}-{Aware} {Recommendation} {Approach} {Based} on {Improved} {Random} {Forest} {Algorithm}},
	volume = {6},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/OAPA.html},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8435916/},
	doi = {10.1109/ACCESS.2018.2865436},
	urldate = {2024-05-22},
	journal = {IEEE Access},
	author = {Li, Xiang and Wang, Zhijian and Wang, Liuyang and Hu, Ronglin and Zhu, Quanyin},
	year = {2018},
	pages = {45071--45085},
}

@article{kim_hybrid_2016-1,
	title = {A hybrid classification algorithm by subspace partitioning through semi-supervised decision tree},
	volume = {60},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316300620},
	doi = {10.1016/j.patcog.2016.04.016},
	abstract = {Among data mining techniques, the decision tree is one of the more widely used methods for building classification models in the real world because of its simplicity and ease of interpretation. However, the method has some drawbacks, including instability, the nonsmooth nature of the decision boundary, and the possibility of overfitting. To overcome these problems, several works have utilized the relative advantages of other classifiers, such as logistic regression, support vector machine, and neural networks, in combination with a decision tree, in hybrid models which avoid the drawbacks of other models. Some hybrid models have used decision trees to quickly and efficiently partition the input space, and many studies have proved the effectiveness of the hybrid methods. However, there is room for further improvement by considering the topological properties of a dataset, because typical decision trees split nodes based only on the target variable. The proposed semi-supervised decision tree splits internal nodes by utilizing both labels and the structural characteristics of data for subspace partitioning, to improve the accuracy of classifiers applied to terminal nodes in the hybrid models. Experimental results confirm the superiority of the proposed algorithm and demonstrate the detailed characteristics of the algorithm.},
	urldate = {2024-05-22},
	journal = {Pattern Recognition},
	author = {Kim, Kyoungok},
	month = dec,
	year = {2016},
	keywords = {Decision tree, Inhomogeneous measure, Semi-supervised decision tree, Subspace partitioning},
	pages = {157--163},
}

@book{antonenko_chains_2023,
	title = {Chains of {Autoreplicative} {Random} {Forests} for missing value imputation in high-dimensional datasets},
	abstract = {Missing values are a common problem in data science and machine learning. Removing instances with missing values can adversely affect the quality of further data analysis. This is exacerbated when there are relatively many more features than instances, and thus the proportion of affected instances is high. Such a scenario is common in many important domains, for example, single nucleotide polymorphism (SNP) datasets provide a large number of features over a genome for a relatively small number of individuals. To preserve as much information as possible prior to modeling, a rigorous imputation scheme is acutely needed. While Denoising Autoencoders is a state-of-the-art method for imputation in high-dimensional data, they still require enough complete cases to be trained on which is often not available in real-world problems. In this paper, we consider missing value imputation as a multi-label classification problem and propose Chains of Autoreplicative Random Forests. Using multi-label Random Forests instead of neural networks works well for low-sampled data as there are fewer parameters to optimize. Experiments on several SNP datasets show that our algorithm effectively imputes missing values based only on information from the dataset and exhibits better performance than standard algorithms that do not require any additional information. In this paper, the algorithm is implemented specifically for SNP data, but it can easily be adapted for other cases of missing value imputation.},
	author = {Antonenko, Ekaterina and Read, Jesse},
	month = jan,
	year = {2023},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\F4CA7UHJ\\Antonenko and Read - 2023 - Chains of Autoreplicative Random Forests for missi.pdf:application/pdf},
}

@article{costa_recent_2023,
	title = {Recent advances in decision trees: an updated survey},
	volume = {56},
	issn = {1573-7462},
	shorttitle = {Recent advances in decision trees},
	url = {https://doi.org/10.1007/s10462-022-10275-5},
	doi = {10.1007/s10462-022-10275-5},
	abstract = {Decision Trees (DTs) are predictive models in supervised learning, known not only for their unquestionable utility in a wide range of applications but also for their interpretability and robustness. Research on the subject is still going strong after almost 60 years since its original inception, and in the last decade, several researchers have tackled key matters in the field. Although many great surveys have been published in the past, there is a gap since none covers the last decade of the field as a whole. This paper proposes a review of the main recent advances in DT research, focusing on three major goals of a predictive learner: issues regarding the fitting of training data, generalization, and interpretability. Moreover, by organizing several topics that have been previously analyzed in isolation, this survey attempts to provide an overview of the field, its key concerns, and future trends, serving as a good entry point for both researchers and newcomers to the machine learning community.},
	language = {en},
	number = {5},
	urldate = {2024-05-22},
	journal = {Artificial Intelligence Review},
	author = {Costa, Vinícius G. and Pedreira, Carlos E.},
	month = may,
	year = {2023},
	keywords = {Machine learning, Classification algorithms, Decision trees, Interpretable models},
	pages = {4765--4800},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\KFENFZMT\\Costa and Pedreira - 2023 - Recent advances in decision trees an updated surv.pdf:application/pdf},
}

@article{ait_hammou_effective_2019-1,
	title = {An effective distributed predictive model with {Matrix} factorization and random forest for {Big} {Data} recommendation systems},
	volume = {137},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304464},
	doi = {10.1016/j.eswa.2019.06.046},
	abstract = {Recommendation systems have been widely deployed to address the challenge of overwhelming information. They are used to enable users to find interesting information from a large volume of data. However, in the era of Big Data, as data become larger and more complicated, a recommendation algorithm that runs in a traditional environment cannot be fast and effective. It requires a high computational cost for performing the training task, which may limit its applicability in real-world Big Data applications. In this paper, we propose a novel distributed recommendation solution for Big Data. It is designed based on Apache Spark to handle large-scale data, improve the prediction quality, and address the data sparsity problem. In particular, thanks to a novel learning process, the model is able to significantly speed up the distributed training, as well as improve the performance in the context of Big Data. Experimental results on three real-world data sets demonstrate that our proposal outperforms existing recommendation methods in terms of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and computational time.},
	urldate = {2024-05-18},
	journal = {Expert Systems with Applications},
	author = {Ait Hammou, Badr and Ait Lahcen, Ayoub and Mouline, Salma},
	month = dec,
	year = {2019},
	keywords = {Random forest, Apache spark, Big Data, Distributed computing, Matrix factorization, Recommendation systems},
	pages = {253--265},
}

@misc{noauthor_effective_nodate,
	title = {An effective distributed predictive model with {Matrix} factorization and random forest for {Big} {Data} recommendation systems - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419304464},
	urldate = {2024-05-18},
}

@inproceedings{khanvilkar_smart_2019,
	title = {Smart {Recommendation} {System} {Based} on {Product} {Reviews} {Using} {Random} {Forest}},
	url = {https://ieeexplore.ieee.org/abstract/document/8945855},
	doi = {10.1109/ICNTE44896.2019.8945855},
	abstract = {Social network, e-commerce sites, blogs are new emerging platforms for people to express their opinion. These sites contain huge amount of text which can be used for different purpose like Sentiment Analysis. Sentiment Analysis is a growing field in natural language processing. Sentiment analysis is major focused on company's improvement. But sentiment analysis can be useful in recommendation system also. Based on various performance measures, this paper compares the results of machine learning algorithms like Multinomial Naive Bayes algorithm, Logistic Regression, SVM Classifier, Decision Tree and Random Forest. These algorithms are used for sentiment analysis of reviews and in turn for product recommendation. In proposed system, Random Forest shows outstanding performance. To create suitable recommendations using the analysis of emotions, there is a need to use polarity obtained through the reviews.},
	urldate = {2024-05-18},
	booktitle = {2019 {International} {Conference} on {Nascent} {Technologies} in {Engineering} ({ICNTE})},
	author = {Khanvilkar, Gayatri and Vora, Deepali},
	month = jan,
	year = {2019},
	keywords = {Random forests, Classification algorithms, Decision trees, Logistics, Machine learning algorithms, NLP, Product Recommendation, Random Forest, Reviews, Sentiment analysis, Sentiment Analysis, Support vector machines},
	pages = {1--9},
}

@article{urdaneta-ponte_recommendation_2021,
	title = {Recommendation {Systems} for {Education}: {Systematic} {Review}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Recommendation {Systems} for {Education}},
	url = {https://www.mdpi.com/2079-9292/10/14/1611},
	doi = {10.3390/electronics10141611},
	abstract = {Recommendation systems have emerged as a response to overload in terms of increased amounts of information online, which has become a problem for users regarding the time spent on their search and the amount of information retrieved by it. In the field of recommendation systems in education, the relevance of recommended educational resources will improve the student’s learning process, and hence the importance of being able to suitably and reliably ensure relevant, useful information. The purpose of this systematic review is to analyze the work undertaken on recommendation systems that support educational practices with a view to acquiring information related to the type of education and areas dealt with, the developmental approach used, and the elements recommended, as well as being able to detect any gaps in this area for future research work. A systematic review was carried out that included 98 articles from a total of 2937 found in main databases (IEEE, ACM, Scopus and WoS), about which it was able to be established that most are geared towards recommending educational resources for users of formal education, in which the main approaches used in recommendation systems are the collaborative approach, the content-based approach, and the hybrid approach, with a tendency to use machine learning in the last two years. Finally, possible future areas of research and development in this field are presented.},
	language = {en},
	number = {14},
	urldate = {2024-05-18},
	journal = {Electronics},
	author = {Urdaneta-Ponte, María Cora and Mendez-Zorrilla, Amaia and Oleagordia-Ruiz, Ibon},
	month = jan,
	year = {2021},
	note = {Number: 14
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, education, recommendation systems, systematic review},
	pages = {1611},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\QI8DE8B3\\Urdaneta-Ponte et al. - 2021 - Recommendation Systems for Education Systematic R.pdf:application/pdf},
}

@article{ko_survey_2022,
	title = {A {Survey} of {Recommendation} {Systems}: {Recommendation} {Models}, {Techniques}, and {Application} {Fields}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {A {Survey} of {Recommendation} {Systems}},
	url = {https://www.mdpi.com/2079-9292/11/1/141},
	doi = {10.3390/electronics11010141},
	abstract = {This paper reviews the research trends that link the advanced technical aspects of recommendation systems that are used in various service areas and the business aspects of these services. First, for a reliable analysis of recommendation models for recommendation systems, data mining technology, and related research by application service, more than 135 top-ranking articles and top-tier conferences published in Google Scholar between 2010 and 2021 were collected and reviewed. Based on this, studies on recommendation system models and the technology used in recommendation systems were systematized, and research trends by year were analyzed. In addition, the application service fields where recommendation systems were used were classified, and research on the recommendation system model and recommendation technique used in each field was analyzed. Furthermore, vast amounts of application service-related data used by recommendation systems were collected from 2010 to 2021 without taking the journal ranking into consideration and reviewed along with various recommendation system studies, as well as applied service field industry data. As a result of this study, it was found that the flow and quantitative growth of various detailed studies of recommendation systems interact with the business growth of the actual applied service field. While providing a comprehensive summary of recommendation systems, this study provides insight to many researchers interested in recommendation systems through the analysis of its various technologies and trends in the service field to which recommendation systems are applied.},
	language = {en},
	number = {1},
	urldate = {2024-05-18},
	journal = {Electronics},
	author = {Ko, Hyeyoung and Lee, Suyeon and Park, Yoonseo and Choi, Anna},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {collaborative filtering, content-based filtering, hybrid system, recommendation algorithm, recommendation system, recommendation technique, recommender system},
	pages = {141},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\IJIW2S97\\Ko et al. - 2022 - A Survey of Recommendation Systems Recommendation.pdf:application/pdf},
}

@book{cormen_introduction_2022,
	address = {Cambridge},
	edition = {4th},
	title = {Introduction to algorithms},
	isbn = {978-0-262-04630-5},
	language = {English},
	publisher = {MIT Press},
	author = {Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
	year = {2022},
	file = {Introduction.to.Algorithms.4th.Leiserson.Stein.Rivest.Cormen.MIT.Press.9780262046305.EBooksWorld.ir.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\BDIF8AKY\\Introduction.to.Algorithms.4th.Leiserson.Stein.Rivest.Cormen.MIT.Press.9780262046305.EBooksWorld.ir.pdf:application/pdf},
}

@article{van_engelen_survey_2020,
	title = {A survey on semi-supervised learning},
	volume = {109},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-019-05855-6},
	doi = {10.1007/s10994-019-05855-6},
	abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
	language = {en},
	number = {2},
	urldate = {2024-03-15},
	journal = {Machine Learning},
	author = {VAN ENGELEN, Jesper E. and Hoos, Holger H.},
	month = feb,
	year = {2020},
	note = {1213 citations (Crossref) [2024-03-15]},
	keywords = {Classification, Semi-supervised learning, Machine learning},
	pages = {373--440},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\6P8PULWL\\van Engelen and Hoos - 2020 - A survey on semi-supervised learning.pdf:application/pdf},
}

@inproceedings{boyd_area_2013,
	address = {Berlin},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Area under the precision-recall curve: point estimates and confidence intervals},
	isbn = {978-3-642-40994-3},
	shorttitle = {Area under the {Precision}-{Recall} {Curve}},
	doi = {10.1007/978-3-642-40994-3_29},
	abstract = {The area under the precision-recall curve (AUCPR) is a single number summary of the information in the precision-recall (PR) curve. Similar to the receiver operating characteristic curve, the PR curve has its own unique properties that make estimating its enclosed area challenging. Besides a point estimate of the area, an interval estimate is often required to express magnitude and uncertainty. In this paper we perform a computational analysis of common AUCPR estimators and their confidence intervals. We find both satisfactory estimates and invalid procedures and we recommend two simple intervals that are robust to a variety of assumptions.},
	language = {en},
	booktitle = {Machine learning and knowledge discovery in databases},
	publisher = {Springer},
	author = {Boyd, Kendrick and Eng, Kevin H. and Page, C. David},
	editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Železný, Filip},
	year = {2013},
	note = {organization= MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, 2013, Berlin},
	keywords = {Average Precision, Bias Ratio, Markov Logic Network, Receiver Operating Character, Roswell Park Cancer Institute},
	pages = {451--466},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\RNSY39NB\\Boyd et al. - 2013 - Area under the Precision-Recall Curve Point Estim.pdf:application/pdf},
}

@book{asratian_bipartite_1998,
	address = {Cambridge},
	series = {Cambridge tracts in mathematics},
	title = {Bipartite graphs and their applications},
	volume = {131},
	publisher = {Cambridge University Press},
	author = {Asratian, Armen S and Denley, Tristan MJ and Häggkvist, Roland},
	year = {1998},
}

@article{ho_random_1998,
	title = {The random subspace method for constructing decision forests},
	volume = {20},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/709601/},
	doi = {10.1109/34.709601},
	number = {8},
	urldate = {2023-09-23},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ho, Tin Kam},
	month = aug,
	year = {1998},
	note = {4272 citations (Crossref) [2023-09-23]},
	pages = {832--844},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\T4QDXYMR\\Tin Kam Ho - 1998 - The random subspace method for constructing decisi.pdf:application/pdf},
}

@inproceedings{santos_predictive_2021,
	address = {Cham},
	title = {Predictive bi-clustering trees for hierarchical multi-label classification},
	isbn = {978-3-030-67664-3},
	doi = {10.1007/978-3-030-67664-3_42},
	abstract = {In the recent literature on multi-label classification, a lot of attention is given to methods that exploit label dependencies. Most of these methods assume that the dependencies are static over the entire instance space. In contrast, here we present an approach that dynamically adapts the label partitions in a multi-label decision tree learning context. In particular, we adapt the recently introduced predictive bi-clustering tree (PBCT) method towards multi-label classification tasks. This way, tree nodes can split the instance-label matrix both in a horizontal and a vertical way. We focus on hierarchical multi-label classification (HMC) tasks, and map the label hierarchy to a feature set over the label space. This feature set is exploited to infer vertical splits, which are regulated by a lookahead strategy in the tree building procedure. We evaluate our proposed method using benchmark datasets. Experiments demonstrate that our proposal (PBCT-HMC) obtained better or competitive results in comparison to its direct competitors, both in terms of predictive performance and model size. Compared to an HMC method that does not produce label partitions though, our method results in larger models on average, while still producing equally large or smaller models in one third of the datasets by creating suitable label partitions.},
	language = {en},
	booktitle = {Machine learning and knowledge discovery in databases},
	publisher = {Springer International Publishing},
	author = {Santos, Bruna Z. and Nakano, Felipe K. and Cerri, Ricardo and Vens, Celine},
	editor = {Hutter, Frank and Kersting, Kristian and Lijffijt, Jefrey and Valera, Isabel},
	year = {2021},
	note = {organization= MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, 2021, Cham},
	keywords = {Predictive clustering trees, Bi-clustering, Hierarchical multi-label classification},
	pages = {701--718},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\LAN99SQM\\Santos et al. - 2021 - Predictive Bi-clustering Trees for Hierarchical Mu.pdf:application/pdf},
}

@inproceedings{pahikkala_two-step_2014,
	address = {Berlin},
	title = {A two-step learning approach for solving full and almost full cold start problems in dyadic prediction},
	isbn = {978-3-662-44851-9},
	doi = {10.1007/978-3-662-44851-9_33},
	abstract = {Dyadic prediction methods operate on pairs of objects (dyads), aiming to infer labels for out-of-sample dyads. We consider the full and almost full cold start problem in dyadic prediction, a setting that occurs when both objects in an out-of-sample dyad have not been observed during training, or if one of them has been observed, but very few times. A popular approach for addressing this problem is to train a model that makes predictions based on a pairwise feature representation of the dyads, or, in case of kernel methods, based on a tensor product pairwise kernel. As an alternative to such a kernel approach, we introduce a novel two-step learning algorithm that borrows ideas from the fields of pairwise learning and spectral filtering. We show theoretically that the two-step method is very closely related to the tensor product kernel approach, and experimentally that it yields a slightly better predictive performance. Moreover, unlike existing tensor product kernel methods, the two-step method allows closed-form solutions for training and parameter selection via cross-validation estimates both in the full and almost full cold start settings, making the approach much more efficient and straightforward to implement.},
	language = {en},
	booktitle = {Machine learning and knowledge discovery in databases},
	publisher = {Springer},
	author = {Pahikkala, Tapio and Stock, Michiel and Airola, Antti and Aittokallio, Tero and De Baets, Bernard and Waegeman, Willem},
	editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
	year = {2014},
	note = {organization= MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, 2014, Berlin},
	keywords = {Dyadic prediction, kernel methods, kernel ridge regression, pairwise learning, transfer learning},
	pages = {517--532},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\BCZAF3PQ\\Pahikkala et al. - 2014 - A Two-Step Learning Approach for Solving Full and .pdf:application/pdf},
}

@book{lodhi_elements_2010,
	address = {Hoboken},
	title = {Elements of computational systems biology},
	isbn = {978-0-470-18093-8 978-0-470-55675-7},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470556757},
	language = {en},
	urldate = {2023-08-29},
	publisher = {Wiley},
	editor = {Lodhi, Huma M. and Muggleton, Stephen H.},
	month = jan,
	year = {2010},
	doi = {10.1002/9780470556757},
	file = {[Wiley Series in Bioinformatics] Huma M. Lodhi, Stephen H. Muggleton - Elements of Computational Systems Biology (Wiley Series in Bioinformatics) (2010, Wiley) - libgen.li.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\PSW9RM3V\\[Wiley Series in Bioinformatics] Huma M. Lodhi, Stephen H. Muggleton - Elements of Computational Systems Biology (Wiley Series in Bioinformatics) (2010, Wiley) - libgen.li.pdf:application/pdf},
}

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	note = {Publisher: Springer},
	pages = {5--32},
}

@article{macisaac_improved_2006,
	title = {An improved map of conserved regulatory sites for {Saccharomyces} cerevisiae},
	volume = {7},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {MacIsaac, Kenzie D and Wang, Ting and Gordon, D Benjamin and Gifford, David K and Stormo, Gary D and Fraenkel, Ernest},
	year = {2006},
	note = {Publisher: BioMed Central},
	pages = {1--14},
}

@inproceedings{chen_xgboost_2016,
	address = {New York},
	title = {{XGBoost}: a scalable tree boosting system},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2024-03-15},
	booktitle = {Proceedings [...]},
	publisher = {ACM/SIGKDD},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {organization= INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, 22, 2016, New York},
	keywords = {large-scale machine learning},
	pages = {785--794},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\87YL5UCL\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@article{dehghan_tripletmultidti_2023,
	title = {{TripletMultiDTI}: multimodal representation learning in drug-target interaction prediction with triplet loss function},
	url = {https://doi.org/10.1016%2Fj.eswa.2023.120754},
	doi = {10.1016/j.eswa.2023.120754},
	journal = {Expert Systems with Applications},
	author = {Dehghan, Alireza and Razzaghi, Parvin and Abbasi, Karim and Gharaghani, Sajjad},
	month = jun,
	year = {2023},
	note = {2 citations (Crossref) [2023-08-29]
Publisher: Elsevier BV},
	keywords = {deep learning},
	pages = {120754},
}

@inproceedings{davis_relationship_2006,
	address = {New York},
	title = {The relationship between precision-recall and {ROC} curves},
	isbn = {978-1-59593-383-6},
	url = {https://dl.acm.org/doi/10.1145/1143844.1143874},
	doi = {10.1145/1143844.1143874},
	abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
	urldate = {2023-10-24},
	booktitle = {Proceedings [...]},
	publisher = {ACM},
	author = {Davis, Jesse and Goadrich, Mark},
	month = jun,
	year = {2006},
	note = {organization= INTERNATIONAL CONFERENCE ON MACHINE LEARNING, 23, 2006, New York},
	pages = {233--240},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\EL8EBTQQ\\Davis and Goadrich - 2006 - The relationship between Precision-Recall and ROC .pdf:application/pdf},
}

@article{wang_survey_nodate,
	title = {Survey of similarity-based prediction of drug-protein interactions},
	volume = {27},
	url = {https://www.eurekaselect.com/article/100229},
	abstract = {Therapeutic activity of a significant majority of drugs is determined by their interactions
with proteins. Databases of drug-protein interactions (DPIs) primarily focus on the therapeutic protein
targets while the knowledge of the off-targets is fragmented and partial. One way to bridge this
knowledge gap is to employ computational methods to predict protein targets for a given drug molecule,
or interacting drugs for given protein targets. We survey a comprehensive set of 35 methods
that were published in high-impact venues and that predict DPIs based on similarity between drugs
and similarity between protein targets. We analyze the internal databases of known PDIs that these
methods utilize to compute similarities, and investigate how they are linked to the 12 publicly available
source databases. We discuss contents, impact and relationships between these internal and
source databases, and well as the timeline of their releases and publications. The 35 predictors exploit
and often combine three types of similarities that consider drug structures, drug profiles, and
target sequences. We review the predictive architectures of these methods, their impact, and we
explain how their internal DPIs databases are linked to the source databases. We also include a detailed
timeline of the development of these predictors and discuss the underlying limitations of the
current resources and predictive tools. Finally, we provide several recommendations concerning the
future development of the related databases and methods.},
	language = {en},
	number = {35},
	urldate = {2024-03-15},
	journal = {Current Medicinal Chemistry},
	author = {Wang, Chen and Kurgan, Lukasz},
	pages = {5856--5886},
	file = {Wang and Kurgan - Survey of Similarity-Based Prediction of Drug-Prot.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\KBSM6BLM\\Wang and Kurgan - Survey of Similarity-Based Prediction of Drug-Prot.pdf:application/pdf},
}

@article{amit_shape_1997,
	title = {Shape quantization and recognition with randomized trees},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/7/1545-1588/6116},
	doi = {10.1162/neco.1997.9.7.1545},
	abstract = {We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures.
            No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions.
            The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred [Formula: see text] symbols. State-of-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on [Formula: see text] symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context.
            [Figure: see text]},
	language = {en},
	number = {7},
	urldate = {2023-09-23},
	journal = {Neural Computation},
	author = {Amit, Yali and Geman, Donald},
	month = oct,
	year = {1997},
	note = {777 citations (Crossref) [2023-09-23]},
	pages = {1545--1588},
}

@incollection{zhou_semi-supervised_2021,
	address = {Singapore},
	title = {Semi-supervised learning},
	isbn = {9789811519666 9789811519673},
	url = {https://link.springer.com/10.1007/978-981-15-1967-3_13},
	language = {en},
	urldate = {2024-03-15},
	booktitle = {Machine {Learning}},
	publisher = {Springer Singapore},
	author = {Zhou, Zhi-Hua},
	collaborator = {Zhou, Zhi-Hua},
	year = {2021},
	doi = {10.1007/978-981-15-1967-3_13},
	pages = {315--341},
}

@inproceedings{alves_semi-supervised_2023,
	address = {Tallinn},
	title = {Semi-supervised hybrid predictive bi-clustering trees for drug-target interaction prediction},
	booktitle = {Proceedings [...]},
	publisher = {ACM/SIGAPP},
	author = {Alves, Andre and Ilidio, Pedro and Cerri, Ricardo},
	year = {2023},
	note = {organization= SYMPOSIUM ON APPLIED COMPUTING, 38, 2023, Tallinn},
	pages = {1163--1170},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: machine learning in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	number = {85},
	urldate = {2024-03-16},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	year = {2011},
	pages = {2825--2830},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\PEJBGGX5\\Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@inproceedings{flach_precision-recall-gain_2015,
	address = {Montreal},
	title = {Precision-recall-gain curves: {PR} analysis done right},
	shorttitle = {Precision-{Recall}-{Gain} {Curves}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/33e8075e9970de0cfea955afd4644bb2-Abstract.html},
	urldate = {2023-11-08},
	booktitle = {Proceedings [...]},
	publisher = {Curran Associates, Inc.},
	author = {Flach, Peter and Kull, Meelis},
	year = {2015},
	note = {organization= ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS, 28, 2015, Montreal},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\HZ8P6HT6\\Flach and Kull - 2015 - Precision-Recall-Gain Curves PR Analysis Done Rig.pdf:application/pdf},
}

@incollection{kornbrot_point_2014,
	address = {Hatfield},
	title = {Point biserial correlation},
	copyright = {Copyright © 2005 John Wiley \& Sons, Ltd. All rights reserved.},
	isbn = {978-1-118-44511-2},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06227},
	abstract = {The point biserial correlation is the value of Pearson's product moment correlation when one of the variables is dichotomous and the other variable is metric. Values range from +1, a perfect positive relation; through zero, no association at all; to −1, a perfect negative correlation. The square of this correlation, r p b 2 , is a measure of effect size in terms of the proportion of variability accounted for by the relation between the two variables.},
	language = {en},
	urldate = {2024-03-03},
	booktitle = {Wiley {StatsRef}: statistics reference online},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Kornbrot, Diana},
	year = {2014},
	doi = {10.1002/9781118445112.stat06227},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat06227},
	keywords = {association, bivariate, Pearson, t Test},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\VRCP3CY4\\Kornbrot - 2014 - Point Biserial Correlation.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\XN5D9USL\\9781118445112.html:text/html},
}

@article{ban_nrlmfupbeta_2019,
	title = {{NRLMF}{\textbackslash}upbeta: beta-distribution-rescored neighborhood regularized logistic matrix factorization for improving the performance of drug–target interaction prediction},
	volume = {18},
	url = {https://doi.org/10.1016%2Fj.bbrep.2019.01.008},
	doi = {10.1016/j.bbrep.2019.01.008},
	journal = {Biochemistry and Biophysics Reports},
	author = {Ban, Tomohiro and Ohue, Masahito and Akiyama, Yutaka},
	month = jul,
	year = {2019},
	note = {9 citations (Crossref) [2023-08-29]
Publisher: Elsevier BV},
	pages = {100615},
}

@article{liu_neighborhood_2016,
	title = {Neighborhood regularized logistic matrix factorization for drug-target interaction prediction},
	volume = {12},
	url = {https://doi.org/10.1371%2Fjournal.pcbi.1004760},
	doi = {10.1371/journal.pcbi.1004760},
	number = {2},
	journal = {PLoS Computational Biology},
	author = {Liu, Yong and Wu, Min and Miao, Chunyan and Zhao, Peilin and Li, Xiao-Li},
	editor = {Przytycka, Teresa M.},
	month = feb,
	year = {2016},
	note = {247 citations (Crossref) [2023-08-29]
Publisher: Public Library of Science (PLoS)},
	keywords = {Machine learning, Drug discovery, Drug interactions, Drug therapy, Forecasting, G protein coupled receptors, Ion channels, Optimization},
	pages = {e1004760},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\7XT89MEH\\Liu et al. - 2016 - Neighborhood Regularized Logistic Matrix Factoriza.pdf:application/pdf},
}

@misc{agarwal_mdi_2023,
	title = {{MDI}+: a flexible random forest-based feature importance framework},
	shorttitle = {{MDI}+},
	url = {http://arxiv.org/abs/2307.01932},
	abstract = {Mean decrease in impurity (MDI) is a popular feature importance measure for random forests (RFs). We show that the MDI for a feature \$X\_k\$ in each tree in an RF is equivalent to the unnormalized \$R{\textasciicircum}2\$ value in a linear regression of the response on the collection of decision stumps that split on \$X\_k\$. We use this interpretation to propose a flexible feature importance framework called MDI+. Specifically, MDI+ generalizes MDI by allowing the analyst to replace the linear regression model and \$R{\textasciicircum}2\$ metric with regularized generalized linear models (GLMs) and metrics better suited for the given data structure. Moreover, MDI+ incorporates additional features to mitigate known biases of decision trees against additive or smooth models. We further provide guidance on how practitioners can choose an appropriate GLM and metric based upon the Predictability, Computability, Stability framework for veridical data science. Extensive data-inspired simulations show that MDI+ significantly outperforms popular feature importance measures in identifying signal features. We also apply MDI+ to two real-world case studies on drug response prediction and breast cancer subtype classification. We show that MDI+ extracts well-established predictive genes with significantly greater stability compared to existing feature importance measures. All code and models are released in a full-fledged python package on Github.},
	urldate = {2024-01-13},
	author = {Agarwal, Abhineet and Kenney, Ana M. and Tan, Yan Shuo and Tang, Tiffany M. and Yu, Bin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01932 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
	file = {arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\37SPKG68\\2307.html:text/html;Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\CDI8ZX8H\\Agarwal et al. - 2023 - MDI+ A Flexible Random Forest-Based Feature Impor.pdf:application/pdf},
}

@inproceedings{liu_isolation_2008,
	address = {Washington},
	title = {Isolation forest},
	url = {https://ieeexplore.ieee.org/abstract/document/4781136},
	doi = {10.1109/ICDM.2008.17},
	abstract = {Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and random forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.},
	urldate = {2024-03-16},
	booktitle = {Proceedings [...]},
	author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
	month = dec,
	year = {2008},
	note = {organization= INTERNATIONAL CONFERENCE ON DATA MINING, 8, 2008, Washington},
	keywords = {anomaly detection, Application software, Astronomy, binary trees, Constraint optimization, Credit cards, Data mining, Detectors, Information technology, isolation forest, Isolation technology, Laboratories, model based, novelty detection, outlier detection, Performance evaluation},
	pages = {413--422},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\u0170502\\Zotero\\storage\\4G6LDV5X\\4781136.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\NRTVK7LM\\Liu et al. - 2008 - Isolation Forest.pdf:application/pdf},
}

@misc{lundberg_explainable_2019,
	title = {Explainable {AI} for trees: from local explanations to global understanding},
	shorttitle = {Explainable {AI} for {Trees}},
	url = {http://arxiv.org/abs/1905.04610},
	doi = {10.48550/arXiv.1905.04610},
	abstract = {Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
	urldate = {2024-03-15},
	author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	month = may,
	year = {2019},
	note = {arXiv:1905.04610 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\MCSLU27G\\Lundberg et al. - 2019 - Explainable AI for Trees From Local Explanations .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\N7ET69ZS\\1905.html:text/html},
}

@inproceedings{liu_computational_2017,
	address = {Philadelphia},
	title = {Computational drug discovery with dyadic positive-unlabeled learning},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611974973.6},
	abstract = {Computational Drug Discovery, which uses computational techniques to facilitate and improve the drug discovery process, has aroused considerable interests in recent years. Drug Repositioning (DR) and Drug-Drug Interaction (DDI) prediction are two key problems in drug discovery and many computational techniques have been proposed for them in the last decade. Although these two problems have mostly been researched separately in the past, both DR and DDI can be formulated as the problem of detecting positive interactions between data entities (DR is between drug and disease, and DDI is between pairwise drugs). The challenge in both problems is that we can only observe a very small portion of positive interactions. In this paper, we propose a novel framework called Dyadic Positive-Unlabeled learning (DyPU) to solve the problem of detecting positive interactions. DyPU forces positive data pairs to rank higher than the average score of unlabeled data pairs. Moreover, we also derive the dual formulation of the proposed method with the rectifier scoring function and we show that the associated non-trivial proximal operator admits a closed form solution. Extensive experiments are conducted on real drug data sets and the results show that our method achieves superior performance comparing with the state-of-the-art.},
	urldate = {2023-11-30},
	booktitle = {Proceedings [...]},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Liu, Yashu and Qiu, Shuang and Zhang, Ping and Gong, Pinghua and Wang, Fei and Xue, Guoliang and Ye, Jieping},
	month = jun,
	year = {2017},
	note = {organization= SIAM INTERNATIONAL CONFERENCE ON DATA MINING, 2017, Philadelphia},
	pages = {45--53},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\F5JSNFG6\\Liu et al. - 2017 - Computational Drug Discovery with Dyadic Positive-.pdf:application/pdf},
}

@article{marco_atlantic_2017,
	title = {Atlantic frugivory: a plant-frugivore interaction data set for the {Atlantic} {Forest}},
	journal = {Ecology},
	author = {Marco, A and Tatiane, C and Wesley, R and Fernanda, R and {others}},
	year = {2017},
	note = {Publisher: John Wiley \& Sons, Ltd},
}

@article{loyola-gonzalez_explainable_2020,
	title = {An explainable artificial intelligence model for clustering numerical databases},
	volume = {8},
	url = {https://doi.org/10.1109%2Faccess.2020.2980581},
	doi = {10.1109/access.2020.2980581},
	journal = {IEEE Access},
	author = {Loyola-Gonzalez, Octavio and Gutierrez-Rodriguez, Andres Eduardo and Medina-Perez, Miguel Angel and Monroy, Raul and Martinez-Trinidad, Jose Francisco and Carrasco-Ochoa, Jesus Ariel and Garcia-Borroto, Milton},
	year = {2020},
	note = {24 citations (Crossref) [2023-08-29]
Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {52370--52384},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\AWFMB9GF\\Loyola-Gonzalez et al. - 2020 - An Explainable Artificial Intelligence Model for C.pdf:application/pdf},
}

@article{duchi_adaptive_2011,
	title = {Adaptive subgradient methods for online learning and stochastic optimization},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	number = {61},
	urldate = {2023-11-15},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\H5HPC3VY\\Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf},
}

@article{huang_moltrans_2020,
	title = {{MolTrans}: molecular interaction transformer for drug–target interaction prediction},
	volume = {37},
	url = {https://doi.org/10.1093%2Fbioinformatics%2Fbtaa880},
	doi = {10.1093/bioinformatics/btaa880},
	number = {6},
	journal = {Bioinformatics},
	author = {Huang, Kexin and Xiao, Cao and Glass, Lucas M. and Sun, Jimeng},
	editor = {Lu, Zhiyong},
	month = oct,
	year = {2020},
	note = {107 citations (Crossref) [2023-08-29]
Publisher: Oxford University Press (OUP)},
	pages = {830--836},
}

@incollection{goos_ensemble_2000,
	address = {Heidelberg},
	series = {Lecture notes in computer science},
	title = {Ensemble methods in machine learning},
	volume = {1857},
	isbn = {978-3-540-67704-8 978-3-540-45014-6},
	url = {http://link.springer.com/10.1007/3-540-45014-9_1},
	urldate = {2023-09-23},
	booktitle = {Multiple classifier systems},
	publisher = {Springer},
	author = {Dietterich, Thomas G.},
	editor = {Goos, Gerhard and Hartmanis, Juris and Van Leeuwen, Jan},
	year = {2000},
	doi = {10.1007/3-540-45014-9_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--15},
	file = {Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\5GLMGMJV\\Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:application/pdf},
}

@book{fernandez_learning_2018,
	address = {Cham},
	title = {Learning from imbalanced data sets},
	isbn = {978-3-319-98073-7 978-3-319-98074-4},
	url = {http://link.springer.com/10.1007/978-3-319-98074-4},
	language = {en},
	urldate = {2023-10-18},
	publisher = {Springer International Publishing},
	author = {Fernández, Alberto and García, Salvador and Galar, Mikel and Prati, Ronaldo C. and Krawczyk, Bartosz and Herrera, Francisco},
	year = {2018},
	doi = {10.1007/978-3-319-98074-4},
	file = {Alberto Fernández_ Salvador García_ Mikel Galar_ Ronaldo C. Prati_ Bartosz Krawczyk_ Francisco Herrera - Learning from Imbalanced Data Sets-Springer International Publishing.epub:C\:\\Users\\u0170502\\Zotero\\storage\\WU3BPVAT\\Alberto Fernández_ Salvador García_ Mikel Galar_ Ronaldo C. Prati_ Bartosz Krawczyk_ Francisco Herrera - Learning from Imbalanced Data Sets-Springer International Publishing.epub:application/epub+zip;Alberto Fernández, Salvador García, Mikel Galar, Ronaldo C. Prati, Bartosz Krawczyk, Francisco Herrera - Learning from Imbalanced Data Sets-Springer International Publishing (2018).pdf:C\:\\Users\\u0170502\\Zotero\\storage\\WNT34FXM\\Alberto Fernández, Salvador García, Mikel Galar, Ronaldo C. Prati, Bartosz Krawczyk, Francisco Herrera - Learning from Imbalanced Data Sets-Springer International Publishing (2018).pdf:application/pdf;Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\UUJBV3PB\\Fernández et al. - 2018 - Learning from Imbalanced Data Sets.pdf:application/pdf},
}

@article{davis_comprehensive_2011,
	title = {Comprehensive analysis of kinase inhibitor selectivity},
	volume = {29},
	number = {11},
	journal = {Nature Biotechnology},
	author = {Davis, Mindy I and Hunt, Jeremy P and Herrgard, Sanna and Ciceri, Pietro and Wodicka, Lisa M and Pallares, Gabriel and Hocker, Michael and Treiber, Daniel K and Zarrinkar, Patrick P},
	year = {2011},
	note = {Publisher: Nature Publishing Group US New York},
	pages = {1046--1051},
}

@inproceedings{jin_multitask_2017,
	address = {Vancouver},
	title = {Multitask dyadic prediction and its application in prediction of adverse drug-drug interaction},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10718},
	doi = {10.1609/aaai.v31i1.10718},
	abstract = {Adverse drug-drug interactions (DDIs) remain a leading cause of morbidity and mortality around the world. Identifying potential DDIs during the drug design process is critical in guiding targeted clinical drug safety testing. Although detection of adverse DDIs is conducted during Phase IV clinical trials, there are still a large number of new DDIs founded by accidents after the drugs were put on market. With the arrival of big data era, more and more pharmaceutical research and development data are becoming available, which provides an invaluable resource for digging insights that can potentially be leveraged in early prediction of DDIs. Many computational approaches have been proposed in recent years for DDI prediction. However, most of them focused on binary prediction (with or without DDI), despite the fact that each DDI is associated with a different type. Predicting the actual DDI type will help us better understand the DDI mechanism and identify proper ways to prevent it. In this paper, we formulate the DDI type prediction problem as a multitask dyadic regression problem, where the prediction of each specific DDI type is treated as a task. Compared with conventional matrix completion approaches which can only impute the missing entries in the DDI matrix, our approach can directly regress those dyadic relationships (DDIs) and thus can be extend to new drugs more easily. We developed an effective proximal gradient method to solve the problem. Evaluation on real world datasets is presented to demonstrate the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2024-03-17},
	booktitle = {Proceedings [...]},
	publisher = {AAAI Press},
	author = {Jin, Bo and Yang, Haoyu and Xiao, Cao and Zhang, Ping and Wei, Xiaopeng and Wang, Fei},
	month = feb,
	year = {2017},
	note = {organization= AAAI CONFERENCE ON ARTIFICIAL INTELIGENCE, 31, 2017, Vancouver},
	keywords = {biomedical application, multitask learning},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\VGVZGBD7\\Jin et al. - 2017 - Multitask Dyadic Prediction and Its Application in.pdf:application/pdf},
}

@incollection{haynes_benjaminihochberg_2013,
	address = {New York},
	title = {Benjamini–{Hochberg} method},
	isbn = {978-1-4419-9863-7},
	url = {https://doi.org/10.1007/978-1-4419-9863-7_1215},
	language = {en},
	urldate = {2024-03-17},
	booktitle = {Encyclopedia of systems biology},
	publisher = {Springer},
	author = {Haynes, Winston},
	editor = {Dubitzky, Werner and Wolkenhauer, Olaf and Cho, Kwang-Hyun and Yokota, Hiroki},
	year = {2013},
	doi = {10.1007/978-1-4419-9863-7_1215},
	pages = {78--78},
}

@book{breiman_classification_1984,
	address = {New York},
	title = {Classification and regression trees},
	isbn = {978-1-315-13947-0},
	publisher = {Routledge},
	author = {Breiman, Leo and Friedman, Jerome and Olshen, R. A. and Ston, Charles J.},
	year = {1984},
	file = {Classification and Regression Trees (Wadsw - Leo Breiman.epub:C\:\\Users\\u0170502\\Zotero\\storage\\VYMTC7E8\\Classification and Regression Trees (Wadsw - Leo Breiman.epub:application/epub+zip},
}

@inproceedings{wang_learning_2020,
	address = {New York},
	title = {Learning from weak-label data: a deep forest expedition},
	shorttitle = {Learning from {Weak}-{Label} {Data}},
	url = {https://doi.org/10.1609/aaai.v34i04.6092},
	doi = {10.1609/aaai.v34i04.6092},
	booktitle = {Proceedings [...]},
	author = {Wang, Qian-Wei and Yang, Liang and Li, Yu-Feng},
	month = apr,
	year = {2020},
	note = {organization= AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, 34, 2020, New York},
	pages = {6251--6258},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\SXHBBBEU\\Wang et al. - 2020 - Learning from Weak-Label Data A Deep Forest Exped.pdf:application/pdf},
}

@article{zhou2018brief,
	title = {A brief introduction to weakly supervised learning},
	volume = {5},
	number = {1},
	journal = {National Science Review},
	author = {Zhou, Zhi-Hua},
	year = {2018},
	note = {Publisher: Oxford University Press},
	pages = {44--53},
}

@article{zhang_ml-knn_2007,
	title = {{ML}-{KNN}: a lazy learning approach to multi-label learning},
	volume = {40},
	issn = {0031-3203},
	shorttitle = {{ML}-{KNN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320307000027},
	doi = {10.1016/j.patcog.2006.12.019},
	abstract = {Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms.},
	number = {7},
	urldate = {2024-03-23},
	journal = {Pattern Recognition},
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	month = jul,
	year = {2007},
	note = {2418 citations (Crossref) [2024-03-23]},
	keywords = {Machine learning, -nearest neighbor, Functional genomics, Lazy learning, Multi-label learning, Natural scene classification, Text categorization},
	pages = {2038--2048},
	file = {ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\JTHZVBMP\\S0031320307000027.html:text/html;Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\I7LJHRK6\\Zhang and Zhou - 2007 - ML-KNN A lazy learning approach to multi-label le.pdf:application/pdf},
}

@article{rodriguez_rotation_2006,
	title = {Rotation forest: a new classifier ensemble method},
	volume = {28},
	shorttitle = {Rotation forest},
	url = {https://ieeexplore.ieee.org/abstract/document/1677518/},
	doi = {https://doi.org/10.1109/TPAMI.2006.211},
	number = {10},
	urldate = {2024-03-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rodriguez, Juan José and Kuncheva, Ludmila I. and Alonso, Carlos J.},
	year = {2006},
	note = {Publisher: IEEE},
	pages = {1619--1630},
}

@misc{kingma_adam_2017,
	title = {Adam: a method for stochastic optimization},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-03-16},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\NIAFXVRI\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\JBRXAD32\\1412.html:text/html},
}

@article{demsar_statistical_2006,
	title = {Statistical comparisons of classifiers over multiple data sets},
	volume = {7},
	url = {https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf},
	urldate = {2024-03-17},
	journal = {Journal of Machine Learning Research},
	author = {Demšar, Janez},
	year = {2006},
	note = {Publisher: JMLR. org},
	pages = {1--30},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\KXZERZ7N\\Demšar - 2006 - Statistical comparisons of classifiers over multip.pdf:application/pdf},
}

@article{crammer_algorithmic_2001,
	title = {On the algorithmic implementation of multiclass kernel-based vector machines},
	volume = {2},
	url = {https://www.jmlr.org/papers/volume2/crammer01a/crammer01a.pdf},
	number = {Dec},
	urldate = {2024-03-16},
	journal = {Journal of Machine Learning Research},
	author = {Crammer, Koby and Singer, Yoram},
	year = {2001},
	pages = {265--292},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\RQNTHJ9H\\Crammer and Singer - 2001 - On the algorithmic implementation of multiclass ke.pdf:application/pdf},
}

@article{rogers_using_2005,
	title = {Using extended-connectivity fingerprints with {Laplacian}-modified {Bayesian} analysis in high-throughput screening follow-up},
	volume = {10},
	number = {7},
	journal = {Journal of Biomolecular Screening},
	author = {Rogers, David and Brown, Robert D and Hahn, Mathew},
	year = {2005},
	note = {Publisher: Sage Publications Sage CA: Thousand Oaks, CA},
	pages = {682--686},
}

@article{zhao_noncode_2016,
	title = {{NONCODE} 2016: an informative and valuable data source of long non-coding {RNAs}},
	volume = {44},
	shorttitle = {{NONCODE} 2016},
	url = {https://academic.oup.com/nar/article-abstract/44/D1/D203/2503065},
	number = {D1},
	urldate = {2024-03-15},
	journal = {Nucleic Acids Research},
	author = {Zhao, Yi and Li, Hui and Fang, Shuangsang and Kang, Yue and Wu, Wei and Hao, Yajing and Li, Ziyang and Bu, Dechao and Sun, Ninghui and Zhang, Michael Q.},
	year = {2016},
	note = {Publisher: Oxford University Press},
	pages = {D203--D208},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\WMND95NK\\2503065.html:text/html},
}

@article{wu_npinter_2006,
	title = {{NPInter}: the noncoding {RNAs} and protein related biomacromolecules interaction database},
	volume = {34},
	number = {suppl\_1},
	journal = {Nucleic Acids Research},
	author = {Wu, Tao and Wang, Jie and Liu, Changning and Zhang, Yong and Shi, Baochen and Zhu, Xiaopeng and Zhang, Zhihua and Skogerbø, Geir and Chen, Lan and Lu, Hongchao and {others}},
	year = {2006},
	note = {Publisher: Oxford University Press},
	pages = {D150--D152},
}

@article{teng_npinter_2020,
	title = {{NPInter} v4. 0: an integrated database of {ncRNA} interactions},
	volume = {48},
	number = {D1},
	journal = {Nucleic Acids Research},
	author = {Teng, Xueyi and Chen, Xiaomin and Xue, Hua and Tang, Yiheng and Zhang, Peng and Kang, Quan and Hao, Yajing and Chen, Runsheng and Zhao, Yi and He, Shunmin},
	year = {2020},
	note = {Publisher: Oxford University Press},
	pages = {D160--D165},
}

@article{liu_noncode_2005,
	title = {{NONCODE}: an integrated knowledge database of non-coding {RNAs}},
	volume = {33},
	shorttitle = {{NONCODE}},
	url = {https://academic.oup.com/nar/article-abstract/33/suppl_1/D112/2505275},
	number = {suppl\_1},
	urldate = {2024-03-15},
	journal = {Nucleic Acids Research},
	author = {Liu, Changning and Bai, Baoyan and Skogerbø, Geir and Cai, Lun and Deng, Wei and Zhang, Yong and Bu, Dongbo and Zhao, Yi and Chen, Runsheng},
	year = {2005},
	note = {Publisher: Oxford University Press},
	pages = {D112--D115},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\YCN3ZR6V\\2505275.html:text/html},
}

@article{huang_mirtarbase_2022,
	title = {{miRTarBase} update 2022: an informative resource for experimentally validated {miRNA}–target interactions},
	volume = {50},
	number = {D1},
	journal = {Nucleic Acids Research},
	author = {Huang, Hsi-Yuan and Lin, Yang-Chi-Dung and Cui, Shidong and Huang, Yixian and Tang, Yun and Xu, Jiatong and Bao, Jiayang and Li, Yulin and Wen, Jia and Zuo, Huali and {others}},
	year = {2022},
	note = {Publisher: Oxford University Press},
	pages = {D222--D230},
}

@article{hsu_mirtarbase_2011,
	title = {{miRTarBase}: a database curates experimentally validated {microRNA}–target interactions},
	volume = {39},
	number = {suppl\_1},
	journal = {Nucleic Acids Research},
	author = {Hsu, Sheng-Da and Lin, Feng-Mao and Wu, Wei-Yun and Liang, Chao and Huang, Wei-Chih and Chan, Wen-Ling and Tsai, Wen-Ting and Chen, Goun-Zhou and Lee, Chia-Jung and Chiu, Chih-Min and {others}},
	year = {2011},
	note = {Publisher: Oxford University Press},
	pages = {D163--D169},
}

@article{griffiths-jones_mirbase_2006,
	title = {{miRBase}: {microRNA} sequences, targets and gene nomenclature},
	volume = {34},
	number = {suppl\_1},
	journal = {Nucleic Acids Research},
	author = {GRIFFITHS-JONES,, Sam and Grocock, Russell J and Van Dongen, Stijn and Bateman, Alex and Enright, Anton J},
	year = {2006},
	note = {Publisher: Oxford University Press},
	pages = {D140--D144},
}

@article{frankish_gencode_2021,
	title = {{GENCODE} 2021},
	volume = {49},
	number = {D1},
	journal = {Nucleic Acids Research},
	author = {Frankish, Adam and Diekhans, Mark and Jungreis, Irwin and Lagarde, Julien and Loveland, Jane E and Mudge, Jonathan M and Sisu, Cristina and Wright, James C and Armstrong, Joel and Barnes, If and {others}},
	year = {2021},
	note = {Publisher: Oxford University Press},
	pages = {D916--D923},
}

@article{consortium_uniprot_2019,
	title = {{UniProt}: a worldwide hub of protein knowledge},
	volume = {47},
	number = {D1},
	journal = {Nucleic Acids Research},
	author = {Consortium, UniProt},
	year = {2019},
	note = {Publisher: Oxford University Press},
	pages = {D506--D515},
}

@article{brohee_unraveling_2011,
	title = {Unraveling networks of co-regulated genes on the sole basis of genome sequences},
	volume = {39},
	number = {15},
	journal = {Nucleic Acids Research},
	author = {Brohée, Sylvain and Janky, Rekin’s and Abdel-Sater, Fadi and Vanderstocken, Gilles and Andre, Bruno and Van Helden, Jacques},
	year = {2011},
	note = {Publisher: Oxford University Press},
	pages = {6340--6358},
}

@article{keyvanpour_dtip-tc2a_2022,
	title = {{DTIP}-{TC2A}: an analytical framework for drug-target interactions prediction methods},
	volume = {99},
	issn = {1476-9271},
	shorttitle = {{DTIP}-{TC2A}},
	url = {https://www.sciencedirect.com/science/article/pii/S1476927122000871},
	doi = {10.1016/j.compbiolchem.2022.107707},
	abstract = {Identifying drug-target interactions through computational methods is raised an important and key step in the process of drug discovery and drug-oriented research during the last years. In addition to the advantages of existing computational methods, there are also challenges that affect methods' efficiency and provide obstacles in the direction of developing these computational methods. However, the literature suffers from lacking a comprehensive and comparative analysis concerning drug-target interactions prediction (DTIP) focusing on the analysis of technical and challenging aspects. It seems necessary to provide a comparative perspective and a different analysis on a macro level due to the importance of the DTIP problem. In this paper, we presented the quadruple framework of analytical, named DTIP-TC2A consists of four main components for DTIP. The first component, categorizing DTIP methods based on the technical aspect ahead and investigating the strengths and weaknesses of different DTIP methods. Second, classify DTIP challenges with a major focus on a well-organized and coherent investigation of challenges and presenting a macro view of the DTIP challenges by systematic identification of them. Third, recommending some general criteria to analyze DTIP methods in form of the proposed classifications. Suggesting a suitable set of qualitative criteria along with using quantitative criteria can lead to a more proper choice of DTIP methods. Fourth, performing a two-phase qualitative analysis and comparison between each class of DTIP approaches based on the proposed functional criteria and the identified challenges ahead in order to understand the superiority of each class of DTIP methods over the other class. We believed that the DTIP-TC2A framework can offer a proper context for efficient selection of DTIP methods, improving the efficiency of a DTIP system due to the nature of computational methods, upgrading DTIP methods by removing the barriers, and presenting new directions of research for further studies through systematic identification of DTIP challenges and purposeful evaluation of challenges and methods.},
	urldate = {2023-11-22},
	journal = {Computational Biology and Chemistry},
	author = {Keyvanpour, Mohammad Reza and Haddadi, Faraneh and Mehrmolaei, Soheila},
	month = aug,
	year = {2022},
	note = {2 citations (Crossref) [2023-11-22]},
	keywords = {Drug discovery, Biological network analysis, Challenge, Drug-target interactions, DTIP, Qualitative evaluation},
	pages = {107707},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\MWG5X3UU\\Keyvanpour et al. - 2022 - DTIP-TC2A An analytical framework for drug-target.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\KIQUBBF4\\S1476927122000871.html:text/html},
}

@article{hu_genetic_2007,
	title = {Genetic reconstruction of a functional transcriptional regulatory network},
	volume = {39},
	number = {5},
	journal = {Nature Genetics},
	author = {Hu, Zhanzhi and Killion, Patrick J and Iyer, Vishwanath R},
	year = {2007},
	note = {Publisher: Nature Publishing Group US New York},
	pages = {683--687},
}

@inproceedings{hu_collaborative_2008,
	address = {Pisa},
	title = {Collaborative filtering for implicit feedback datasets},
	url = {https://ieeexplore.ieee.org/abstract/document/4781121},
	doi = {10.1109/ICDM.2008.22},
	abstract = {A common task of recommender systems is to improve customer experience through personalized recommendations based on prior implicit feedback. These systems passively track different sorts of user behavior, such as purchase history, watching habits and browsing activity, in order to model user preferences. Unlike the much more extensively researched explicit feedback, we do not have any direct input from the users regarding their preferences. In particular, we lack substantial evidence on which products consumer dislike. In this work we identify unique properties of implicit feedback datasets. We propose treating the data as indication of positive and negative preference associated with vastly varying confidence levels. This leads to a factor model which is especially tailored for implicit feedback recommenders. We also suggest a scalable optimization procedure, which scales linearly with the data size. The algorithm is used successfully within a recommender system for television shows. It compares favorably with well tuned implementations of other known methods. In addition, we offer a novel way to give explanations to recommendations given by this factor model.},
	urldate = {2023-10-31},
	booktitle = {Proceedings [...]},
	publisher = {IEEE},
	author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
	month = dec,
	year = {2008},
	note = {organization= INTERNATIONAL CONFERENCE ON DATA MINING, 8, 2008, Pisa},
	pages = {263--272},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\u0170502\\Zotero\\storage\\B2AXKDVP\\4781121.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\6WLQKXZW\\Hu et al. - 2008 - Collaborative Filtering for Implicit Feedback Data.pdf:application/pdf},
}

@article{he_learning_2009,
	title = {Learning from imbalanced data},
	volume = {21},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/abstract/document/5128907},
	doi = {10.1109/TKDE.2008.239},
	abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
	number = {9},
	urldate = {2023-10-17},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {He, Haibo and Garcia, Edwardo A.},
	month = sep,
	year = {2009},
	note = {4981 citations (Crossref) [2023-10-17]
Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	pages = {1263--1284},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\u0170502\\Zotero\\storage\\DK2B5HKX\\5128907.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\QUTFAJP7\\He and Garcia - 2009 - Learning from Imbalanced Data.pdf:application/pdf},
}

@article{rafailidis_modeling_2016,
	title = {Modeling users preference dynamics and side information in recommender systems},
	volume = {46},
	issn = {2168-2232},
	url = {https://ieeexplore.ieee.org/abstract/document/7194815},
	doi = {10.1109/TSMC.2015.2460691},
	abstract = {In recommender systems user preferences can be fairly dynamic, as users tend to exploit a wide range of items and modify their tastes accordingly over time. In this paper, we model user-item interactions over time using a tensor that has time as a dimension (mode). To account for the fact that user preferences change individually, we propose a new measure of user-preference dynamics (UPD) that captures the rate with which the current preferences of each user have been shifted. UPD shows the variability in how users interact with items in recommender systems. We generate recommendations based on a tensor factorization technique, where the importance of past user preferences are weighted according to their UPD values, that is, higher UPD values downweigh more past user preferences. Additionally, we exploit users' side data, such as demographics, which improve the accuracy of recommendations based on a coupled tensor-matrix factorization scheme. Our empirical evaluation uses two real benchmark datasets from the social media platforms Last.fm and MovieLens, containing users' history records pertaining to listening to songs and viewing movies, respectively. We demonstrate that in both datasets, there are users with a varying level of dynamics, expressed by the UPD metric. Our experimental results show that the proposed method outperforms several baselines, by taking into account both dynamics and side data of users.},
	number = {6},
	urldate = {2024-03-15},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Rafailidis, Dimitrios and Nanopoulos, Alexandros},
	month = jun,
	year = {2016},
	note = {82 citations (Crossref) [2024-03-15]
Conference Name: IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	keywords = {Recommender systems, Optimization, Approximation methods, Collaboration, Coupled tensor factorization (CTF), Motion pictures, Music, recommender systems, Tensile stress, users' dynamics, users’ dynamics},
	pages = {782--792},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\u0170502\\Zotero\\storage\\DNILAR59\\7194815.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\SIJGHELP\\Rafailidis and Nanopoulos - 2016 - Modeling Users Preference Dynamics and Side Inform.pdf:application/pdf},
}

@article{saito_precision-recall_2015,
	title = {The precision-recall plot is more informative than the {ROC} plot when evaluating binary classifiers on imbalanced datasets},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432},
	doi = {10.1371/journal.pone.0118432},
	abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
	language = {en},
	number = {3},
	urldate = {2023-10-18},
	journal = {PLOS ONE},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	month = mar,
	year = {2015},
	note = {2037 citations (Crossref) [2023-10-18]
Publisher: Public Library of Science},
	keywords = {Support vector machines, Bioinformatics, Caenorhabditis elegans, Exponential functions, Genome-wide association studies, Interpolation, Measurement, MicroRNAs},
	pages = {e0118432},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\WH5S8GHK\\Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than.pdf:application/pdf},
}

@article{hao_predicting_2017,
	title = {Predicting drug-target interactions by dual-network integrated logistic matrix factorization},
	volume = {7},
	number = {1},
	journal = {Scientific Reports},
	author = {Hao, Ming and Bryant, Stephen H and Wang, Yanli},
	year = {2017},
	note = {Publisher: Nature Publishing Group UK London},
	pages = {40376},
}

@article{benavoli_should_2016,
	title = {Should we really use post-hoc tests based on mean-ranks?},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/benavoli16a.html},
	abstract = {The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms 
A
𝐴
 and 
B
𝐵
 depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between 
A
𝐴
 and 
B
𝐵
 could be declared significant if the pool comprises algorithms 
C,D,E
𝐶
,
𝐷
,
𝐸
 and not significant if the pool comprises algorithms 
F,G,H
𝐹
,
𝐺
,
𝐻
. To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test.},
	number = {5},
	urldate = {2024-01-12},
	journal = {Journal of Machine Learning Research},
	author = {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca},
	year = {2016},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\ESJNESRI\\Benavoli et al. - 2016 - Should We Really Use Post-Hoc Tests Based on Mean-.pdf:application/pdf},
}

@book{hastie_elements_2001,
	address = {New York},
	series = {Springer series in statistics},
	title = {The elements of statistical learning},
	isbn = {978-1-4899-0519-2 978-0-387-21606-5},
	url = {http://link.springer.com/10.1007/978-0-387-21606-5},
	urldate = {2024-03-15},
	publisher = {Springer},
	author = {Hastie, Trevor and Friedman, Jerome and Tibshirani, Robert},
	year = {2001},
	doi = {10.1007/978-0-387-21606-5},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\7NEMCYVG\\Hastie et al. - 2001 - The Elements of Statistical Learning.pdf:application/pdf},
}

@book{zhu_introduction_2022,
	address = {Switzerland},
	title = {Introduction to semi-supervised learning},
	publisher = {Springer Nature},
	author = {Zhu, Xiaojin and Goldberg, Andrew B.},
	year = {2022},
}

@article{thafar_dti2vec_2021,
	title = {{DTi2Vec}: drug–target interaction prediction using network embedding and ensemble learning},
	volume = {13},
	issn = {1758-2946},
	shorttitle = {{DTi2Vec}},
	url = {https://doi.org/10.1186/s13321-021-00552-w},
	doi = {10.1186/s13321-021-00552-w},
	abstract = {Drug–target interaction (DTI) prediction is a crucial step in drug discovery and repositioning as it reduces experimental validation costs if done right. Thus, developing in-silico methods to predict potential DTI has become a competitive research niche, with one of its main focuses being improving the prediction accuracy. Using machine learning (ML) models for this task, specifically network-based approaches, is effective and has shown great advantages over the other computational methods. However, ML model development involves upstream hand-crafted feature extraction and other processes that impact prediction accuracy. Thus, network-based representation learning techniques that provide automated feature extraction combined with traditional ML classifiers dealing with downstream link prediction tasks may be better-suited paradigms. Here, we present such a method, DTi2Vec, which identifies DTIs using network representation learning and ensemble learning techniques. DTi2Vec constructs the heterogeneous network, and then it automatically generates features for each drug and target using the nodes embedding technique. DTi2Vec demonstrated its ability in drug–target link prediction compared to several state-of-the-art network-based methods, using four benchmark datasets and large-scale data compiled from DrugBank. DTi2Vec showed a statistically significant increase in the prediction performances in terms of AUPR. We verified the "novel" predicted DTIs using several databases and scientific literature. DTi2Vec is a simple yet effective method that provides high DTI prediction performance while being scalable and efficient in computation, translating into a powerful drug repositioning tool.},
	number = {1},
	urldate = {2023-11-22},
	journal = {Journal of Cheminformatics},
	author = {Thafar, Maha A. and Olayan, Rawan S. and Albaradei, Somayah and Bajic, Vladimir B. and Gojobori, Takashi and Essack, Magbubah and Gao, Xin},
	month = sep,
	year = {2021},
	note = {20 citations (Crossref) [2023-11-22]},
	keywords = {Link prediction, Ensemble learning, Drug–target interaction, Cheminformatics, Drug repositioning, Heterogeneous network, Network embedding, Random walk, Representation learning},
	pages = {71},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\KXVF9ZEE\\Thafar et al. - 2021 - DTi2Vec Drug–target interaction prediction using .pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\4BBEVSTB\\s13321-021-00552-w.html:text/html},
}

@article{he_simboost_2017,
	title = {{SimBoost}: a read-across approach for predicting drug–target binding affinities using gradient boosting machines},
	volume = {9},
	number = {1},
	journal = {Journal of Cheminformatics},
	author = {He, Tong and Heidemeyer, Marten and Ban, Fuqiang and Cherkasov, Artem and Ester, Martin},
	year = {2017},
	note = {Publisher: BioMed Central},
	keywords = {Drug–target interaction, Applicability Domain, Gradient boosting, Prediction interval, QSAR, Read-across},
	pages = {1--14},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\D7BPK2TH\\He et al. - 2017 - SimBoost a read-across approach for predicting dr.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\QMSDCNGJ\\s13321-017-0209-z.html:text/html},
}

@article{sagi_ensemble_2018,
	title = {Ensemble learning: a survey},
	volume = {8},
	issn = {1942-4787, 1942-4795},
	shorttitle = {Ensemble learning},
	url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1249},
	doi = {10.1002/widm.1249},
	abstract = {Ensemble methods are considered the state‐of‐the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state‐of‐the‐art ensemble methods and discusses current challenges and trends in the field.
            
              This article is categorized under:
              
                
                  Algorithmic Development {\textgreater} Ensemble Methods
                
                
                  Technologies {\textgreater} Machine Learning
                
                
                  Technologies {\textgreater} Classification},
	language = {en},
	number = {4},
	urldate = {2023-09-23},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Sagi, Omer and Rokach, Lior},
	month = jul,
	year = {2018},
	note = {886 citations (Crossref) [2023-09-23]},
	pages = {e1249},
}

@article{pliakos_drug-target_2020,
	title = {Drug-target interaction prediction with tree-ensemble learning and output space reconstruction},
	volume = {21},
	journal = {BMC Bioinformatics},
	author = {Pliakos, Konstantinos and Vens, Celine},
	year = {2020},
	note = {Publisher: Springer},
	pages = {1--11},
}

@article{pliakos_network_2019,
	title = {Network inference with ensembles of bi-clustering trees},
	volume = {20},
	doi = {10.1186/s12859-019-3104-y},
	journal = {BMC Bioinformatics},
	author = {Pliakos, Konstantinos and Vens, Celine},
	year = {2019},
	note = {Publisher: Springer},
	pages = {1--12},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\J2JXKQCE\\Pliakos and Vens - 2019 - Network inference with ensembles of bi-clustering .pdf:application/pdf},
}

@inproceedings{menon_log-linear_2010,
	address = {Washington},
	title = {A log-linear model with latent leatures for dyadic prediction},
	url = {https://ieeexplore.ieee.org/abstract/document/5693990?casa_token=5XEvIOHU1OMAAAAA:Bymab0jmtjfbBmayXVALNP8CnIeLGi9Qh3CG1SPewKStCzpACWf91Q1zKh8I_UUrUPYYncLOEOs},
	doi = {10.1109/ICDM.2010.148},
	abstract = {In dyadic prediction, labels must be predicted for pairs (dyads) whose members possess unique identifiers and, sometimes, additional features called side-information. Special cases of this problem include collaborative filtering and link prediction. We present a new log-linear model for dyadic prediction that is the first to satisfy several important desiderata: (i) labels may be ordinal or nominal, (ii) side-information can be easily exploited if present, (iii) with or without side-information, latent features are inferred for dyad members, (iv) the model is resistant to sample-selection bias, (v) it can learn well-calibrated probabilities, and (vi) it can scale to large datasets. To our knowledge, no existing method satisfies all the above criteria. In particular, many methods assume that the labels are binary or numerical, and cannot use side-information. Experimental results show that the new method is competitive with previous specialized methods for collaborative filtering and link prediction. Other experimental results demonstrate that the new method succeeds for dyadic prediction tasks where previous methods cannot be used. In particular, the new method predicts nominal labels accurately, and by using side-information it solves the cold-start problem in collaborative filtering.},
	urldate = {2024-03-17},
	booktitle = {Proceedings [...]},
	publisher = {IEEE Press},
	author = {Menon, Aditya Krishna and Elkan, Charles},
	month = dec,
	year = {2010},
	note = {organization= INTERNATIONAL CONFERENCE ON DATA MINING, 10, 2010, Washington},
	keywords = {Dyadic prediction, collaborative filtering, Approximation methods, Collaboration, Data models, link prediction, log-linear model, Mathematical model, Numerical models, Predictive models, Training},
	pages = {364--373},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\u0170502\\Zotero\\storage\\6ASZ9E2K\\5693990.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\9NNBMJX8\\Menon and Elkan - 2010 - A Log-Linear Model with Latent Features for Dyadic.pdf:application/pdf},
}

@article{lu_link_2011,
	title = {Link prediction in complex networks: a survey},
	volume = {390},
	issn = {0378-4371},
	shorttitle = {Link prediction in complex networks},
	url = {https://www.sciencedirect.com/science/article/pii/S037843711000991X},
	doi = {10.1016/j.physa.2010.11.027},
	abstract = {Link prediction in complex networks has attracted increasing attention from both physical and computer science communities. The algorithms can be used to extract missing information, identify spurious interactions, evaluate network evolving mechanisms, and so on. This article summaries recent progress about link prediction algorithms, emphasizing on the contributions from physical perspectives and approaches, such as the random-walk-based methods and the maximum likelihood methods. We also introduce three typical applications: reconstruction of networks, evaluation of network evolving mechanism and classification of partially labeled networks. Finally, we introduce some applications and outline future challenges of link prediction algorithms.},
	number = {6},
	urldate = {2024-03-17},
	journal = {Physica A: statistical mechanics and its applications},
	author = {Lü, Linyuan and Zhou, Tao},
	month = mar,
	year = {2011},
	note = {1878 citations (Crossref) [2024-03-17]},
	keywords = {Link prediction, Complex networks, Maximum likelihood methods, Node similarity, Probabilistic models},
	pages = {1150--1170},
	file = {ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\BFCRVN2E\\S037843711000991X.html:text/html;Submitted Version:C\:\\Users\\u0170502\\Zotero\\storage\\PWSZLNB8\\Lü and Zhou - 2011 - Link prediction in complex networks A survey.pdf:application/pdf},
}

@article{faith_large-scale_2007,
	title = {Large-scale mapping and validation of {Escherichia} coli transcriptional regulation from a compendium of expression profiles},
	volume = {5},
	number = {1},
	journal = {PLoS Biology},
	author = {Faith, Jeremiah J and Hayete, Boris and Thaden, Joshua T and Mogno, Ilaria and Wierzbowski, Jamey and Cottarel, Guillaume and Kasif, Simon and Collins, James J and Gardner, Timothy S},
	year = {2007},
	note = {Publisher: Public Library of Science San Francisco, USA},
	pages = {e8},
}

@article{chen_machine_2018,
	title = {Machine learning for drug-target interaction prediction},
	volume = {23},
	url = {https://doi.org/10.3390%2Fmolecules23092208},
	doi = {10.3390/molecules23092208},
	number = {9},
	journal = {Molecules},
	author = {Chen, Ruolan and Liu, Xiangrong and Jin, Shuting and Lin, Jiawei and Liu, Juan},
	month = aug,
	year = {2018},
	note = {137 citations (Crossref) [2023-08-29]
Publisher: MDPI AG},
	pages = {2208},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\G4SCZ433\\Chen et al. - 2018 - Machine Learning for Drug-Target Interaction Predi.pdf:application/pdf},
}

@article{benjamini_controlling_1995,
	title = {Controlling the false discovery rate: a practical and powerful approach to multiple testing},
	volume = {57},
	copyright = {© 1995 Royal Statistical Society},
	issn = {2517-6161},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x},
	doi = {10.1111/j.2517-6161.1995.tb02031.x},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses — the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferronitype procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	language = {en},
	number = {1},
	urldate = {2024-03-17},
	journal = {Journal of the Royal Statistical Society Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	note = {23462 citations (Crossref) [2024-03-17]
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1995.tb02031.x},
	keywords = {bonferroni-type procedures, familywise error rate, multiple-comparison procedures, p-values},
	pages = {289--300},
	file = {Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\H64LU37E\\j.2517-6161.1995.tb02031.html:text/html},
}

@article{geurts_extremely_2006,
	title = {Extremely randomized trees},
	volume = {63},
	journal = {Machine Learning},
	author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	year = {2006},
	note = {Publisher: Springer},
	pages = {3--42},
	file = {Geurts et al. - 2006 - Extremely randomized trees.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\3SPLNMXA\\Geurts et al. - 2006 - Extremely randomized trees.pdf:application/pdf},
}

@inproceedings{tan_improved_2016,
	address = {Berlin},
	title = {Improved representation learning for question answer matching},
	booktitle = {Proceedings [...]},
	publisher = {ACL},
	author = {Tan, Ming and Dos Santos, Cicero and Xiang, Bing and Zhou, Bowen},
	year = {2016},
	note = {organization= ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, 54, 2016, Berlin},
	pages = {464--473},
}

@inproceedings{amasyali_comparison_2011,
	address = {Istanbul},
	title = {Comparison of single and ensemble classifiers in terms of accuracy and execution time},
	isbn = {978-1-61284-919-5},
	url = {http://ieeexplore.ieee.org/document/5946119/},
	doi = {10.1109/INISTA.2011.5946119},
	urldate = {2023-09-23},
	booktitle = {Proceedings [...]},
	publisher = {IEEE},
	author = {Amasyali, M. F. and Ersoy, O. K.},
	month = jun,
	year = {2011},
	note = {organization= INTERNATIONAL SYMPOSIUM ON INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA), 2011, Istanbul},
	pages = {470--474},
}

@misc{vert_reconstruction_2008,
	title = {Reconstruction of biological networks by supervised machine learning approaches},
	url = {http://arxiv.org/abs/0806.0215},
	abstract = {We review a recent trend in computational systems biology which aims at using pattern recognition algorithms to infer the structure of large-scale biological networks from heterogeneous genomic data. We present several strategies that have been proposed and that lead to different pattern recognition problems and algorithms. The strenght of these approaches is illustrated on the reconstruction of metabolic, protein-protein and regulatory networks of model organisms. In all cases, state-of-the-art performance is reported.},
	urldate = {2023-08-29},
	author = {Vert, Jean Philippe},
	month = sep,
	year = {2008},
	note = {arXiv:0806.0215 [q-bio]},
	keywords = {Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\UEZUEVYL\\Vert - 2008 - Reconstruction of biological networks by supervise.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\7AHFWMTT\\0806.html:text/html},
}

@article{johnson_logistic_2014,
	title = {Logistic matrix factorization for implicit feedback data},
	volume = {27},
	url = {http://web.stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf},
	number = {78},
	urldate = {2024-03-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Johnson, Christopher C.},
	year = {2014},
	note = {Publisher: Montréal, Canada},
	pages = {1--9},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\DM3R94ZF\\Johnson - 2014 - Logistic matrix factorization for implicit feedbac.pdf:application/pdf},
}

@misc{powers_evaluation_2020,
	title = {Evaluation: from precision, recall and {F}-measure to {ROC}, informedness, markedness and correlation},
	shorttitle = {Evaluation},
	url = {http://arxiv.org/abs/2010.16061},
	abstract = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case.},
	urldate = {2024-03-15},
	author = {Powers, David M. W.},
	month = oct,
	year = {2020},
	note = {arXiv:2010.16061 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0170502\\Zotero\\storage\\998CX29F\\Powers - 2020 - Evaluation from precision, recall and F-measure t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\5TT3C623\\2010.html:text/html},
}

@book{schafer_introduction_1966,
	address = {New York},
	title = {An introduction to nonassociative algebras},
	urldate = {2024-03-15},
	publisher = {Academic Press},
	author = {Schafer, Richard D.},
	year = {1966},
}

@book{chapelle_semi-supervised_2006,
	address = {Cambridge},
	series = {Adaptive computation and machine learning},
	title = {Semi-supervised learning},
	isbn = {978-0-262-03358-9},
	language = {en},
	publisher = {MIT Press},
	editor = {Chapelle, Olivier and Schölkopf, Bernhard and Zien, Alexander},
	year = {2006},
	note = {OCLC: ocm64898359},
	keywords = {Supervised learning (Machine learning)},
	file = {Chapelle et al. - 2006 - Semi-supervised learning.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\C8KDYWQS\\Chapelle et al. - 2006 - Semi-supervised learning.pdf:application/pdf},
}

@book{murphy_machine_2012,
	address = {Cambridge},
	series = {Adaptive computation and machine learning series},
	title = {Machine learning: a probabilistic perspective},
	isbn = {978-0-262-01802-9},
	shorttitle = {Machine learning},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	year = {2012},
	keywords = {Machine learning, Probabilities},
	file = {Murphy - 2012 - Machine learning a probabilistic perspective.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\VW9YJI3Q\\Murphy - 2012 - Machine learning a probabilistic perspective.pdf:application/pdf},
}

@article{liu_drug-target_2022,
	title = {Drug-target interaction prediction via an ensemble of weighted nearest neighbors with interaction recovery},
	volume = {52},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-021-02495-z},
	doi = {10.1007/s10489-021-02495-z},
	abstract = {Predicting drug-target interactions (DTI) via reliable computational methods is an effective and efficient way to mitigate the enormous costs and time of the drug discovery process. Structure-based drug similarities and sequence-based target protein similarities are the commonly used information for DTI prediction. Among numerous computational methods, neighborhood-based chemogenomic approaches that leverage drug and target similarities to perform predictions directly are simple but promising ones. However, existing similarity-based methods need to be re-trained to predict interactions for any new drugs or targets and cannot directly perform predictions for both new drugs, new targets, and new drug-target pairs. Furthermore, a large amount of missing (undetected) interactions in current DTI datasets hinders most DTI prediction methods. To address these issues, we propose a new method denoted as Weighted k-Nearest Neighbor with Interaction Recovery (WkNNIR). Not only can WkNNIR estimate interactions of any new drugs and/or new targets without any need of re-training, but it can also recover missing interactions (false negatives). In addition, WkNNIR exploits local imbalance to promote the influence of more reliable similarities on the interaction recovery and prediction processes. We also propose a series of ensemble methods that employ diverse sampling strategies and could be coupled with WkNNIR as well as any other DTI prediction method to improve performance. Experimental results over five benchmark datasets demonstrate the effectiveness of our approaches in predicting drug-target interactions. Lastly, we confirm the practical prediction ability of proposed methods to discover reliable interactions that were not reported in the original benchmark datasets.},
	language = {en},
	number = {4},
	urldate = {2024-03-23},
	journal = {Applied Intelligence},
	author = {Liu, Bin and Pliakos, Konstantinos and Vens, Celine and Tsoumakas, Grigorios},
	month = mar,
	year = {2022},
	note = {12 citations (Crossref) [2024-03-23]},
	keywords = {Ensemble learning, Drug-target interactions, Interaction recovery, Local imbalance, Nearest neighbor},
	pages = {3705--3727},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\XBWD3KNA\\Liu et al. - 2022 - Drug-target interaction prediction via an ensemble.pdf:application/pdf},
}

@inproceedings{lavrac_rule_1999,
	address = {Berlin, Heidelberg},
	title = {Rule {Evaluation} {Measures}: {A} {Unifying} {View}},
	isbn = {978-3-540-48751-7},
	shorttitle = {Rule {Evaluation} {Measures}},
	doi = {10.1007/3-540-48751-4_17},
	abstract = {Numerous measures are used for performance evaluation in machine learning. In predictive knowledge discovery, the most frequently used measure is classification accuracy. With new tasks being addressed in knowledge discovery, new measures appear. In descriptive knowledge discovery, where induced rules are not primarily intended for classification, new measures used are novelty in clausal and subgroup discovery, and support and confidence in association rule learning. Additional measures are needed as many descriptive knowledge discovery tasks involve the induction of a large set of redundant rules and the problem is the ranking and filtering of the induced rule set. In this paper we develop a unifying view on some of the existing measures for predictive and descriptive induction. We provide a common terminology and notation by means of contingency tables. We demonstrate how to trade off these measures, by using what we call weighted relative accuracy. The paper furthermore demonstrates that many rule evaluation measures developed for predictive knowledge discovery can be adapted to descriptive knowledge discovery tasks.},
	language = {en},
	booktitle = {Inductive {Logic} {Programming}},
	publisher = {Springer},
	author = {Lavrač, Nada and Flach, Peter and Zupan, Blaz},
	editor = {Džeroski, Sašo and Flach, Peter},
	year = {1999},
	keywords = {Association Rule, Contingency Table, Inductive Logic Programming, Knowledge Discovery, Relative Accuracy},
	pages = {174--185},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\SMSQZJRL\\Lavrač et al. - 1999 - Rule Evaluation Measures A Unifying View.pdf:application/pdf},
}

@article{ishwaran_random_2008,
	title = {Random survival forests},
	volume = {2},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Random-survival-forests/10.1214/08-AOAS169.full},
	doi = {10.1214/08-AOAS169},
	abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
	number = {3},
	urldate = {2024-06-25},
	journal = {The Annals of Applied Statistics},
	author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
	month = sep,
	year = {2008},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Conservation of events, cumulative hazard function, ensemble, out-of-bag, prediction error, survival tree},
	pages = {841--860},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\NKQCCL4W\\Ishwaran et al. - 2008 - Random survival forests.pdf:application/pdf},
}

@article{nateghi_haredasht_predicting_2022,
	title = {Predicting {Survival} {Outcomes} in the {Presence} of {Unlabeled} {Data}},
	volume = {111},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/10.1007/s10994-022-06257-x},
	doi = {10.1007/s10994-022-06257-x},
	language = {en},
	number = {11},
	urldate = {2024-08-06},
	journal = {Machine Learning},
	author = {Nateghi Haredasht, Fateme and Vens, Celine},
	month = nov,
	year = {2022},
	pages = {4139--4157},
	file = {Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\UTDH9ZIY\\Nateghi Haredasht and Vens - 2022 - Predicting Survival Outcomes in the Presence of Un.pdf:application/pdf},
}

@article{haredasht_exploiting_2023,
	title = {Exploiting {Censored} {Information} in {Self}-{Training} for {Time}-to-{Event} {Prediction}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10239393/},
	doi = {10.1109/ACCESS.2023.3312310},
	urldate = {2024-08-06},
	journal = {IEEE Access},
	author = {Haredasht, Fateme Nateghi and Dauda, Kazeem Adesina and Vens, Celine},
	year = {2023},
	pages = {96831--96840},
}

@article{roy_survival_2022,
	title = {Survival analysis with semi-supervised predictive clustering trees},
	volume = {141},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521007952},
	doi = {10.1016/j.compbiomed.2021.105001},
	language = {en},
	urldate = {2024-08-06},
	journal = {Computers in Biology and Medicine},
	author = {Roy, Bijit and Stepišnik, Tomaž and Vens, Celine and Džeroski, Sašo},
	month = feb,
	year = {2022},
	pages = {105001},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\9CBG2W27\\Roy et al. - 2022 - Survival analysis with semi-supervised predictive .pdf:application/pdf},
}

@inproceedings{dedja_active_2023,
	title = {Active {Learning} for {Survival} {Analysis} with {Incrementally} {Disclosed} {Label} {Information}.},
	url = {https://ceur-ws.org/Vol-3470/paper6.pdf},
	urldate = {2024-08-06},
	booktitle = {{IAL}@ {PKDD}/{ECML}},
	author = {Dedja, Klest and Nakano, Felipe Kenji and Vens, Celine},
	year = {2023},
	pages = {46--64},
	file = {Available Version (via Google Scholar):C\:\\Users\\u0170502\\Zotero\\storage\\H7MYSW3F\\Dedja et al. - 2023 - Active Learning for Survival Analysis with Increme.pdf:application/pdf},
}

@article{haredasht_exploiting_2023-1,
	title = {Exploiting {Censored} {Information} in {Self}-{Training} for {Time}-to-{Event} {Prediction}},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10239393},
	doi = {10.1109/ACCESS.2023.3312310},
	abstract = {A common problem in medical applications is predicting the time until an event of interest such as the onset of a disease, time to tumor recurrence, and time to mortality. Traditionally, classical survival analysis techniques have been used to address this problem. However, these techniques are of limited usage when considering nonlinear and interaction effects among biomarkers, and high profiling survival datasets. Although supervised machine learning techniques have shown some advantages over standard statistical methods in handling high-dimensional datasets, their application to survival analysis, particularly in the context of feature-based approaches, is at best limited. A major reason behind this is the difficulty in processing censored data, which is a common component of survival analysis. In this paper, we have transformed the time-to-event prediction problem into a semi-supervised regression problem. We utilize a self-training wrapper approach, where an outer layer guides the iterative refinement of predictions. This approach enhances the performance of our model by leveraging confident predictions from censored instances. The self-training wrapper is applied in conjunction with random survival forests as the base learner. In this approach, censored observations are introduced as partially labeled observations since their predicted time (target value) should exceed the censoring time. First, the algorithm builds a base model over the observed instances and then augments them iteratively with highly confident predictions over the censored set, using a smart stopping criterion based on the censoring time. The proposed approach has been evaluated and compared on fifteen real-world survival analysis datasets, including clinical and high-dimensional data. The ability of our proposed approach to integrate partial supervision information within a semi-supervised learning strategy has enabled it to achieve competitive performance compared to baseline models, particularly in the case of a high-dimensional regime.},
	urldate = {2024-08-09},
	journal = {IEEE Access},
	author = {Haredasht, Fateme Nateghi and Dauda, Kazeem Adesina and Vens, Celine},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Data models, Predictive models, Training, Analytical models, Learning systems, Prediction algorithms, Random survival forest, self-training, semi-supervised learning, Semisupervised learning, survival analysis, Timing, Tumors},
	pages = {96831--96840},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\8EMT46H7\\Haredasht et al. - 2023 - Exploiting Censored Information in Self-Training f.pdf:application/pdf},
}

@article{gharahighehi_addressing_2022,
	title = {Addressing the {Cold}-{Start} {Problem} in {Collaborative} {Filtering} {Through} {Positive}-{Unlabeled} {Learning} and {Multi}-{Target} {Prediction}},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9940479/?arnumber=9940479},
	doi = {10.1109/ACCESS.2022.3219071},
	abstract = {The cold-start problem is one of the main challenges in recommender systems and specifically in collaborative filtering methods. Such methods, albeit effective, typically can not handle new items or users that do not have any prior interaction activity in the system. In this paper, we propose a novel two-step approach to address the cold-start problem. First, we view the user-item interactions in a positive unlabeled (PU) learning setting and reconstruct the interaction matrix between users and warm items, detecting missing links and recommending warm items to existing users. Second, an inductive multi-target regressor is trained on this reconstructed interaction matrix and subsequently predicts interactions for new items that enter the system. To the best of our knowledge, this is the first time that such a two-step PU learning method is proposed to address the cold-start problem in recommender systems. To evaluate the proposed approach, we employed four benchmark datasets from movie and news recommendation domains with explicit and implicit feedback. We compared our method against three other competitor approaches that address the cold-start problem and showed that our proposed method significantly outperforms them, achieving in a case an increase of 16.9\% in terms of NDCG.},
	urldate = {2024-08-09},
	journal = {IEEE Access},
	author = {Gharahighehi, Alireza and Pliakos, Konstantinos and Vens, Celine},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Collaborative filtering, Recommender systems, PU learning, collaborative filtering, Predictive models, Learning systems, cold-start problem, Reliability, Sparse matrices, Task analysis},
	pages = {117189--117198},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\u0170502\\Zotero\\storage\\H8QW6LVB\\9940479.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\KIXHKVQ3\\Gharahighehi et al. - 2022 - Addressing the Cold-Start Problem in Collaborative.pdf:application/pdf},
}

@inproceedings{vultureanu-albisi_recommender_2021,
	title = {Recommender {Systems}: {An} {Explainable} {AI} {Perspective}},
	shorttitle = {Recommender {Systems}},
	url = {https://ieeexplore.ieee.org/abstract/document/9548125},
	doi = {10.1109/INISTA52262.2021.9548125},
	abstract = {In recent years, in the era of information overload development, the need for recommender systems that make personalized suggestion systems has become a very exciting field for researchers. To develop models that generate high-quality recommendations, the explainable recommendation has been introduced, proposing to develop intuitive and trustworthy explanations. The problem that the explainable recommendation wants to solve is to let people understand why certain elements rather than other are recommended by the system. This paper briefly overviews the short history of explainable AI and then it presents its role and applicability in the domain of recommender systems. Our work contributes to understanding the concept of explainable recommendation and what it should accomplish to increase its acceptability and to enable its accurate evaluation.},
	urldate = {2024-08-09},
	booktitle = {2021 {International} {Conference} on {INnovations} in {Intelligent} {SysTems} and {Applications} ({INISTA})},
	author = {Vultureanu-Albişi, Alexandra and Bădică, Costin},
	month = aug,
	year = {2021},
	keywords = {recommender system, Computational modeling, Computer science, explainable AI, Focusing, Human computer interaction, intelligent HCI, Philosophical considerations, Psychology, Technological innovation, XAI},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\JC5SY2GJ\\Vultureanu-Albişi and Bădică - 2021 - Recommender Systems An Explainable AI Perspective.pdf:application/pdf},
}

@article{zhang_explainable_2020,
	title = {Explainable {Recommendation}: {A} {Survey} and {New} {Perspectives}},
	volume = {14},
	issn = {1554-0669, 1554-0677},
	shorttitle = {Explainable {Recommendation}},
	url = {https://www.nowpublishers.com/article/Details/INR-066},
	doi = {10.1561/1500000066},
	abstract = {Explainable Recommendation: A Survey and New Perspectives},
	language = {English},
	number = {1},
	urldate = {2024-08-09},
	journal = {Foundations and Trends® in Information Retrieval},
	author = {Zhang, Yongfeng and Chen, Xu},
	month = mar,
	year = {2020},
	note = {Publisher: Now Publishers, Inc.},
	pages = {1--101},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\NRQGD2L4\\Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:application/pdf},
}

@article{zhang_explainable_2020-1,
	title = {Explainable {Recommendation}: {A} {Survey} and {New} {Perspectives}},
	volume = {14},
	issn = {1554-0669, 1554-0677},
	shorttitle = {Explainable {Recommendation}},
	url = {http://www.nowpublishers.com/article/Details/INR-066},
	doi = {10.1561/1500000066},
	language = {en},
	number = {1},
	urldate = {2024-08-09},
	journal = {Foundations and Trends® in Information Retrieval},
	author = {Zhang, Yongfeng and Chen, Xu},
	year = {2020},
	pages = {1--101},
	file = {Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\T4DDXLY9\\Zhang and Chen - 2020 - Explainable Recommendation A Survey and New Persp.pdf:application/pdf},
}

@article{wang_machine_2019,
	title = {Machine {Learning} for {Survival} {Analysis}: {A} {Survey}},
	volume = {51},
	issn = {0360-0300},
	shorttitle = {Machine {Learning} for {Survival} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3214306},
	doi = {10.1145/3214306},
	abstract = {Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called censoring can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data.},
	number = {6},
	urldate = {2024-08-09},
	journal = {ACM Comput. Surv.},
	author = {Wang, Ping and Li, Yan and Reddy, Chandan K.},
	month = feb,
	year = {2019},
	pages = {110:1--110:36},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\PFAH6WAR\\Wang et al. - 2019 - Machine Learning for Survival Analysis A Survey.pdf:application/pdf},
}

@article{kim_analysis_2021,
	title = {Analysis of risk factors correlated with angiographic vasospasm in patients with aneurysmal subarachnoid hemorrhage using explainable predictive modeling},
	volume = {91},
	issn = {0967-5868},
	url = {https://www.sciencedirect.com/science/article/pii/S0967586821003799},
	doi = {10.1016/j.jocn.2021.07.028},
	abstract = {Cerebral vasospasm (CAV) is a major complication of aneurysmal subarachnoid hemorrhage (aSAH) in patients with ruptured intracranial aneurysm. Explainable artificial intelligence (XAI) was used to analyze the contribution of risk factors on the development of CAV. We obtained data about patients (n = 343) treated for aSAH in our hospital. Predictive factors including age, aneurysm size, Hunt and Hess grade, and modified Fisher grade were used as input to analyze the contribution and correlation of factors correlated with CAV using a random forest regressor. An analysis conducted using an XAI model showed that aneurysm size (27.6\%) was most significantly associated with the development of CAV, followed by age (20.7\%) and Glasgow coma scale score (7.1\%). In some patients with an estimated artificial intelligence-selected CAV value of 51\%, the important risk factors were aneurysm size (9.1 mm) and location, and hypertension is also considered a major influencing factor. We could predict that Fisher grade 3 contributed to 20.3\%, and the group using Antiplatelet contributed to 12.2\% which is expected to lower cerebral CAV compared to the Control group (16.9\%). The accuracy rate of the XAI system was 85.5\% (area under the curve = 0.88). Using the modeling, aneurysm size and age were quantitatively analyzed and were found to be significantly associated with CAV in patients with aSAH. Hence, XAI modeling techniques can be used to analyze factors correlated with CAV by schematizing prediction results in some patients. Moreover, poor Fisher grade and use of postoperative antiplatelet agent are important factors for prediction of CAV.},
	urldate = {2024-08-14},
	journal = {Journal of Clinical Neuroscience},
	author = {Kim, Kwang Hyeon and Koo, Hae-Won and Lee, Byung-Jou and Sohn, Moon-Jun},
	month = sep,
	year = {2021},
	keywords = {Aneurysm, Cerebral arterial vasospasm, Explainable artificial intelligence, Feature analysis, Subarachnoid hemorrhage},
	pages = {334--342},
	file = {ScienceDirect Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\P69A6FF3\\S0967586821003799.html:text/html},
}

@article{kim_analysis_2021-1,
	title = {Analysis of risk factors correlated with angiographic vasospasm in patients with aneurysmal subarachnoid hemorrhage using explainable predictive modeling},
	volume = {91},
	issn = {09675868},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0967586821003799},
	doi = {10.1016/j.jocn.2021.07.028},
	language = {en},
	urldate = {2024-08-14},
	journal = {Journal of Clinical Neuroscience},
	author = {Kim, Kwang Hyeon and Koo, Hae-Won and Lee, Byung-Jou and Sohn, Moon-Jun},
	month = sep,
	year = {2021},
	pages = {334--342},
}

@article{kim_analysis_2021-2,
	title = {Analysis of risk factors correlated with angiographic vasospasm in patients with aneurysmal subarachnoid hemorrhage using explainable predictive modeling},
	volume = {91},
	issn = {09675868},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0967586821003799},
	doi = {10.1016/j.jocn.2021.07.028},
	abstract = {Cerebral vasospasm (CAV) is a major complication of aneurysmal subarachnoid hemorrhage (aSAH) in patients with ruptured intracranial aneurysm. Explainable artiﬁcial intelligence (XAI) was used to analyze the contribution of risk factors on the development of CAV. We obtained data about patients (n = 343) treated for aSAH in our hospital. Predictive factors including age, aneurysm size, Hunt and Hess grade, and modiﬁed Fisher grade were used as input to analyze the contribution and correlation of factors correlated with CAV using a random forest regressor. An analysis conducted using an XAI model showed that aneurysm size (27.6\%) was most signiﬁcantly associated with the development of CAV, followed by age (20.7\%) and Glasgow coma scale score (7.1\%). In some patients with an estimated artiﬁcial intelligence-selected CAV value of 51\%, the important risk factors were aneurysm size (9.1 mm) and location, and hypertension is also considered a major inﬂuencing factor. We could predict that Fisher grade 3 contributed to 20.3\%, and the group using Antiplatelet contributed to 12.2\% which is expected to lower cerebral CAV compared to the Control group (16.9\%). The accuracy rate of the XAI system was 85.5\% (area under the curve = 0.88). Using the modeling, aneurysm size and age were quantitatively analyzed and were found to be signiﬁcantly associated with CAV in patients with aSAH. Hence, XAI modeling techniques can be used to analyze factors correlated with CAV by schematizing prediction results in some patients. Moreover, poor Fisher grade and use of postoperative antiplatelet agent are important factors for prediction of CAV.},
	language = {en},
	urldate = {2024-08-14},
	journal = {Journal of Clinical Neuroscience},
	author = {Kim, Kwang Hyeon and Koo, Hae-Won and Lee, Byung-Jou and Sohn, Moon-Jun},
	month = sep,
	year = {2021},
	pages = {334--342},
	file = {Kim et al. - 2021 - Analysis of risk factors correlated with angiograp.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\9UYVYRZR\\Kim et al. - 2021 - Analysis of risk factors correlated with angiograp.pdf:application/pdf},
}

@misc{wang_machine_2017,
	title = {Machine {Learning} for {Survival} {Analysis}: {A} {Survey}},
	shorttitle = {Machine {Learning} for {Survival} {Analysis}},
	url = {http://arxiv.org/abs/1708.04649},
	abstract = {Accurately predicting the time of occurrence of an event of interest is a critical problem in longitudinal data analysis. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. Such a phenomenon is called censoring which can be effectively handled using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the representative statistical methods along with the machine learning techniques used in survival analysis and provide a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. We hope that this paper will provide a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data.},
	language = {en},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Wang, Ping and Li, Yan and Reddy, Chandan K.},
	month = aug,
	year = {2017},
	note = {arXiv:1708.04649 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Wang et al. - 2017 - Machine Learning for Survival Analysis A Survey.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\6FJCT75W\\Wang et al. - 2017 - Machine Learning for Survival Analysis A Survey.pdf:application/pdf},
}

@book{kleinbaum_survival_2012,
	address = {New York, NY},
	series = {Statistics for {Biology} and {Health}},
	title = {Survival {Analysis}: {A} {Self}-{Learning} {Text}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-6645-2 978-1-4419-6646-9},
	shorttitle = {Survival {Analysis}},
	url = {http://link.springer.com/10.1007/978-1-4419-6646-9},
	language = {en},
	urldate = {2024-08-14},
	publisher = {Springer New York},
	author = {Kleinbaum, David G. and Klein, Mitchel},
	year = {2012},
	doi = {10.1007/978-1-4419-6646-9},
	file = {Kleinbaum and Klein - 2012 - Survival Analysis A Self-Learning Text.pdf:C\:\\Users\\u0170502\\Zotero\\storage\\3HACJIR3\\Kleinbaum and Klein - 2012 - Survival Analysis A Self-Learning Text.pdf:application/pdf},
}

@book{settles_active_2012,
	address = {Cham},
	series = {Synthesis {Lectures} on {Artificial} {Intelligence} and {Machine} {Learning}},
	title = {Active {Learning}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-031-00432-2 978-3-031-01560-1},
	url = {https://link.springer.com/10.1007/978-3-031-01560-1},
	language = {en},
	urldate = {2024-08-14},
	publisher = {Springer International Publishing},
	author = {Settles, Burr},
	year = {2012},
	doi = {10.1007/978-3-031-01560-1},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\XVKC9ZUF\\Settles - 2012 - Active Learning.pdf:application/pdf},
}

@inproceedings{pesovski_systematic_2022,
	title = {Systematic {Review} of the published {Explainable} {Educational} {Recommendation} {Systems}},
	url = {https://ieeexplore.ieee.org/abstract/document/10032029},
	doi = {10.1109/ITHET56107.2022.10032029},
	abstract = {The goal of this paper is to systematically review the available literature on the topic of explainable recommendation systems in education, especially when recommendation systems are integrated as a part of learning management systems (LMS). The focus years for analyzing available literature are the years between 2010 and 2022, period when online learning is expanding and online learning platforms are continuously being developed, which makes these years relevant for scoping this review. The topic of interest in this research are recommendation algorithms whose results can be explained and interpreted. The first part of the methodology used in the paper utilizes an NLP-powered toolkit that automates a big part of the review process by automatically analyzing articles indexed in the IEEE Xplore, PubMed, Springer, Elsevier and MDPI digital libraries. The toolkit relies on the PRISMA methodology for standardizing systematic reviews. First, a quantitative analysis of all available literature is performed, followed by a qualitative analysis of the few selected articles which indeed focus on the explainability when implementing recommendation systems in educational context. The relevant articles are analyzed in detail and compared on multiple indicators like the field of work, tools and techniques used, and how explainability is achieved. The results show that although the amount of available research is growing and new learning management systems are being developed at a fast pace in the last few years, the explainability of the machine learning techniques used in the recommendation systems is not a popular topic among the researchers and developers with research interest in educational context. The amount of the available literature for explainable recommendation systems in educational environment is scarce, but is expected to grow following the global trend of explainable artificial intelligence ({\textbackslash}mathrmx{\textbackslash}mathrmA{\textbackslash}mathrmI) as key technique for practical implementation of advanced AI models.},
	urldate = {2024-08-16},
	booktitle = {2022 20th {International} {Conference} on {Information} {Technology} {Based} {Higher} {Education} and {Training} ({ITHET})},
	author = {Pesovski, Ivica and Bogdanova, Ana Madevska and Trajkovik, Vladimir},
	month = nov,
	year = {2022},
	note = {ISSN: 2380-1603},
	keywords = {Machine learning, Training, educational recommendation systems, explainable recommendation systems, Learning management systems, Market research, qualitative analysis, quantitative analysis, Statistical analysis, Switches, Systematics},
	pages = {1--8},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\AA5V2XEZ\\Pesovski et al. - 2022 - Systematic Review of the published Explainable Edu.pdf:application/pdf},
}

@article{pliakos_integrating_2019,
	title = {Integrating machine learning into item response theory for addressing the cold start problem in adaptive learning systems},
	volume = {137},
	issn = {0360-1315},
	url = {https://www.sciencedirect.com/science/article/pii/S036013151930096X},
	doi = {10.1016/j.compedu.2019.04.009},
	abstract = {Adaptive learning systems aim to provide learning items tailored to the behavior and needs of individual learners. However, one of the outstanding challenges in adaptive item selection is that often the corresponding systems do not have information on initial ability levels of new learners entering a learning environment. Thus, the proficiency of those new learners is very difficult to be predicted. This heavily impairs the quality of personalized items' recommendation during the initial phase of the learning environment. In order to handle this issue, known as the cold-start problem, we propose a system that combines item response theory (IRT) with machine learning. Specifically, we perform ability estimation and item response prediction for new learners by integrating IRT with classification and regression trees built on learners’ side information. The goal of this work is to build a learning system that incorporates IRT and machine learning into a unified framework. We compare the proposed hybrid model to alternative approaches by conducting experiments on two educational data sets. The obtained results affirmed the potential of the proposed method. In particular, the obtained results indicate that IRT combined with Random Forests provides the lowest error for the ability estimation and the highest accuracy in terms of response prediction. This way, we deduce that the employment of machine learning in combination with IRT could indeed alleviate the effect of the cold start problem in an adaptive learning environment.},
	urldate = {2024-08-16},
	journal = {Computers \& Education},
	author = {Pliakos, Konstantinos and Joo, Seang-Hwane and Park, Jung Yeon and Cornillie, Frederik and Vens, Celine and Van den Noortgate, Wim},
	month = aug,
	year = {2019},
	keywords = {Machine learning, Decision tree learning, Adaptive learning system, Cold-start problem, Item response theory},
	pages = {91--103},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\3JH7PBV2\\Pliakos et al. - 2019 - Integrating machine learning into item response th.pdf:application/pdf},
}

@article{burrell_how_2016,
	title = {How the machine ‘thinks’: {Understanding} opacity in machine learning algorithms},
	volume = {3},
	issn = {2053-9517},
	shorttitle = {How the machine ‘thinks’},
	url = {https://doi.org/10.1177/2053951715622512},
	doi = {10.1177/2053951715622512},
	abstract = {This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm.},
	language = {en},
	number = {1},
	urldate = {2024-10-11},
	journal = {Big Data \& Society},
	author = {Burrell, Jenna},
	month = jun,
	year = {2016},
	note = {Publisher: SAGE Publications Ltd},
	pages = {2053951715622512},
	file = {SAGE PDF Full Text:C\:\\Users\\u0170502\\Zotero\\storage\\TCG4TVTF\\Burrell - 2016 - How the machine ‘thinks’ Understanding opacity in.pdf:application/pdf},
}

@inproceedings{norouzi_efficient_2015,
	title = {Efficient {Non}-greedy {Optimization} of {Decision} {Trees}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/1579779b98ce9edb98dd85606f2c119d-Abstract.html},
	abstract = {Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d{\textasciicircum}2), where d is the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.},
	urldate = {2024-10-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Norouzi, Mohammad and Collins, Maxwell and Johnson, Matthew A and Fleet, David J and Kohli, Pushmeet},
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\6A5N3YV7\\Norouzi et al. - 2015 - Efficient Non-greedy Optimization of Decision Tree.pdf:application/pdf},
}

@article{wang_structure-aware_2022,
	title = {Structure-{Aware} {Multimodal} {Deep} {Learning} for {Drug}–{Protein} {Interaction} {Prediction}},
	volume = {62},
	issn = {1549-9596},
	url = {https://doi.org/10.1021/acs.jcim.2c00060},
	doi = {10.1021/acs.jcim.2c00060},
	abstract = {Identifying drug–protein interactions (DPIs) is crucial in drug discovery, and a number of machine learning methods have been developed to predict DPIs. Existing methods usually use unrealistic data sets with hidden bias, which will limit the accuracy of virtual screening methods. Meanwhile, most DPI prediction methods pay more attention to molecular representation but lack effective research on protein representation and high-level associations between different instances. To this end, we present the novel structure-aware multimodal deep DPI prediction model, STAMP-DPI, which was trained on a curated industry-scale benchmark data set. We built a high-quality benchmark data set named GalaxyDB for DPI prediction. This industry-scale data set along with an unbiased training procedure resulted in a more robust benchmark study. For informative protein representation, we constructed a structure-aware graph neural network method from the protein sequence by combining predicted contact maps and graph neural networks. Through further integration of structure-based representation and high-level pretrained embeddings for molecules and proteins, our model effectively captures the feature representation of the interactions between them. As a result, STAMP-DPI outperformed state-of-the-art DPI prediction methods by decreasing 7.00\% mean square error (MSE) in the Davis data set and improving 8.89\% area under the curve (AUC) in the GalaxyDB data set. Moreover, our model is an interpretable model with the transformer-based interaction mechanism, which can accurately reveal the binding sites between molecules and proteins.},
	number = {5},
	urldate = {2024-10-17},
	journal = {Journal of Chemical Information and Modeling},
	author = {Wang, Penglei and Zheng, Shuangjia and Jiang, Yize and Li, Chengtao and Liu, Junhong and Wen, Chang and Patronov, Atanas and Qian, Dahong and Chen, Hongming and Yang, Yuedong},
	month = mar,
	year = {2022},
	note = {Publisher: American Chemical Society},
	pages = {1308--1317},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\ZDMS9K7I\\Wang et al. - 2022 - Structure-Aware Multimodal Deep Learning for Drug–.pdf:application/pdf},
}

@article{zhao_hyperattentiondti_2022,
	title = {{HyperAttentionDTI}: improving drug–protein interaction prediction by sequence-based deep learning with attention mechanism},
	volume = {38},
	issn = {1367-4803},
	shorttitle = {{HyperAttentionDTI}},
	url = {https://doi.org/10.1093/bioinformatics/btab715},
	doi = {10.1093/bioinformatics/btab715},
	abstract = {Identifying drug–target interactions (DTIs) is a crucial step in drug repurposing and drug discovery. Accurately identifying DTIs in silico can significantly shorten development time and reduce costs. Recently, many sequence-based methods are proposed for DTI prediction and improve performance by introducing the attention mechanism. However, these methods only model single non-covalent inter-molecular interactions among drugs and proteins and ignore the complex interaction between atoms and amino acids.In this article, we propose an end-to-end bio-inspired model based on the convolutional neural network (CNN) and attention mechanism, named HyperAttentionDTI, for predicting DTIs. We use deep CNNs to learn the feature matrices of drugs and proteins. To model complex non-covalent inter-molecular interactions among atoms and amino acids, we utilize the attention mechanism on the feature matrices and assign an attention vector to each atom or amino acid. We evaluate HpyerAttentionDTI on three benchmark datasets and the results show that our model achieves significantly improved performance compared with the state-of-the-art baselines. Moreover, a case study on the human Gamma-aminobutyric acid receptors confirm that our model can be used as a powerful tool to predict DTIs.The codes of our model are available at https://github.com/zhaoqichang/HpyerAttentionDTI and https://zenodo.org/record/5039589.Supplementary data are available at Bioinformatics online.},
	number = {3},
	urldate = {2024-10-17},
	journal = {Bioinformatics},
	author = {Zhao, Qichang and Zhao, Haochen and Zheng, Kai and Wang, Jianxin},
	month = jan,
	year = {2022},
	pages = {655--662},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\I3BX7KB9\\Zhao et al. - 2022 - HyperAttentionDTI improving drug–protein interact.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\CDVNHR5A\\6401997.html:text/html},
}

@article{xia_semi-supervised_2010,
	title = {Semi-supervised drug-protein interaction prediction from heterogeneous biological spaces},
	volume = {4},
	issn = {1752-0509},
	url = {https://doi.org/10.1186/1752-0509-4-S2-S6},
	doi = {10.1186/1752-0509-4-S2-S6},
	abstract = {Predicting drug-protein interactions from heterogeneous biological data sources is a key step for in silico drug discovery. The difficulty of this prediction task lies in the rarity of known drug-protein interactions and myriad unknown interactions to be predicted. To meet this challenge, a manifold regularization semi-supervised learning method is presented to tackle this issue by using labeled and unlabeled information which often generates better results than using the labeled data alone. Furthermore, our semi-supervised learning method integrates known drug-protein interaction network information as well as chemical structure and genomic sequence data.},
	language = {en},
	number = {2},
	urldate = {2024-10-17},
	journal = {BMC Systems Biology},
	author = {Xia, Zheng and Wu, Ling-Yun and Zhou, Xiaobo and Wong, Stephen TC},
	month = sep,
	year = {2010},
	keywords = {Chemical Structure Similarity, Drug Domain, Genomic Space, Manifold Regularization, Unlabeled Sample},
	pages = {S6},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\Z4D2PDN3\\Xia et al. - 2010 - Semi-supervised drug-protein interaction predictio.pdf:application/pdf},
}

@article{frank_using_1998,
	title = {Using {Model} {Trees} for {Classification}},
	volume = {32},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007421302149},
	doi = {10.1023/A:1007421302149},
	abstract = {Model trees, which are a type of decision tree with linear regression functions at the leaves, form the basis of a recent successful technique for predicting continuous numeric values. They can be applied to classification problems by employing a standard method of transforming a classification problem into a problem of function approximation. Surprisingly, using this simple transformation the model tree inducer M5′, based on Quinlan's M5, generates more accurate classifiers than the state-of-the-art decision tree learner C5.0, particularly when most of the attributes are numeric.},
	language = {en},
	number = {1},
	urldate = {2024-10-17},
	journal = {Machine Learning},
	author = {Frank, Eibe and Wang, Yong and Inglis, Stuart and Holmes, Geoffrey and Witten, Ian H.},
	month = jul,
	year = {1998},
	keywords = {decision trees, Artificial Intelligence, C5.0, classification algorithms, M5, Model trees},
	pages = {63--76},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\95LB3VNS\\Frank et al. - 1998 - Using Model Trees for Classification.pdf:application/pdf},
}

@article{landwehr_logistic_2005,
	title = {Logistic {Model} {Trees}},
	volume = {59},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-005-0466-3},
	doi = {10.1007/s10994-005-0466-3},
	abstract = {Tree induction methods and linear models are popular techniques for supervised learning tasks, both for the prediction of nominal classes and numeric values. For predicting numeric quantities, there has been work on combining these two schemes into ‘model trees’, i.e. trees that contain linear regression functions at the leaves. In this paper, we present an algorithm that adapts this idea for classification problems, using logistic regression instead of linear regression. We use a stagewise fitting process to construct the logistic regression models that can select relevant attributes in the data in a natural way, and show how this approach can be used to build the logistic regression models at the leaves by incrementally refining those constructed at higher levels in the tree. We compare the performance of our algorithm to several other state-of-the-art learning schemes on 36 benchmark UCI datasets, and show that it produces accurate and compact classifiers.},
	language = {en},
	number = {1},
	urldate = {2024-10-17},
	journal = {Machine Learning},
	author = {Landwehr, Niels and Hall, Mark and Frank, Eibe},
	month = may,
	year = {2005},
	keywords = {classification, Artificial Intelligence, logistic regression, model trees},
	pages = {161--205},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\DKLMSXSG\\Landwehr et al. - 2005 - Logistic Model Trees.pdf:application/pdf},
}

@article{ikonomovska_learning_2011,
	title = {Learning model trees from evolving data streams},
	volume = {23},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-010-0201-y},
	doi = {10.1007/s10618-010-0201-y},
	abstract = {The problem of real-time extraction of meaningful patterns from time-changing data streams is of increasing importance for the machine learning and data mining communities. Regression in time-changing data streams is a relatively unexplored topic, despite the apparent applications. This paper proposes an efficient and incremental stream mining algorithm which is able to learn regression and model trees from possibly unbounded, high-speed and time-changing data streams. The algorithm is evaluated extensively in a variety of settings involving artificial and real data. To the best of our knowledge there is no other general purpose algorithm for incremental learning regression/model trees able to perform explicit change detection and informed adaptation. The algorithm performs online and in real-time, observes each example only once at the speed of arrival, and maintains at any-time a ready-to-use model tree. The tree leaves contain linear models induced online from the examples assigned to them, a process with low complexity. The algorithm has mechanisms for drift detection and model adaptation, which enable it to maintain accurate and updated regression models at any time. The drift detection mechanism exploits the structure of the tree in the process of local change detection. As a response to local drift, the algorithm is able to update the tree structure only locally. This approach improves the any-time performance and greatly reduces the costs of adaptation.},
	language = {en},
	number = {1},
	urldate = {2024-10-17},
	journal = {Data Mining and Knowledge Discovery},
	author = {Ikonomovska, Elena and Gama, João and Džeroski, Sašo},
	month = jul,
	year = {2011},
	keywords = {Artificial Intelligence, Model trees, Concept drift, Incremental algorithms, Non-stationary data streams, On-line change detection, On-line learning, Regression trees, Stream data mining},
	pages = {128--168},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\J2K5JFVZ\\Ikonomovska et al. - 2011 - Learning model trees from evolving data streams.pdf:application/pdf},
}

@inproceedings{norouzi_efficient_2015-1,
	title = {Efficient {Non}-greedy {Optimization} of {Decision} {Trees}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/1579779b98ce9edb98dd85606f2c119d-Abstract.html},
	abstract = {Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d{\textasciicircum}2), where d is the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.},
	urldate = {2024-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Norouzi, Mohammad and Collins, Maxwell and Johnson, Matthew A and Fleet, David J and Kohli, Pushmeet},
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\8MJ8ZAHV\\Norouzi et al. - 2015 - Efficient Non-greedy Optimization of Decision Tree.pdf:application/pdf},
}

@article{xie_it_2018,
	title = {It is time to apply biclustering: a comprehensive review of biclustering applications in biological and biomedical data},
	volume = {20},
	shorttitle = {It is time to apply biclustering},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC6931057/},
	doi = {10.1093/bib/bby014},
	abstract = {Biclustering is a powerful data mining technique that allows clustering of rows and columns, simultaneously, in a matrix-format data set. It was first applied to gene expression data in 2000, aiming to identify co-expressed genes under a subset of ...},
	language = {en},
	number = {4},
	urldate = {2024-10-23},
	journal = {Briefings in Bioinformatics},
	author = {Xie, Juan and Ma, Anjun and Fennell, Anne and Ma, Qin and Zhao, Jing},
	month = feb,
	year = {2018},
	pmid = {29490019},
	pages = {1450},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\HLNKX5CG\\Xie et al. - 2018 - It is time to apply biclustering a comprehensive .pdf:application/pdf},
}

@misc{dedja_bellatrex_2023,
	title = {{BELLATREX}: {Building} {Explanations} through a {LocaLly} {AccuraTe} {Rule} {EXtractor}},
	shorttitle = {{BELLATREX}},
	url = {http://arxiv.org/abs/2203.15511},
	doi = {10.48550/arXiv.2203.15511},
	abstract = {Tree-ensemble algorithms, such as random forest, are effective machine learning methods popular for their flexibility, high performance, and robustness to overfitting. However, since multiple learners are combined, they are not as interpretable as a single decision tree. In this work we propose a novel method that is Building Explanations through a LocalLy AccuraTe Rule EXtractor (Bellatrex), and is able to explain the forest prediction for a given test instance with only a few diverse rules. Starting from the decision trees generated by a random forest, our method 1) pre-selects a subset of the rules used to make the prediction, 2) creates a vector representation of such rules, 3) projects them to a low-dimensional space, 4) clusters such representations to pick a rule from each cluster to explain the instance prediction. We test the effectiveness of Bellatrex on 89 real-world datasets and we demonstrate the validity of our method for binary classification, regression, multi-label classification and time-to-event tasks. To the best of our knowledge, it is the first time that an interpretability toolbox can handle all these tasks within the same framework. We also show that our extracted surrogate model can approximate the performance of the corresponding ensemble model in all considered tasks, while selecting only few trees from the whole forest. We also show that our proposed approach substantially outperforms other explainable methods in terms of predictive performance.},
	urldate = {2024-11-03},
	publisher = {arXiv},
	author = {Dedja, Klest and Nakano, Felipe Kenji and Pliakos, Konstantinos and Vens, Celine},
	month = jan,
	year = {2023},
	note = {arXiv:2203.15511},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\u0170502\\Zotero\\storage\\NU29YA62\\Dedja et al. - 2023 - BELLATREX Building Explanations through a LocaLly.pdf:application/pdf;Snapshot:C\:\\Users\\u0170502\\Zotero\\storage\\55JD4RGJ\\2203.html:text/html},
}

@article{lee_principal_2023,
	title = {A principal odor map unifies diverse tasks in olfactory perception},
	volume = {381},
	url = {https://www.science.org/doi/10.1126/science.ade4401},
	doi = {10.1126/science.ade4401},
	abstract = {Mapping molecular structure to odor perception is a key challenge in olfaction. We used graph neural networks to generate a principal odor map (POM) that preserves perceptual relationships and enables odor quality prediction for previously uncharacterized odorants. The model was as reliable as a human in describing odor quality: On a prospective validation set of 400 out-of-sample odorants, the model-generated odor profile more closely matched the trained panel mean than did the median panelist. By applying simple, interpretable, theoretically rooted transformations, the POM outperformed chemoinformatic models on several other odor prediction tasks, indicating that the POM successfully encoded a generalized map of structure-odor relationships. This approach broadly enables odor prediction and paves the way toward digitizing odors.},
	number = {6661},
	urldate = {2024-11-13},
	journal = {Science},
	author = {Lee, Brian K. and Mayhew, Emily J. and Sanchez-Lengeling, Benjamin and Wei, Jennifer N. and Qian, Wesley W. and Little, Kelsie A. and Andres, Matthew and Nguyen, Britney B. and Moloy, Theresa and Yasonik, Jacob and Parker, Jane K. and Gerkin, Richard C. and Mainland, Joel D. and Wiltschko, Alexander B.},
	month = sep,
	year = {2023},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {999--1006},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\XUZ4BDID\\Lee et al. - 2023 - A principal odor map unifies diverse tasks in olfa.pdf:application/pdf},
}

@article{keller_predicting_2017,
	title = {Predicting human olfactory perception from chemical features of odor molecules},
	volume = {355},
	url = {https://www.science.org/doi/10.1126/science.aal2014},
	doi = {10.1126/science.aal2014},
	abstract = {It is still not possible to predict whether a given molecule will have a perceived odor or what olfactory percept it will produce. We therefore organized the crowd-sourced DREAM Olfaction Prediction Challenge. Using a large olfactory psychophysical data set, teams developed machine-learning algorithms to predict sensory attributes of molecules based on their chemoinformatic features. The resulting models accurately predicted odor intensity and pleasantness and also successfully predicted 8 among 19 rated semantic descriptors (“garlic,” “fish,” “sweet,” “fruit,” “burnt,” “spices,” “flower,” and “sour”). Regularized linear models performed nearly as well as random forest–based ones, with a predictive accuracy that closely approaches a key theoretical limit. These models help to predict the perceptual qualities of virtually any molecule with high accuracy and also reverse-engineer the smell of a molecule.},
	number = {6327},
	urldate = {2024-11-13},
	journal = {Science},
	author = {Keller, Andreas and Gerkin, Richard C. and Guan, Yuanfang and Dhurandhar, Amit and Turu, Gabor and Szalai, Bence and Mainland, Joel D. and Ihara, Yusuke and Yu, Chung Wen and Wolfinger, Russ and Vens, Celine and schietgat, leander and De Grave, Kurt and Norel, Raquel and {DREAM OLFACTION PREDICTION CONSORTIUM} and Stolovitzky, Gustavo and Cecchi, Guillermo A. and Vosshall, Leslie B. and meyer, pablo},
	month = feb,
	year = {2017},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {820--826},
	file = {Accepted Version:C\:\\Users\\u0170502\\Zotero\\storage\\CE2MFI6Y\\Keller et al. - 2017 - Predicting human olfactory perception from chemica.pdf:application/pdf},
}

@article{heid_chemprop_2024,
	title = {Chemprop: {A} {Machine} {Learning} {Package} for {Chemical} {Property} {Prediction}},
	volume = {64},
	issn = {1549-9596},
	shorttitle = {Chemprop},
	url = {https://doi.org/10.1021/acs.jcim.3c01250},
	doi = {10.1021/acs.jcim.3c01250},
	abstract = {Deep learning has become a powerful and frequently employed tool for the prediction of molecular properties, thus creating a need for open-source and versatile software solutions that can be operated by nonexperts. Among the current approaches, directed message-passing neural networks (D-MPNNs) have proven to perform well on a variety of property prediction tasks. The software package Chemprop implements the D-MPNN architecture and offers simple, easy, and fast access to machine-learned molecular properties. Compared to its initial version, we present a multitude of new Chemprop functionalities such as the support of multimolecule properties, reactions, atom/bond-level properties, and spectra. Further, we incorporate various uncertainty quantification and calibration methods along with related metrics as well as pretraining and transfer learning workflows, improved hyperparameter optimization, and other customization options concerning loss functions or atom/bond features. We benchmark D-MPNN models trained using Chemprop with the new reaction, atom-level, and spectra functionality on a variety of property prediction data sets, including MoleculeNet and SAMPL, and observe state-of-the-art performance on the prediction of water-octanol partition coefficients, reaction barrier heights, atomic partial charges, and absorption spectra. Chemprop enables out-of-the-box training of D-MPNN models for a variety of problem settings in fast, user-friendly, and open-source software.},
	number = {1},
	urldate = {2024-11-13},
	journal = {Journal of Chemical Information and Modeling},
	author = {Heid, Esther and Greenman, Kevin P. and Chung, Yunsie and Li, Shih-Cheng and Graff, David E. and Vermeire, Florence H. and Wu, Haoyang and Green, William H. and McGill, Charles J.},
	month = jan,
	year = {2024},
	note = {Publisher: American Chemical Society},
	pages = {9--17},
	file = {Full Text PDF:C\:\\Users\\u0170502\\Zotero\\storage\\3S8YMH4L\\Heid et al. - 2024 - Chemprop A Machine Learning Package for Chemical .pdf:application/pdf},
}
